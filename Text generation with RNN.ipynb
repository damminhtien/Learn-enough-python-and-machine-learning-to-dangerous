{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import necessary modules\n",
    "from __future__ import print_function\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential \n",
    "from keras.utils.vis_utils import plot_model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# copyright 2007 google, inc. all rights reserved.\\n # licensed to psf under a contributor agreement.\\n \"\"\"abstract base classes (abcs) according to pep 3119.\"\"\"\\n from _weakrefset import weakset\\n def abstractmethod(funcobj):\\n \"\"\"a decorator indicating abstract methods.\\n requires that the metaclass is abcmeta or derived from it.  a\\n class that has a metaclass derived from abcmeta cannot be\\n instantiated unless all of its abstract methods are overridden.\\n the abstract methods can be called using any of the normal\\n \\'super\\' call mechanisms.\\n usage:\\n class c(metaclass=abcmeta):\\n @abstractmethod\\n def my_abstract_method(self, ...):\\n ...\\n \"\"\"\\n funcobj.__isabstractmethod__ = true\\n return funcobj\\n class abstractclassmethod(classmethod):\\n \"\"\"\\n a decorator indicating abstract classmethods.\\n similar to abstractmethod.\\n usage:\\n class c(metaclass=abcmeta):\\n @abstractclassmethod\\n def my_abstract_classmethod(cls, ...):\\n ...\\n \\'abstractclassmethod\\' is deprecated. use \\'classmethod\\' with\\n \\'abstractmethod\\' instead.\\n \"\"\"\\n __isabstractmethod__ = true\\n def __init__(self, callable):\\n callable.__isabstractmethod__ = true\\n super().__init__(callable)\\n class abstractstaticmethod(staticmethod):\\n \"\"\"\\n a decorator indicating abstract staticmethods.\\n similar to abstractmethod.\\n usage:\\n class c(metaclass=abcmeta):\\n @abstractstaticmethod\\n def my_abstract_staticmethod(...):\\n ...\\n \\'abstractstaticmethod\\' is deprecated. use \\'staticmethod\\' with\\n \\'abstractmethod\\' instead.\\n \"\"\"\\n __isabstractmethod__ = true\\n def __init__(self, callable):\\n callable.__isabstractmethod__ = true\\n super().__init__(callable)\\n class abstractproperty(property):\\n \"\"\"\\n a decorator indicating abstract properties.\\n requires that the metaclass is abcmeta or derived from it.  a\\n class that has a metaclass derived from abcmeta cannot be\\n instantiated unless all of its abstract properties are overridden.\\n the abstract properties can be called using any of the normal\\n \\'super\\' call mechanisms.\\n usage:\\n class c(metaclass=abcmeta):\\n @abstractproperty\\n def my_abstract_property(self):\\n ...\\n this defines a read-only property; you can also define a read-write\\n abstract property using the \\'long\\' form of property declaration:\\n class c(metaclass=abcmeta):\\n def getx(self): ...\\n def setx(self, value): ...\\n x = abstractproperty(getx, setx)\\n \\'abstractproperty\\' is deprecated. use \\'property\\' with \\'abstractmethod\\'\\n instead.\\n \"\"\"\\n __isabstractmethod__ = true\\n class abcmeta(type):\\n \"\"\"metaclass for defining abstract base classes (abcs).\\n use this metaclass to create an abc.  an abc can be subclassed\\n directly, and then acts as a mix-in class.  you can also register\\n unrelated concrete classes (even built-in classes) and unrelated\\n abcs as \\'virtual subclasses\\' -- these and their descendants will\\n be considered subclasses of the registering abc by the built-in\\n issubclass() function, but the registering abc won\\'t show up in\\n their mro (method resolution order) nor will method\\n implementations defined by the registering abc be callable (not\\n even via super()).\\n \"\"\"\\n # a global counter that is incremented each time a class is\\n # registered as a virtual subclass of anything.  it forces the\\n # negative cache to be cleared before its next use.\\n # note: this counter is private. use `abc.get_cache_token()` for\\n #       external code.\\n _abc_invalidation_counter = 0\\n def __new__(mcls, name, bases, namespace):\\n cls = super().__new__(mcls, name, bases, namespace)\\n # compute set of abstract method names\\n abstracts = {name\\n for name, value in namespace.items()\\n if getattr(value, \"__isabstractmethod__\", false)}\\n for base in bases:\\n for name in getattr(base, \"__abstractmethods__\", set()):\\n value = getattr(cls, name, none)\\n if getattr(value, \"__isabstractmethod__\", false):\\n abstracts.add(name)\\n cls.__abstractmethods__ = frozenset(abstracts)\\n # set up inheritance registry\\n cls._abc_registry = weakset()\\n cls._abc_cache = weakset()\\n cls._abc_negative_cache = weakset()\\n cls._abc_negative_cache_version = abcmeta._abc_invalidation_counter\\n return cls\\n def register(cls, subclass):\\n \"\"\"register a virtual subclass of an abc.\\n returns the subclass, to allow usage as a class decorator.\\n \"\"\"\\n if not isinstance(subclass, type):\\n raise typeerror(\"can only register classes\")\\n if issubclass(subclass, cls):\\n return subclass  # already a subclass\\n # subtle: test for cycles *after* testing for \"already a subclass\";\\n # this means we allow x.register(x) and interpret it as a no-op.\\n if issubclass(cls, subclass):\\n # this would create a cycle, which is bad for the algorithm below\\n raise runtimeerror(\"refusing to create an inheritance cycle\")\\n cls._abc_registry.add(subclass)\\n abcmeta._abc_invalidation_counter += 1  # invalidate negative cache\\n return subclass\\n def _dump_registry(cls, file=none):\\n \"\"\"debug helper to print the abc registry.\"\"\"\\n print(\"class: %s.%s\" % (cls.__module__, cls.__qualname__), file=file)\\n print(\"inv.counter: %s\" % abcmeta._abc_invalidation_counter, file=file)\\n for name in sorted(cls.__dict__.keys()):\\n if name.startswith(\"_abc_\"):\\n value = getattr(cls, name)\\n print(\"%s: %r\" % (name, value), file=file)\\n def __instancecheck__(cls, instance):\\n \"\"\"override for isinstance(instance, cls).\"\"\"\\n # inline the cache checking\\n subclass = instance.__class__\\n if subclass in cls._abc_cache:\\n return true\\n subtype = type(instance)\\n if subtype is subclass:\\n if (cls._abc_negative_cache_version ==\\n abcmeta._abc_invalidation_counter and\\n subclass in cls._abc_negative_cache):\\n return false\\n # fall back to the subclass check.\\n return cls.__subclasscheck__(subclass)\\n return any(cls.__subclasscheck__(c) for c in {subclass, subtype})\\n def __subclasscheck__(cls, subclass):\\n \"\"\"override for issubclass(subclass, cls).\"\"\"\\n # check cache\\n if subclass in cls._abc_cache:\\n return true\\n # check negative cache; may have to invalidate\\n if cls._abc_negative_cache_version < abcmeta._abc_invalidation_counter:\\n # invalidate the negative cache\\n cls._abc_negative_cache = weakset()\\n cls._abc_negative_cache_version = abcmeta._abc_invalidation_counter\\n elif subclass in cls._abc_negative_cache:\\n return false\\n # check the subclass hook\\n ok = cls.__subclasshook__(subclass)\\n if ok is not notimplemented:\\n assert isinstance(ok, bool)\\n if ok:\\n cls._abc_cache.add(subclass)\\n else:\\n cls._abc_negative_cache.add(subclass)\\n return ok\\n # check if it\\'s a direct subclass\\n if cls in getattr(subclass, \\'__mro__\\', ()):\\n cls._abc_cache.add(subclass)\\n return true\\n # check if it\\'s a subclass of a registered class (recursive)\\n for rcls in cls._abc_registry:\\n if issubclass(subclass, rcls):\\n cls._abc_cache.add(subclass)\\n return true\\n # check if it\\'s a subclass of a subclass (recursive)\\n for scls in cls.__subclasses__():\\n if issubclass(subclass, scls):\\n cls._abc_cache.add(subclass)\\n return true\\n # no dice; update negative cache\\n cls._abc_negative_cache.add(subclass)\\n return false\\n class abc(metaclass=abcmeta):\\n \"\"\"helper class that provides a standard way to create an abc using\\n inheritance.\\n \"\"\"\\n pass\\n def get_cache_token():\\n \"\"\"returns the current abc cache token.\\n the token is an opaque object (supporting equality testing) identifying the\\n current version of the abc cache for virtual subclasses. the token changes\\n with every call to ``register()`` on any abc.\\n \"\"\"\\n return abcmeta._abc_invalidation_counter\\n #! /usr/bin/python3.5\\n \"\"\"base16, base32, base64 (rfc 3548), base85 and ascii85 data encodings\"\"\"\\n # modified 04-oct-1995 by jack jansen to use binascii module\\n # modified 30-dec-2003 by barry warsaw to add full rfc 3548 support\\n # modified 22-may-2007 by guido van rossum to use bytes everywhere\\n import re\\n import struct\\n import binascii\\n __all__ = [\\n # legacy interface exports traditional rfc 2045 base64 encodings\\n \\'encode\\', \\'decode\\', \\'encodebytes\\', \\'decodebytes\\',\\n # generalized interface for other encodings\\n \\'b64encode\\', \\'b64decode\\', \\'b32encode\\', \\'b32decode\\',\\n \\'b16encode\\', \\'b16decode\\',\\n # base85 and ascii85 encodings\\n \\'b85encode\\', \\'b85decode\\', \\'a85encode\\', \\'a85decode\\',\\n # standard base64 encoding\\n \\'standard_b64encode\\', \\'standard_b64decode\\',\\n # some common base64 alternatives.  as referenced by rfc 3458, see thread\\n # starting at:\\n #\\n # http://zgp.org/pipermail/p2p-hackers/2001-september/000316.html\\n \\'urlsafe_b64encode\\', \\'urlsafe_b64decode\\',\\n ]\\n bytes_types = (bytes, bytearray)  # types acceptable as binary data\\n def _bytes_from_decode_data(s):\\n if isinstance(s, str):\\n try:\\n return s.encode(\\'ascii\\')\\n except unicodeencodeerror:\\n raise valueerror(\\'string argument should contain only ascii characters\\')\\n if isinstance(s, bytes_types):\\n return s\\n try:\\n return memoryview(s).tobytes()\\n except typeerror:\\n raise typeerror(\"argument should be a bytes-like object or ascii \"\\n \"string, not %r\" % s.__class__.__name__) from none\\n # base64 encoding/decoding uses binascii\\n def b64encode(s, altchars=none):\\n \"\"\"encode the bytes-like object s using base64 and return a bytes object.\\n optional altchars should be a byte string of length 2 which specifies an\\n alternative alphabet for the \\'+\\' and \\'/\\' characters.  this allows an\\n application to e.g. generate url or filesystem safe base64 strings.\\n \"\"\"\\n # strip off the trailing newline\\n encoded = binascii.b2a_base64(s)[:-1]\\n if altchars is not none:\\n assert len(altchars) == 2, repr(altchars)\\n return encoded.translate(bytes.maketrans(b\\'+/\\', altchars))\\n return encoded\\n def b64decode(s, altchars=none, validate=false):\\n \"\"\"decode the base64 encoded bytes-like object or ascii string s.\\n optional altchars must be a bytes-like object or ascii string of length 2\\n which specifies the alternative alphabet used instead of the \\'+\\' and \\'/\\'\\n characters.\\n the result is returned as a bytes object.  a binascii.error is raised if\\n s is incorrectly padded.\\n if validate is false (the default), characters that are neither in the\\n normal base-64 alphabet nor the alternative alphabet are discarded prior\\n to the padding check.  if validate is true, these non-alphabet characters\\n in the input result in a binascii.error.\\n \"\"\"\\n s = _bytes_from_decode_data(s)\\n if altchars is not none:\\n altchars = _bytes_from_decode_data(altchars)\\n assert len(altchars) == 2, repr(altchars)\\n s = s.translate(bytes.maketrans(altchars, b\\'+/\\'))\\n if validate and not re.match(b\\'^[a-za-z0-9+/]*={0,2}$\\', s):\\n raise binascii.error(\\'non-base64 digit found\\')\\n return binascii.a2b_base64(s)\\n def standard_b64encode(s):\\n \"\"\"encode bytes-like object s using the standard base64 alphabet.\\n the result is returned as a bytes object.\\n \"\"\"\\n return b64encode(s)\\n def standard_b64decode(s):\\n \"\"\"decode bytes encoded with the standard base64 alphabet.\\n argument s is a bytes-like object or ascii string to decode.  the result\\n is returned as a bytes object.  a binascii.error is raised if the input\\n is incorrectly padded.  characters that are not in the standard alphabet\\n are discarded prior to the padding check.\\n \"\"\"\\n return b64decode(s)\\n _urlsafe_encode_translation = bytes.maketrans(b\\'+/\\', b\\'-_\\')\\n _urlsafe_decode_translation = bytes.maketrans(b\\'-_\\', b\\'+/\\')\\n def urlsafe_b64encode(s):\\n \"\"\"encode bytes using the url- and filesystem-safe base64 alphabet.\\n argument s is a bytes-like object to encode.  the result is returned as a\\n bytes object.  the alphabet uses \\'-\\' instead of \\'+\\' and \\'_\\' instead of\\n \\'/\\'.\\n \"\"\"\\n return b64encode(s).translate(_urlsafe_encode_translation)\\n def urlsafe_b64decode(s):\\n \"\"\"decode bytes using the url- and filesystem-safe base64 alphabet.\\n argument s is a bytes-like object or ascii string to decode.  the result\\n is returned as a bytes object.  a binascii.error is raised if the input\\n is incorrectly padded.  characters that are not in the url-safe base-64\\n alphabet, and are not a plus \\'+\\' or slash \\'/\\', are discarded prior to the\\n padding check.\\n the alphabet uses \\'-\\' instead of \\'+\\' and \\'_\\' instead of \\'/\\'.\\n \"\"\"\\n s = _bytes_from_decode_data(s)\\n s = s.translate(_urlsafe_decode_translation)\\n return b64decode(s)\\n # base32 encoding/decoding must be done in python\\n _b32alphabet = b\\'abcdefghijklmnopqrstuvwxyz234567\\'\\n _b32tab2 = none\\n _b32rev = none\\n def b32encode(s):\\n \"\"\"encode the bytes-like object s using base32 and return a bytes object.\\n \"\"\"\\n global _b32tab2\\n # delay the initialization of the table to not waste memory\\n # if the function is never called\\n if _b32tab2 is none:\\n b32tab = [bytes((i,)) for i in _b32alphabet]\\n _b32tab2 = [a + b for a in b32tab for b in b32tab]\\n b32tab = none\\n if not isinstance(s, bytes_types):\\n s = memoryview(s).tobytes()\\n leftover = len(s) % 5\\n # pad the last quantum with zero bits if necessary\\n if leftover:\\n s = s + bytes(5 - leftover)  # don\\'t use += !\\n encoded = bytearray()\\n from_bytes = int.from_bytes\\n b32tab2 = _b32tab2\\n for i in range(0, len(s), 5):\\n c = from_bytes(s[i: i + 5], \\'big\\')\\n encoded += (b32tab2[c >> 30] +           # bits 1 - 10\\n b32tab2[(c >> 20) & 0x3ff] + # bits 11 - 20\\n b32tab2[(c >> 10) & 0x3ff] + # bits 21 - 30\\n b32tab2[c & 0x3ff]           # bits 31 - 40\\n )\\n # adjust for any leftover partial quanta\\n if leftover == 1:\\n encoded[-6:] = b\\'======\\'\\n elif leftover == 2:\\n encoded[-4:] = b\\'====\\'\\n elif leftover == 3:\\n encoded[-3:] = b\\'===\\'\\n elif leftover == 4:\\n encoded[-1:] = b\\'=\\'\\n return bytes(encoded)\\n def b32decode(s, casefold=false, map01=none):\\n \"\"\"decode the base32 encoded bytes-like object or ascii string s.\\n optional casefold is a flag specifying whether a lowercase alphabet is\\n acceptable as input.  for security purposes, the default is false.\\n rfc 3548 allows for optional mapping of the digit 0 (zero) to the\\n letter o (oh), and for optional mapping of the digit 1 (one) to\\n either the letter i (eye) or letter l (el).  the optional argument\\n map01 when not none, specifies which letter the digit 1 should be\\n mapped to (when map01 is not none, the digit 0 is always mapped to\\n the letter o).  for security purposes the default is none, so that\\n 0 and 1 are not allowed in the input.\\n the result is returned as a bytes object.  a binascii.error is raised if\\n the input is incorrectly padded or if there are non-alphabet\\n characters present in the input.\\n \"\"\"\\n global _b32rev\\n # delay the initialization of the table to not waste memory\\n # if the function is never called\\n if _b32rev is none:\\n _b32rev = {v: k for k, v in enumerate(_b32alphabet)}\\n s = _bytes_from_decode_data(s)\\n if len(s) % 8:\\n raise binascii.error(\\'incorrect padding\\')\\n # handle section 2.4 zero and one mapping.  the flag map01 will be either\\n # false, or the character to map the digit 1 (one) to.  it should be\\n # either l (el) or i (eye).\\n if map01 is not none:\\n map01 = _bytes_from_decode_data(map01)\\n assert len(map01) == 1, repr(map01)\\n s = s.translate(bytes.maketrans(b\\'01\\', b\\'o\\' + map01))\\n if casefold:\\n s = s.upper()\\n # strip off pad characters from the right.  we need to count the pad\\n # characters because this will tell us how many null bytes to remove from\\n # the end of the decoded string.\\n l = len(s)\\n s = s.rstrip(b\\'=\\')\\n padchars = l - len(s)\\n # now decode the full quanta\\n decoded = bytearray()\\n b32rev = _b32rev\\n for i in range(0, len(s), 8):\\n quanta = s[i: i + 8]\\n acc = 0\\n try:\\n for c in quanta:\\n acc = (acc << 5) + b32rev[c]\\n except keyerror:\\n raise binascii.error(\\'non-base32 digit found\\') from none\\n decoded += acc.to_bytes(5, \\'big\\')\\n # process the last, partial quanta\\n if padchars:\\n acc <<= 5 * padchars\\n last = acc.to_bytes(5, \\'big\\')\\n if padchars == 1:\\n decoded[-5:] = last[:-1]\\n elif padchars == 3:\\n decoded[-5:] = last[:-2]\\n elif padchars == 4:\\n decoded[-5:] = last[:-3]\\n elif padchars == 6:\\n decoded[-5:] = last[:-4]\\n else:\\n raise binascii.error(\\'incorrect padding\\')\\n return bytes(decoded)\\n # rfc 3548, base 16 alphabet specifies uppercase, but hexlify() returns\\n # lowercase.  the rfc also recommends against accepting input case\\n # insensitively.\\n def b16encode(s):\\n \"\"\"encode the bytes-like object s using base16 and return a bytes object.\\n \"\"\"\\n return binascii.hexlify(s).upper()\\n def b16decode(s, casefold=false):\\n \"\"\"decode the base16 encoded bytes-like object or ascii string s.\\n optional casefold is a flag specifying whether a lowercase alphabet is\\n acceptable as input.  for security purposes, the default is false.\\n the result is returned as a bytes object.  a binascii.error is raised if\\n s is incorrectly padded or if there are non-alphabet characters present\\n in the input.\\n \"\"\"\\n s = _bytes_from_decode_data(s)\\n if casefold:\\n s = s.upper()\\n if re.search(b\\'[^0-9a-f]\\', s):\\n raise binascii.error(\\'non-base16 digit found\\')\\n return binascii.unhexlify(s)\\n #\\n # ascii85 encoding/decoding\\n #\\n _a85chars = none\\n _a85chars2 = none\\n _a85start = b\"<~\"\\n _a85end = b\"~>\"\\n def _85encode(b, chars, chars2, pad=false, foldnuls=false, foldspaces=false):\\n # helper function for a85encode and b85encode\\n if not isinstance(b, bytes_types):\\n b = memoryview(b).tobytes()\\n padding = (-len(b)) % 4\\n if padding:\\n b = b + b\\'\\\\0\\' * padding\\n words = struct.struct(\\'!%di\\' % (len(b) // 4)).unpack(b)\\n chunks = [b\\'z\\' if foldnuls and not word else\\n b\\'y\\' if foldspaces and word == 0x20202020 else\\n (chars2[word // 614125] +\\n chars2[word // 85 % 7225] +\\n chars[word % 85])\\n for word in words]\\n if padding and not pad:\\n if chunks[-1] == b\\'z\\':\\n chunks[-1] = chars[0] * 5\\n chunks[-1] = chunks[-1][:-padding]\\n return b\\'\\'.join(chunks)\\n def a85encode(b, *, foldspaces=false, wrapcol=0, pad=false, adobe=false):\\n \"\"\"encode bytes-like object b using ascii85 and return a bytes object.\\n foldspaces is an optional flag that uses the special short sequence \\'y\\'\\n instead of 4 consecutive spaces (ascii 0x20) as supported by \\'btoa\\'. this\\n feature is not supported by the \"standard\" adobe encoding.\\n wrapcol controls whether the output should have newline (b\\'\\\\\\\\n\\') characters\\n added to it. if this is non-zero, each output line will be at most this\\n many characters long.\\n pad controls whether the input is padded to a multiple of 4 before\\n encoding. note that the btoa implementation always pads.\\n adobe controls whether the encoded byte sequence is framed with <~ and ~>,\\n which is used by the adobe implementation.\\n \"\"\"\\n global _a85chars, _a85chars2\\n # delay the initialization of tables to not waste memory\\n # if the function is never called\\n if _a85chars is none:\\n _a85chars = [bytes((i,)) for i in range(33, 118)]\\n _a85chars2 = [(a + b) for a in _a85chars for b in _a85chars]\\n result = _85encode(b, _a85chars, _a85chars2, pad, true, foldspaces)\\n if adobe:\\n result = _a85start + result\\n if wrapcol:\\n wrapcol = max(2 if adobe else 1, wrapcol)\\n chunks = [result[i: i + wrapcol]\\n for i in range(0, len(result), wrapcol)]\\n if adobe:\\n if len(chunks[-1]) + 2 > wrapcol:\\n chunks.append(b\\'\\')\\n result = b\\'\\\\n\\'.join(chunks)\\n if adobe:\\n result += _a85end\\n return result\\n def a85decode(b, *, foldspaces=false, adobe=false, ignorechars=b\\' \\\\t\\\\n\\\\r\\\\v\\'):\\n \"\"\"decode the ascii85 encoded bytes-like object or ascii string b.\\n foldspaces is a flag that specifies whether the \\'y\\' short sequence should be\\n accepted as shorthand for 4 consecutive spaces (ascii 0x20). this feature is\\n not supported by the \"standard\" adobe encoding.\\n adobe controls whether the input sequence is in adobe ascii85 format (i.e.\\n is framed with <~ and ~>).\\n ignorechars should be a byte string containing characters to ignore from the\\n input. this should only contain whitespace characters, and by default\\n contains all whitespace characters in ascii.\\n the result is returned as a bytes object.\\n \"\"\"\\n b = _bytes_from_decode_data(b)\\n if adobe:\\n if not b.endswith(_a85end):\\n raise valueerror(\\n \"ascii85 encoded byte sequences must end \"\\n \"with {!r}\".format(_a85end)\\n )\\n if b.startswith(_a85start):\\n b = b[2:-2]  # strip off start/end markers\\n else:\\n b = b[:-2]\\n #\\n # we have to go through this stepwise, so as to ignore spaces and handle\\n # special short sequences\\n #\\n packi = struct.struct(\\'!i\\').pack\\n decoded = []\\n decoded_append = decoded.append\\n curr = []\\n curr_append = curr.append\\n curr_clear = curr.clear\\n for x in b + b\\'u\\' * 4:\\n if b\\'!\\'[0] <= x <= b\\'u\\'[0]:\\n curr_append(x)\\n if len(curr) == 5:\\n acc = 0\\n for x in curr:\\n acc = 85 * acc + (x - 33)\\n try:\\n decoded_append(packi(acc))\\n except struct.error:\\n raise valueerror(\\'ascii85 overflow\\') from none\\n curr_clear()\\n elif x == b\\'z\\'[0]:\\n if curr:\\n raise valueerror(\\'z inside ascii85 5-tuple\\')\\n decoded_append(b\\'\\\\0\\\\0\\\\0\\\\0\\')\\n elif foldspaces and x == b\\'y\\'[0]:\\n if curr:\\n raise valueerror(\\'y inside ascii85 5-tuple\\')\\n decoded_append(b\\'\\\\x20\\\\x20\\\\x20\\\\x20\\')\\n elif x in ignorechars:\\n # skip whitespace\\n continue\\n else:\\n raise valueerror(\\'non-ascii85 digit found: %c\\' % x)\\n result = b\\'\\'.join(decoded)\\n padding = 4 - len(curr)\\n if padding:\\n # throw away the extra padding\\n result = result[:-padding]\\n return result\\n # the following code is originally taken (with permission) from mercurial\\n _b85alphabet = (b\"0123456789abcdefghijklmnopqrstuvwxyz\"\\n b\"abcdefghijklmnopqrstuvwxyz!#$%&()*+-;<=>?@^_`{|}~\")\\n _b85chars = none\\n _b85chars2 = none\\n _b85dec = none\\n def b85encode(b, pad=false):\\n \"\"\"encode bytes-like object b in base85 format and return a bytes object.\\n if pad is true, the input is padded with b\\'\\\\\\\\0\\' so its length is a multiple of\\n 4 bytes before encoding.\\n \"\"\"\\n global _b85chars, _b85chars2\\n # delay the initialization of tables to not waste memory\\n # if the function is never called\\n if _b85chars is none:\\n _b85chars = [bytes((i,)) for i in _b85alphabet]\\n _b85chars2 = [(a + b) for a in _b85chars for b in _b85chars]\\n return _85encode(b, _b85chars, _b85chars2, pad)\\n def b85decode(b):\\n \"\"\"decode the base85-encoded bytes-like object or ascii string b\\n the result is returned as a bytes object.\\n \"\"\"\\n global _b85dec\\n # delay the initialization of tables to not waste memory\\n # if the function is never called\\n if _b85dec is none:\\n _b85dec = [none] * 256\\n for i, c in enumerate(_b85alphabet):\\n _b85dec[c] = i\\n b = _bytes_from_decode_data(b)\\n padding = (-len(b)) % 5\\n b = b + b\\'~\\' * padding\\n out = []\\n packi = struct.struct(\\'!i\\').pack\\n for i in range(0, len(b), 5):\\n chunk = b[i:i + 5]\\n acc = 0\\n try:\\n for c in chunk:\\n acc = acc * 85 + _b85dec[c]\\n except typeerror:\\n for j, c in enumerate(chunk):\\n if _b85dec[c] is none:\\n raise valueerror(\\'bad base85 character at position %d\\'\\n % (i + j)) from none\\n raise\\n try:\\n out.append(packi(acc))\\n except struct.error:\\n raise valueerror(\\'base85 overflow in hunk starting at byte %d\\'\\n % i) from none\\n result = b\\'\\'.join(out)\\n if padding:\\n result = result[:-padding]\\n return result\\n # legacy interface.  this code could be cleaned up since i don\\'t believe\\n # binascii has any line length limitations.  it just doesn\\'t seem worth it\\n # though.  the files should be opened in binary mode.\\n maxlinesize = 76 # excluding the crlf\\n maxbinsize = (maxlinesize//4)*3\\n def encode(input, output):\\n \"\"\"encode a file; input and output are binary files.\"\"\"\\n while true:\\n s = input.read(maxbinsize)\\n if not s:\\n break\\n while len(s) < maxbinsize:\\n ns = input.read(maxbinsize-len(s))\\n if not ns:\\n break\\n s += ns\\n line = binascii.b2a_base64(s)\\n output.write(line)\\n def decode(input, output):\\n \"\"\"decode a file; input and output are binary files.\"\"\"\\n while true:\\n line = input.readline()\\n if not line:\\n break\\n s = binascii.a2b_base64(line)\\n output.write(s)\\n def _input_type_check(s):\\n try:\\n m = memoryview(s)\\n except typeerror as err:\\n msg = \"expected bytes-like object, not %s\" % s.__class__.__name__\\n raise typeerror(msg) from err\\n if m.format not in (\\'c\\', \\'b\\', \\'b\\'):\\n msg = (\"expected single byte elements, not %r from %s\" %\\n (m.format, s.__class__.__name__))\\n raise typeerror(msg)\\n if m.ndim != 1:\\n msg = (\"expected 1-d data, not %d-d data from %s\" %\\n (m.ndim, s.__class__.__name__))\\n raise typeerror(msg)\\n def encodebytes(s):\\n \"\"\"encode a bytestring into a bytes object containing multiple lines\\n of base-64 data.\"\"\"\\n _input_type_check(s)\\n pieces = []\\n for i in range(0, len(s), maxbinsize):\\n chunk = s[i : i + maxbinsize]\\n pieces.append(binascii.b2a_base64(chunk))\\n return b\"\".join(pieces)\\n def encodestring(s):\\n \"\"\"legacy alias of encodebytes().\"\"\"\\n import warnings\\n warnings.warn(\"encodestring() is a deprecated alias, use encodebytes()\",\\n deprecationwarning, 2)\\n return encodebytes(s)\\n def decodebytes(s):\\n \"\"\"decode a bytestring of base-64 data into a bytes object.\"\"\"\\n _input_type_check(s)\\n return binascii.a2b_base64(s)\\n def decodestring(s):\\n \"\"\"legacy alias of decodebytes().\"\"\"\\n import warnings\\n warnings.warn(\"decodestring() is a deprecated alias, use decodebytes()\",\\n deprecationwarning, 2)\\n return decodebytes(s)\\n # usable as a script...\\n def main():\\n \"\"\"small main program\"\"\"\\n import sys, getopt\\n try:\\n opts, args = getopt.getopt(sys.argv[1:], \\'deut\\')\\n except getopt.error as msg:\\n sys.stdout = sys.stderr\\n print(msg)\\n print(\"\"\"usage: %s [-d|-e|-u|-t] [file|-]\\n -d, -u: decode\\n -e: encode (default)\\n -t: encode and decode string \\'aladdin:open sesame\\'\"\"\"%sys.argv[0])\\n sys.exit(2)\\n func = encode\\n for o, a in opts:\\n if o == \\'-e\\': func = encode\\n if o == \\'-d\\': func = decode\\n if o == \\'-u\\': func = decode\\n if o == \\'-t\\': test(); return\\n if args and args[0] != \\'-\\':\\n with open(args[0], \\'rb\\') as f:\\n func(f, sys.stdout.buffer)\\n else:\\n func(sys.stdin.buffer, sys.stdout.buffer)\\n def test():\\n s0 = b\"aladdin:open sesame\"\\n print(repr(s0))\\n s1 = encodebytes(s0)\\n print(repr(s1))\\n s2 = decodebytes(s1)\\n print(repr(s2))\\n assert s0 == s2\\n if __name__ == \\'__main__\\':\\n main()\\n \"\"\"bisection algorithms.\"\"\"\\n def insort_right(a, x, lo=0, hi=none):\\n \"\"\"insert item x in list a, and keep it sorted assuming a is sorted.\\n if x is already in a, insert it to the right of the rightmost x.\\n optional args lo (default 0) and hi (default len(a)) bound the\\n slice of a to be searched.\\n \"\"\"\\n if lo < 0:\\n raise valueerror(\\'lo must be non-negative\\')\\n if hi is none:\\n hi = len(a)\\n while lo < hi:\\n mid = (lo+hi)//2\\n if x < a[mid]: hi = mid\\n else: lo = mid+1\\n a.insert(lo, x)\\n insort = insort_right   # backward compatibility\\n def bisect_right(a, x, lo=0, hi=none):\\n \"\"\"return the index where to insert item x in list a, assuming a is sorted.\\n the return value i is such that all e in a[:i] have e <= x, and all e in\\n a[i:] have e > x.  so if x already appears in the list, a.insert(x) will\\n insert just after the rightmost x already there.\\n optional args lo (default 0) and hi (default len(a)) bound the\\n slice of a to be searched.\\n \"\"\"\\n if lo < 0:\\n raise valueerror(\\'lo must be non-negative\\')\\n if hi is none:\\n hi = len(a)\\n while lo < hi:\\n mid = (lo+hi)//2\\n if x < a[mid]: hi = mid\\n else: lo = mid+1\\n return lo\\n bisect = bisect_right   # backward compatibility\\n def insort_left(a, x, lo=0, hi=none):\\n \"\"\"insert item x in list a, and keep it sorted assuming a is sorted.\\n if x is already in a, insert it to the left of the leftmost x.\\n optional args lo (default 0) and hi (default len(a)) bound the\\n slice of a to be searched.\\n \"\"\"\\n if lo < 0:\\n raise valueerror(\\'lo must be non-negative\\')\\n if hi is none:\\n hi = len(a)\\n while lo < hi:\\n mid = (lo+hi)//2\\n if a[mid] < x: lo = mid+1\\n else: hi = mid\\n a.insert(lo, x)\\n def bisect_left(a, x, lo=0, hi=none):\\n \"\"\"return the index where to insert item x in list a, assuming a is sorted.\\n the return value i is such that all e in a[:i] have e < x, and all e in\\n a[i:] have e >= x.  so if x already appears in the list, a.insert(x) will\\n insert just before the leftmost x already there.\\n optional args lo (default 0) and hi (default len(a)) bound the\\n slice of a to be searched.\\n \"\"\"\\n if lo < 0:\\n raise valueerror(\\'lo must be non-negative\\')\\n if hi is none:\\n hi = len(a)\\n while lo < hi:\\n mid = (lo+hi)//2\\n if a[mid] < x: lo = mid+1\\n else: hi = mid\\n return lo\\n # overwrite above definitions with a fast c implementation\\n try:\\n from _bisect import *\\n except importerror:\\n pass\\n \"\"\"a minimal subset of the locale module used at interpreter startup\\n (imported by the _io module), in order to reduce startup time.\\n don\\'t import directly from third-party code; use the `locale` module instead!\\n \"\"\"\\n import sys\\n import _locale\\n if sys.platform.startswith(\"win\"):\\n def getpreferredencoding(do_setlocale=true):\\n return _locale._getdefaultlocale()[1]\\n else:\\n try:\\n _locale.codeset\\n except attributeerror:\\n def getpreferredencoding(do_setlocale=true):\\n # this path for legacy systems needs the more complex\\n # getdefaultlocale() function, import the full locale module.\\n import locale\\n return locale.getpreferredencoding(do_setlocale)\\n else:\\n def getpreferredencoding(do_setlocale=true):\\n assert not do_setlocale\\n result = _locale.nl_langinfo(_locale.codeset)\\n if not result and sys.platform == \\'darwin\\':\\n # nl_langinfo can return an empty string\\n # when the setting has an invalid value.\\n # default to utf-8 in that case because\\n # utf-8 is the default charset on osx and\\n # returning nothing will crash the\\n # interpreter.\\n result = \\'utf-8\\'\\n return result\\n \"\"\" codecs -- python codec registry, api and helpers.\\n written by marc-andre lemburg (mal@lemburg.com).\\n (c) copyright cnri, all rights reserved. no warranty.\\n \"\"\"#\"\\n import builtins, sys\\n ### registry and builtin stateless codec functions\\n try:\\n from _codecs import *\\n except importerror as why:\\n raise systemerror(\\'failed to load the builtin codecs: %s\\' % why)\\n __all__ = [\"register\", \"lookup\", \"open\", \"encodedfile\", \"bom\", \"bom_be\",\\n \"bom_le\", \"bom32_be\", \"bom32_le\", \"bom64_be\", \"bom64_le\",\\n \"bom_utf8\", \"bom_utf16\", \"bom_utf16_le\", \"bom_utf16_be\",\\n \"bom_utf32\", \"bom_utf32_le\", \"bom_utf32_be\",\\n \"codecinfo\", \"codec\", \"incrementalencoder\", \"incrementaldecoder\",\\n \"streamreader\", \"streamwriter\",\\n \"streamreaderwriter\", \"streamrecoder\",\\n \"getencoder\", \"getdecoder\", \"getincrementalencoder\",\\n \"getincrementaldecoder\", \"getreader\", \"getwriter\",\\n \"encode\", \"decode\", \"iterencode\", \"iterdecode\",\\n \"strict_errors\", \"ignore_errors\", \"replace_errors\",\\n \"xmlcharrefreplace_errors\",\\n \"backslashreplace_errors\", \"namereplace_errors\",\\n \"register_error\", \"lookup_error\"]\\n ### constants\\n #\\n # byte order mark (bom = zero width no-break space = u+feff)\\n # and its possible byte string values\\n # for utf8/utf16/utf32 output and little/big endian machines\\n #\\n # utf-8\\n bom_utf8 = b\\'\\\\xef\\\\xbb\\\\xbf\\'\\n # utf-16, little endian\\n bom_le = bom_utf16_le = b\\'\\\\xff\\\\xfe\\'\\n # utf-16, big endian\\n bom_be = bom_utf16_be = b\\'\\\\xfe\\\\xff\\'\\n # utf-32, little endian\\n bom_utf32_le = b\\'\\\\xff\\\\xfe\\\\x00\\\\x00\\'\\n # utf-32, big endian\\n bom_utf32_be = b\\'\\\\x00\\\\x00\\\\xfe\\\\xff\\'\\n if sys.byteorder == \\'little\\':\\n # utf-16, native endianness\\n bom = bom_utf16 = bom_utf16_le\\n # utf-32, native endianness\\n bom_utf32 = bom_utf32_le\\n else:\\n # utf-16, native endianness\\n bom = bom_utf16 = bom_utf16_be\\n # utf-32, native endianness\\n bom_utf32 = bom_utf32_be\\n # old broken names (don\\'t use in new code)\\n bom32_le = bom_utf16_le\\n bom32_be = bom_utf16_be\\n bom64_le = bom_utf32_le\\n bom64_be = bom_utf32_be\\n ### codec base classes (defining the api)\\n class codecinfo(tuple):\\n \"\"\"codec details when looking up the codec registry\"\"\"\\n # private api to allow python 3.4 to blacklist the known non-unicode\\n # codecs in the standard library. a more general mechanism to\\n # reliably distinguish test encodings from other codecs will hopefully\\n # be defined for python 3.5\\n #\\n # see http://bugs.python.org/issue19619\\n _is_text_encoding = true # assume codecs are text encodings by default\\n def __new__(cls, encode, decode, streamreader=none, streamwriter=none,\\n incrementalencoder=none, incrementaldecoder=none, name=none,\\n *, _is_text_encoding=none):\\n self = tuple.__new__(cls, (encode, decode, streamreader, streamwriter))\\n self.name = name\\n self.encode = encode\\n self.decode = decode\\n self.incrementalencoder = incrementalencoder\\n self.incrementaldecoder = incrementaldecoder\\n self.streamwriter = streamwriter\\n self.streamreader = streamreader\\n if _is_text_encoding is not none:\\n self._is_text_encoding = _is_text_encoding\\n return self\\n def __repr__(self):\\n return \"<%s.%s object for encoding %s at %#x>\" % \\\\\\n (self.__class__.__module__, self.__class__.__qualname__,\\n self.name, id(self))\\n class codec:\\n \"\"\" defines the interface for stateless encoders/decoders.\\n the .encode()/.decode() methods may use different error\\n handling schemes by providing the errors argument. these\\n string values are predefined:\\n \\'strict\\' - raise a valueerror error (or a subclass)\\n \\'ignore\\' - ignore the character and continue with the next\\n \\'replace\\' - replace with a suitable replacement character;\\n python will use the official u+fffd replacement\\n character for the builtin unicode codecs on\\n decoding and \\'?\\' on encoding.\\n \\'surrogateescape\\' - replace with private code points u+dcnn.\\n \\'xmlcharrefreplace\\' - replace with the appropriate xml\\n character reference (only for encoding).\\n \\'backslashreplace\\'  - replace with backslashed escape sequences.\\n \\'namereplace\\'       - replace with \\\\\\\\n{...} escape sequences\\n (only for encoding).\\n the set of allowed values can be extended via register_error.\\n \"\"\"\\n def encode(self, input, errors=\\'strict\\'):\\n \"\"\" encodes the object input and returns a tuple (output\\n object, length consumed).\\n errors defines the error handling to apply. it defaults to\\n \\'strict\\' handling.\\n the method may not store state in the codec instance. use\\n streamwriter for codecs which have to keep state in order to\\n make encoding efficient.\\n the encoder must be able to handle zero length input and\\n return an empty object of the output object type in this\\n situation.\\n \"\"\"\\n raise notimplementederror\\n def decode(self, input, errors=\\'strict\\'):\\n \"\"\" decodes the object input and returns a tuple (output\\n object, length consumed).\\n input must be an object which provides the bf_getreadbuf\\n buffer slot. python strings, buffer objects and memory\\n mapped files are examples of objects providing this slot.\\n errors defines the error handling to apply. it defaults to\\n \\'strict\\' handling.\\n the method may not store state in the codec instance. use\\n streamreader for codecs which have to keep state in order to\\n make decoding efficient.\\n the decoder must be able to handle zero length input and\\n return an empty object of the output object type in this\\n situation.\\n \"\"\"\\n raise notimplementederror\\n class incrementalencoder(object):\\n \"\"\"\\n an incrementalencoder encodes an input in multiple steps. the input can\\n be passed piece by piece to the encode() method. the incrementalencoder\\n remembers the state of the encoding process between calls to encode().\\n \"\"\"\\n def __init__(self, errors=\\'strict\\'):\\n \"\"\"\\n creates an incrementalencoder instance.\\n the incrementalencoder may use different error handling schemes by\\n providing the errors keyword argument. see the module docstring\\n for a list of possible values.\\n \"\"\"\\n self.errors = errors\\n self.buffer = \"\"\\n def encode(self, input, final=false):\\n \"\"\"\\n encodes input and returns the resulting object.\\n \"\"\"\\n raise notimplementederror\\n def reset(self):\\n \"\"\"\\n resets the encoder to the initial state.\\n \"\"\"\\n def getstate(self):\\n \"\"\"\\n return the current state of the encoder.\\n \"\"\"\\n return 0\\n def setstate(self, state):\\n \"\"\"\\n set the current state of the encoder. state must have been\\n returned by getstate().\\n \"\"\"\\n class bufferedincrementalencoder(incrementalencoder):\\n \"\"\"\\n this subclass of incrementalencoder can be used as the baseclass for an\\n incremental encoder if the encoder must keep some of the output in a\\n buffer between calls to encode().\\n \"\"\"\\n def __init__(self, errors=\\'strict\\'):\\n incrementalencoder.__init__(self, errors)\\n # unencoded input that is kept between calls to encode()\\n self.buffer = \"\"\\n def _buffer_encode(self, input, errors, final):\\n # overwrite this method in subclasses: it must encode input\\n # and return an (output, length consumed) tuple\\n raise notimplementederror\\n def encode(self, input, final=false):\\n # encode input (taking the buffer into account)\\n data = self.buffer + input\\n (result, consumed) = self._buffer_encode(data, self.errors, final)\\n # keep unencoded input until the next call\\n self.buffer = data[consumed:]\\n return result\\n def reset(self):\\n incrementalencoder.reset(self)\\n self.buffer = \"\"\\n def getstate(self):\\n return self.buffer or 0\\n def setstate(self, state):\\n self.buffer = state or \"\"\\n class incrementaldecoder(object):\\n \"\"\"\\n an incrementaldecoder decodes an input in multiple steps. the input can\\n be passed piece by piece to the decode() method. the incrementaldecoder\\n remembers the state of the decoding process between calls to decode().\\n \"\"\"\\n def __init__(self, errors=\\'strict\\'):\\n \"\"\"\\n create an incrementaldecoder instance.\\n the incrementaldecoder may use different error handling schemes by\\n providing the errors keyword argument. see the module docstring\\n for a list of possible values.\\n \"\"\"\\n self.errors = errors\\n def decode(self, input, final=false):\\n \"\"\"\\n decode input and returns the resulting object.\\n \"\"\"\\n raise notimplementederror\\n def reset(self):\\n \"\"\"\\n reset the decoder to the initial state.\\n \"\"\"\\n def getstate(self):\\n \"\"\"\\n return the current state of the decoder.\\n this must be a (buffered_input, additional_state_info) tuple.\\n buffered_input must be a bytes object containing bytes that\\n were passed to decode() that have not yet been converted.\\n additional_state_info must be a non-negative integer\\n representing the state of the decoder without yet having\\n processed the contents of buffered_input.  in the initial state\\n and after reset(), getstate() must return (b\"\", 0).\\n \"\"\"\\n return (b\"\", 0)\\n def setstate(self, state):\\n \"\"\"\\n set the current state of the decoder.\\n state must have been returned by getstate().  the effect of\\n setstate((b\"\", 0)) must be equivalent to reset().\\n \"\"\"\\n class bufferedincrementaldecoder(incrementaldecoder):\\n \"\"\"\\n this subclass of incrementaldecoder can be used as the baseclass for an\\n incremental decoder if the decoder must be able to handle incomplete\\n byte sequences.\\n \"\"\"\\n def __init__(self, errors=\\'strict\\'):\\n incrementaldecoder.__init__(self, errors)\\n # undecoded input that is kept between calls to decode()\\n self.buffer = b\"\"\\n def _buffer_decode(self, input, errors, final):\\n # overwrite this method in subclasses: it must decode input\\n # and return an (output, length consumed) tuple\\n raise notimplementederror\\n def decode(self, input, final=false):\\n # decode input (taking the buffer into account)\\n data = self.buffer + input\\n (result, consumed) = self._buffer_decode(data, self.errors, final)\\n # keep undecoded input until the next call\\n self.buffer = data[consumed:]\\n return result\\n def reset(self):\\n incrementaldecoder.reset(self)\\n self.buffer = b\"\"\\n def getstate(self):\\n # additional state info is always 0\\n return (self.buffer, 0)\\n def setstate(self, state):\\n # ignore additional state info\\n self.buffer = state[0]\\n #\\n # the streamwriter and streamreader class provide generic working\\n # interfaces which can be used to implement new encoding submodules\\n # very easily. see encodings/utf_8.py for an example on how this is\\n # done.\\n #\\n class streamwriter(codec):\\n def __init__(self, stream, errors=\\'strict\\'):\\n \"\"\" creates a streamwriter instance.\\n stream must be a file-like object open for writing.\\n the streamwriter may use different error handling\\n schemes by providing the errors keyword argument. these\\n parameters are predefined:\\n \\'strict\\' - raise a valueerror (or a subclass)\\n \\'ignore\\' - ignore the character and continue with the next\\n \\'replace\\'- replace with a suitable replacement character\\n \\'xmlcharrefreplace\\' - replace with the appropriate xml\\n character reference.\\n \\'backslashreplace\\'  - replace with backslashed escape\\n sequences.\\n \\'namereplace\\'       - replace with \\\\\\\\n{...} escape sequences.\\n the set of allowed parameter values can be extended via\\n register_error.\\n \"\"\"\\n self.stream = stream\\n self.errors = errors\\n def write(self, object):\\n \"\"\" writes the object\\'s contents encoded to self.stream.\\n \"\"\"\\n data, consumed = self.encode(object, self.errors)\\n self.stream.write(data)\\n def writelines(self, list):\\n \"\"\" writes the concatenated list of strings to the stream\\n using .write().\\n \"\"\"\\n self.write(\\'\\'.join(list))\\n def reset(self):\\n \"\"\" flushes and resets the codec buffers used for keeping state.\\n calling this method should ensure that the data on the\\n output is put into a clean state, that allows appending\\n of new fresh data without having to rescan the whole\\n stream to recover state.\\n \"\"\"\\n pass\\n def seek(self, offset, whence=0):\\n self.stream.seek(offset, whence)\\n if whence == 0 and offset == 0:\\n self.reset()\\n def __getattr__(self, name,\\n getattr=getattr):\\n \"\"\" inherit all other methods from the underlying stream.\\n \"\"\"\\n return getattr(self.stream, name)\\n def __enter__(self):\\n return self\\n def __exit__(self, type, value, tb):\\n self.stream.close()\\n ###\\n class streamreader(codec):\\n charbuffertype = str\\n def __init__(self, stream, errors=\\'strict\\'):\\n \"\"\" creates a streamreader instance.\\n stream must be a file-like object open for reading.\\n the streamreader may use different error handling\\n schemes by providing the errors keyword argument. these\\n parameters are predefined:\\n \\'strict\\' - raise a valueerror (or a subclass)\\n \\'ignore\\' - ignore the character and continue with the next\\n \\'replace\\'- replace with a suitable replacement character\\n \\'backslashreplace\\' - replace with backslashed escape sequences;\\n the set of allowed parameter values can be extended via\\n register_error.\\n \"\"\"\\n self.stream = stream\\n self.errors = errors\\n self.bytebuffer = b\"\"\\n self._empty_charbuffer = self.charbuffertype()\\n self.charbuffer = self._empty_charbuffer\\n self.linebuffer = none\\n def decode(self, input, errors=\\'strict\\'):\\n raise notimplementederror\\n def read(self, size=-1, chars=-1, firstline=false):\\n \"\"\" decodes data from the stream self.stream and returns the\\n resulting object.\\n chars indicates the number of decoded code points or bytes to\\n return. read() will never return more data than requested,\\n but it might return less, if there is not enough available.\\n size indicates the approximate maximum number of decoded\\n bytes or code points to read for decoding. the decoder\\n can modify this setting as appropriate. the default value\\n -1 indicates to read and decode as much as possible.  size\\n is intended to prevent having to decode huge files in one\\n step.\\n if firstline is true, and a unicodedecodeerror happens\\n after the first line terminator in the input only the first line\\n will be returned, the rest of the input will be kept until the\\n next call to read().\\n the method should use a greedy read strategy, meaning that\\n it should read as much data as is allowed within the\\n definition of the encoding and the given size, e.g.  if\\n optional encoding endings or state markers are available\\n on the stream, these should be read too.\\n \"\"\"\\n # if we have lines cached, first merge them back into characters\\n if self.linebuffer:\\n self.charbuffer = self._empty_charbuffer.join(self.linebuffer)\\n self.linebuffer = none\\n # read until we get the required number of characters (if available)\\n while true:\\n # can the request be satisfied from the character buffer?\\n if chars >= 0:\\n if len(self.charbuffer) >= chars:\\n break\\n elif size >= 0:\\n if len(self.charbuffer) >= size:\\n break\\n # we need more data\\n if size < 0:\\n newdata = self.stream.read()\\n else:\\n newdata = self.stream.read(size)\\n # decode bytes (those remaining from the last call included)\\n data = self.bytebuffer + newdata\\n if not data:\\n break\\n try:\\n newchars, decodedbytes = self.decode(data, self.errors)\\n except unicodedecodeerror as exc:\\n if firstline:\\n newchars, decodedbytes = \\\\\\n self.decode(data[:exc.start], self.errors)\\n lines = newchars.splitlines(keepends=true)\\n if len(lines)<=1:\\n raise\\n else:\\n raise\\n # keep undecoded bytes until the next call\\n self.bytebuffer = data[decodedbytes:]\\n # put new characters in the character buffer\\n self.charbuffer += newchars\\n # there was no data available\\n if not newdata:\\n break\\n if chars < 0:\\n # return everything we\\'ve got\\n result = self.charbuffer\\n self.charbuffer = self._empty_charbuffer\\n else:\\n # return the first chars characters\\n result = self.charbuffer[:chars]\\n self.charbuffer = self.charbuffer[chars:]\\n return result\\n def readline(self, size=none, keepends=true):\\n \"\"\" read one line from the input stream and return the\\n decoded data.\\n size, if given, is passed as size argument to the\\n read() method.\\n \"\"\"\\n # if we have lines cached from an earlier read, return\\n # them unconditionally\\n if self.linebuffer:\\n line = self.linebuffer[0]\\n del self.linebuffer[0]\\n if len(self.linebuffer) == 1:\\n # revert to charbuffer mode; we might need more data\\n # next time\\n self.charbuffer = self.linebuffer[0]\\n self.linebuffer = none\\n if not keepends:\\n line = line.splitlines(keepends=false)[0]\\n return line\\n readsize = size or 72\\n line = self._empty_charbuffer\\n # if size is given, we call read() only once\\n while true:\\n data = self.read(readsize, firstline=true)\\n if data:\\n # if we\\'re at a \"\\\\r\" read one extra character (which might\\n # be a \"\\\\n\") to get a proper line ending. if the stream is\\n # temporarily exhausted we return the wrong line ending.\\n if (isinstance(data, str) and data.endswith(\"\\\\r\")) or \\\\\\n (isinstance(data, bytes) and data.endswith(b\"\\\\r\")):\\n data += self.read(size=1, chars=1)\\n line += data\\n lines = line.splitlines(keepends=true)\\n if lines:\\n if len(lines) > 1:\\n # more than one line result; the first line is a full line\\n # to return\\n line = lines[0]\\n del lines[0]\\n if len(lines) > 1:\\n # cache the remaining lines\\n lines[-1] += self.charbuffer\\n self.linebuffer = lines\\n self.charbuffer = none\\n else:\\n # only one remaining line, put it back into charbuffer\\n self.charbuffer = lines[0] + self.charbuffer\\n if not keepends:\\n line = line.splitlines(keepends=false)[0]\\n break\\n line0withend = lines[0]\\n line0withoutend = lines[0].splitlines(keepends=false)[0]\\n if line0withend != line0withoutend: # we really have a line end\\n # put the rest back together and keep it until the next call\\n self.charbuffer = self._empty_charbuffer.join(lines[1:]) + \\\\\\n self.charbuffer\\n if keepends:\\n line = line0withend\\n else:\\n line = line0withoutend\\n break\\n # we didn\\'t get anything or this was our only try\\n if not data or size is not none:\\n if line and not keepends:\\n line = line.splitlines(keepends=false)[0]\\n break\\n if readsize < 8000:\\n readsize *= 2\\n return line\\n def readlines(self, sizehint=none, keepends=true):\\n \"\"\" read all lines available on the input stream\\n and return them as a list.\\n line breaks are implemented using the codec\\'s decoder\\n method and are included in the list entries.\\n sizehint, if given, is ignored since there is no efficient\\n way to finding the true end-of-line.\\n \"\"\"\\n data = self.read()\\n return data.splitlines(keepends)\\n def reset(self):\\n \"\"\" resets the codec buffers used for keeping state.\\n note that no stream repositioning should take place.\\n this method is primarily intended to be able to recover\\n from decoding errors.\\n \"\"\"\\n self.bytebuffer = b\"\"\\n self.charbuffer = self._empty_charbuffer\\n self.linebuffer = none\\n def seek(self, offset, whence=0):\\n \"\"\" set the input stream\\'s current position.\\n resets the codec buffers used for keeping state.\\n \"\"\"\\n self.stream.seek(offset, whence)\\n self.reset()\\n def __next__(self):\\n \"\"\" return the next decoded line from the input stream.\"\"\"\\n line = self.readline()\\n if line:\\n return line\\n raise stopiteration\\n def __iter__(self):\\n return self\\n def __getattr__(self, name,\\n getattr=getattr):\\n \"\"\" inherit all other methods from the underlying stream.\\n \"\"\"\\n return getattr(self.stream, name)\\n def __enter__(self):\\n return self\\n def __exit__(self, type, value, tb):\\n self.stream.close()\\n ###\\n class streamreaderwriter:\\n \"\"\" streamreaderwriter instances allow wrapping streams which\\n work in both read and write modes.\\n the design is such that one can use the factory functions\\n returned by the codec.lookup() function to construct the\\n instance.\\n \"\"\"\\n # optional attributes set by the file wrappers below\\n encoding = \\'unknown\\'\\n def __init__(self, stream, reader, writer, errors=\\'strict\\'):\\n \"\"\" creates a streamreaderwriter instance.\\n stream must be a stream-like object.\\n reader, writer must be factory functions or classes\\n providing the streamreader, streamwriter interface resp.\\n error handling is done in the same way as defined for the\\n streamwriter/readers.\\n \"\"\"\\n self.stream = stream\\n self.reader = reader(stream, errors)\\n self.writer = writer(stream, errors)\\n self.errors = errors\\n def read(self, size=-1):\\n return self.reader.read(size)\\n def readline(self, size=none):\\n return self.reader.readline(size)\\n def readlines(self, sizehint=none):\\n return self.reader.readlines(sizehint)\\n def __next__(self):\\n \"\"\" return the next decoded line from the input stream.\"\"\"\\n return next(self.reader)\\n def __iter__(self):\\n return self\\n def write(self, data):\\n return self.writer.write(data)\\n def writelines(self, list):\\n return self.writer.writelines(list)\\n def reset(self):\\n self.reader.reset()\\n self.writer.reset()\\n def seek(self, offset, whence=0):\\n self.stream.seek(offset, whence)\\n self.reader.reset()\\n if whence == 0 and offset == 0:\\n self.writer.reset()\\n def __getattr__(self, name,\\n getattr=getattr):\\n \"\"\" inherit all other methods from the underlying stream.\\n \"\"\"\\n return getattr(self.stream, name)\\n # these are needed to make \"with codecs.open(...)\" work properly\\n def __enter__(self):\\n return self\\n def __exit__(self, type, value, tb):\\n self.stream.close()\\n ###\\n class streamrecoder:\\n \"\"\" streamrecoder instances translate data from one encoding to another.\\n they use the complete set of apis returned by the\\n codecs.lookup() function to implement their task.\\n data written to the streamrecoder is first decoded into an\\n intermediate format (depending on the \"decode\" codec) and then\\n written to the underlying stream using an instance of the provided\\n writer class.\\n in the other direction, data is read from the underlying stream using\\n a reader instance and then encoded and returned to the caller.\\n \"\"\"\\n # optional attributes set by the file wrappers below\\n data_encoding = \\'unknown\\'\\n file_encoding = \\'unknown\\'\\n def __init__(self, stream, encode, decode, reader, writer,\\n errors=\\'strict\\'):\\n \"\"\" creates a streamrecoder instance which implements a two-way\\n conversion: encode and decode work on the frontend (the\\n data visible to .read() and .write()) while reader and writer\\n work on the backend (the data in stream).\\n you can use these objects to do transparent\\n transcodings from e.g. latin-1 to utf-8 and back.\\n stream must be a file-like object.\\n encode and decode must adhere to the codec interface; reader and\\n writer must be factory functions or classes providing the\\n streamreader and streamwriter interfaces resp.\\n error handling is done in the same way as defined for the\\n streamwriter/readers.\\n \"\"\"\\n self.stream = stream\\n self.encode = encode\\n self.decode = decode\\n self.reader = reader(stream, errors)\\n self.writer = writer(stream, errors)\\n self.errors = errors\\n def read(self, size=-1):\\n data = self.reader.read(size)\\n data, bytesencoded = self.encode(data, self.errors)\\n return data\\n def readline(self, size=none):\\n if size is none:\\n data = self.reader.readline()\\n else:\\n data = self.reader.readline(size)\\n data, bytesencoded = self.encode(data, self.errors)\\n return data\\n def readlines(self, sizehint=none):\\n data = self.reader.read()\\n data, bytesencoded = self.encode(data, self.errors)\\n return data.splitlines(keepends=true)\\n def __next__(self):\\n \"\"\" return the next decoded line from the input stream.\"\"\"\\n data = next(self.reader)\\n data, bytesencoded = self.encode(data, self.errors)\\n return data\\n def __iter__(self):\\n return self\\n def write(self, data):\\n data, bytesdecoded = self.decode(data, self.errors)\\n return self.writer.write(data)\\n def writelines(self, list):\\n data = \\'\\'.join(list)\\n data, bytesdecoded = self.decode(data, self.errors)\\n return self.writer.write(data)\\n def reset(self):\\n self.reader.reset()\\n self.writer.reset()\\n def __getattr__(self, name,\\n getattr=getattr):\\n \"\"\" inherit all other methods from the underlying stream.\\n \"\"\"\\n return getattr(self.stream, name)\\n def __enter__(self):\\n return self\\n def __exit__(self, type, value, tb):\\n self.stream.close()\\n ### shortcuts\\n def open(filename, mode=\\'r\\', encoding=none, errors=\\'strict\\', buffering=1):\\n \"\"\" open an encoded file using the given mode and return\\n a wrapped version providing transparent encoding/decoding.\\n note: the wrapped version will only accept the object format\\n defined by the codecs, i.e. unicode objects for most builtin\\n codecs. output is also codec dependent and will usually be\\n unicode as well.\\n underlying encoded files are always opened in binary mode.\\n the default file mode is \\'r\\', meaning to open the file in read mode.\\n encoding specifies the encoding which is to be used for the\\n file.\\n errors may be given to define the error handling. it defaults\\n to \\'strict\\' which causes valueerrors to be raised in case an\\n encoding error occurs.\\n buffering has the same meaning as for the builtin open() api.\\n it defaults to line buffered.\\n the returned wrapped file object provides an extra attribute\\n .encoding which allows querying the used encoding. this\\n attribute is only available if an encoding was specified as\\n parameter.\\n \"\"\"\\n if encoding is not none and \\\\\\n \\'b\\' not in mode:\\n # force opening of the file in binary mode\\n mode = mode + \\'b\\'\\n file = builtins.open(filename, mode, buffering)\\n if encoding is none:\\n return file\\n info = lookup(encoding)\\n srw = streamreaderwriter(file, info.streamreader, info.streamwriter, errors)\\n # add attributes to simplify introspection\\n srw.encoding = encoding\\n return srw\\n def encodedfile(file, data_encoding, file_encoding=none, errors=\\'strict\\'):\\n \"\"\" return a wrapped version of file which provides transparent\\n encoding translation.\\n data written to the wrapped file is decoded according\\n to the given data_encoding and then encoded to the underlying\\n file using file_encoding. the intermediate data type\\n will usually be unicode but depends on the specified codecs.\\n bytes read from the file are decoded using file_encoding and then\\n passed back to the caller encoded using data_encoding.\\n if file_encoding is not given, it defaults to data_encoding.\\n errors may be given to define the error handling. it defaults\\n to \\'strict\\' which causes valueerrors to be raised in case an\\n encoding error occurs.\\n the returned wrapped file object provides two extra attributes\\n .data_encoding and .file_encoding which reflect the given\\n parameters of the same name. the attributes can be used for\\n introspection by python programs.\\n \"\"\"\\n if file_encoding is none:\\n file_encoding = data_encoding\\n data_info = lookup(data_encoding)\\n file_info = lookup(file_encoding)\\n sr = streamrecoder(file, data_info.encode, data_info.decode,\\n file_info.streamreader, file_info.streamwriter, errors)\\n # add attributes to simplify introspection\\n sr.data_encoding = data_encoding\\n sr.file_encoding = file_encoding\\n return sr\\n ### helpers for codec lookup\\n def getencoder(encoding):\\n \"\"\" lookup up the codec for the given encoding and return\\n its encoder function.\\n raises a lookuperror in case the encoding cannot be found.\\n \"\"\"\\n return lookup(encoding).encode\\n def getdecoder(encoding):\\n \"\"\" lookup up the codec for the given encoding and return\\n its decoder function.\\n raises a lookuperror in case the encoding cannot be found.\\n \"\"\"\\n return lookup(encoding).decode\\n def getincrementalencoder(encoding):\\n \"\"\" lookup up the codec for the given encoding and return\\n its incrementalencoder class or factory function.\\n raises a lookuperror in case the encoding cannot be found\\n or the codecs doesn\\'t provide an incremental encoder.\\n \"\"\"\\n encoder = lookup(encoding).incrementalencoder\\n if encoder is none:\\n raise lookuperror(encoding)\\n return encoder\\n def getincrementaldecoder(encoding):\\n \"\"\" lookup up the codec for the given encoding and return\\n its incrementaldecoder class or factory function.\\n raises a lookuperror in case the encoding cannot be found\\n or the codecs doesn\\'t provide an incremental decoder.\\n \"\"\"\\n decoder = lookup(encoding).incrementaldecoder\\n if decoder is none:\\n raise lookuperror(encoding)\\n return decoder\\n def getreader(encoding):\\n \"\"\" lookup up the codec for the given encoding and return\\n its streamreader class or factory function.\\n raises a lookuperror in case the encoding cannot be found.\\n \"\"\"\\n return lookup(encoding).streamreader\\n def getwriter(encoding):\\n \"\"\" lookup up the codec for the given encoding and return\\n its streamwriter class or factory function.\\n raises a lookuperror in case the encoding cannot be found.\\n \"\"\"\\n return lookup(encoding).streamwriter\\n def iterencode(iterator, encoding, errors=\\'strict\\', **kwargs):\\n \"\"\"\\n encoding iterator.\\n encodes the input strings from the iterator using an incrementalencoder.\\n errors and kwargs are passed through to the incrementalencoder\\n constructor.\\n \"\"\"\\n encoder = getincrementalencoder(encoding)(errors, **kwargs)\\n for input in iterator:\\n output = encoder.encode(input)\\n if output:\\n yield output\\n output = encoder.encode(\"\", true)\\n if output:\\n yield output\\n def iterdecode(iterator, encoding, errors=\\'strict\\', **kwargs):\\n \"\"\"\\n decoding iterator.\\n decodes the input strings from the iterator using an incrementaldecoder.\\n errors and kwargs are passed through to the incrementaldecoder\\n constructor.\\n \"\"\"\\n decoder = getincrementaldecoder(encoding)(errors, **kwargs)\\n for input in iterator:\\n output = decoder.decode(input)\\n if output:\\n yield output\\n output = decoder.decode(b\"\", true)\\n if output:\\n yield output\\n ### helpers for charmap-based codecs\\n def make_identity_dict(rng):\\n \"\"\" make_identity_dict(rng) -> dict\\n return a dictionary where elements of the rng sequence are\\n mapped to themselves.\\n \"\"\"\\n return {i:i for i in rng}\\n def make_encoding_map(decoding_map):\\n \"\"\" creates an encoding map from a decoding map.\\n if a target mapping in the decoding map occurs multiple\\n times, then that target is mapped to none (undefined mapping),\\n causing an exception when encountered by the charmap codec\\n during translation.\\n one example where this happens is cp875.py which decodes\\n multiple character to \\\\\\\\u001a.\\n \"\"\"\\n m = {}\\n for k,v in decoding_map.items():\\n if not v in m:\\n m[v] = k\\n else:\\n m[v] = none\\n return m\\n ### error handlers\\n try:\\n strict_errors = lookup_error(\"strict\")\\n ignore_errors = lookup_error(\"ignore\")\\n replace_errors = lookup_error(\"replace\")\\n xmlcharrefreplace_errors = lookup_error(\"xmlcharrefreplace\")\\n backslashreplace_errors = lookup_error(\"backslashreplace\")\\n namereplace_errors = lookup_error(\"namereplace\")\\n except lookuperror:\\n # in --disable-unicode builds, these error handler are missing\\n strict_errors = none\\n ignore_errors = none\\n replace_errors = none\\n xmlcharrefreplace_errors = none\\n backslashreplace_errors = none\\n namereplace_errors = none\\n # tell modulefinder that using codecs probably needs the encodings\\n # package\\n _false = 0\\n if _false:\\n import encodings\\n ### tests\\n if __name__ == \\'__main__\\':\\n # make stdout translate latin-1 output into utf-8 output\\n sys.stdout = encodedfile(sys.stdout, \\'latin-1\\', \\'utf-8\\')\\n # have stdin translate latin-1 input into utf-8 input\\n sys.stdin = encodedfile(sys.stdin, \\'utf-8\\', \\'latin-1\\')\\n # copyright 2007 google, inc. all rights reserved.\\n # licensed to psf under a contributor agreement.\\n \"\"\"abstract base classes (abcs) for collections, according to pep 3119.\\n unit tests are in test_collections.\\n \"\"\"\\n from abc import abcmeta, abstractmethod\\n import sys\\n __all__ = [\"awaitable\", \"coroutine\", \"asynciterable\", \"asynciterator\",\\n \"hashable\", \"iterable\", \"iterator\", \"generator\",\\n \"sized\", \"container\", \"callable\",\\n \"set\", \"mutableset\",\\n \"mapping\", \"mutablemapping\",\\n \"mappingview\", \"keysview\", \"itemsview\", \"valuesview\",\\n \"sequence\", \"mutablesequence\",\\n \"bytestring\",\\n ]\\n # this module has been renamed from collections.abc to _collections_abc to\\n # speed up interpreter startup. some of the types such as mutablemapping are\\n # required early but collections module imports a lot of other modules.\\n # see issue #19218\\n __name__ = \"collections.abc\"\\n # private list of types that we want to register with the various abcs\\n # so that they will pass tests like:\\n #       it = iter(somebytearray)\\n #       assert isinstance(it, iterable)\\n # note:  in other implementations, these types many not be distinct\\n # and they make have their own implementation specific types that\\n # are not included on this list.\\n bytes_iterator = type(iter(b\\'\\'))\\n bytearray_iterator = type(iter(bytearray()))\\n #callable_iterator = ???\\n dict_keyiterator = type(iter({}.keys()))\\n dict_valueiterator = type(iter({}.values()))\\n dict_itemiterator = type(iter({}.items()))\\n list_iterator = type(iter([]))\\n list_reverseiterator = type(iter(reversed([])))\\n range_iterator = type(iter(range(0)))\\n set_iterator = type(iter(set()))\\n str_iterator = type(iter(\"\"))\\n tuple_iterator = type(iter(()))\\n zip_iterator = type(iter(zip()))\\n ## views ##\\n dict_keys = type({}.keys())\\n dict_values = type({}.values())\\n dict_items = type({}.items())\\n ## misc ##\\n mappingproxy = type(type.__dict__)\\n generator = type((lambda: (yield))())\\n ## coroutine ##\\n async def _coro(): pass\\n _coro = _coro()\\n coroutine = type(_coro)\\n _coro.close()  # prevent resourcewarning\\n del _coro\\n ### one-trick ponies ###\\n class hashable(metaclass=abcmeta):\\n __slots__ = ()\\n @abstractmethod\\n def __hash__(self):\\n return 0\\n @classmethod\\n def __subclasshook__(cls, c):\\n if cls is hashable:\\n for b in c.__mro__:\\n if \"__hash__\" in b.__dict__:\\n if b.__dict__[\"__hash__\"]:\\n return true\\n break\\n return notimplemented\\n class awaitable(metaclass=abcmeta):\\n __slots__ = ()\\n @abstractmethod\\n def __await__(self):\\n yield\\n @classmethod\\n def __subclasshook__(cls, c):\\n if cls is awaitable:\\n for b in c.__mro__:\\n if \"__await__\" in b.__dict__:\\n if b.__dict__[\"__await__\"]:\\n return true\\n break\\n return notimplemented\\n class coroutine(awaitable):\\n __slots__ = ()\\n @abstractmethod\\n def send(self, value):\\n \"\"\"send a value into the coroutine.\\n return next yielded value or raise stopiteration.\\n \"\"\"\\n raise stopiteration\\n @abstractmethod\\n def throw(self, typ, val=none, tb=none):\\n \"\"\"raise an exception in the coroutine.\\n return next yielded value or raise stopiteration.\\n \"\"\"\\n if val is none:\\n if tb is none:\\n raise typ\\n val = typ()\\n if tb is not none:\\n val = val.with_traceback(tb)\\n raise val\\n def close(self):\\n \"\"\"raise generatorexit inside coroutine.\\n \"\"\"\\n try:\\n self.throw(generatorexit)\\n except (generatorexit, stopiteration):\\n pass\\n else:\\n raise runtimeerror(\"coroutine ignored generatorexit\")\\n @classmethod\\n def __subclasshook__(cls, c):\\n if cls is coroutine:\\n mro = c.__mro__\\n for method in (\\'__await__\\', \\'send\\', \\'throw\\', \\'close\\'):\\n for base in mro:\\n if method in base.__dict__:\\n break\\n else:\\n return notimplemented\\n return true\\n return notimplemented\\n coroutine.register(coroutine)\\n class asynciterable(metaclass=abcmeta):\\n __slots__ = ()\\n @abstractmethod\\n def __aiter__(self):\\n return asynciterator()\\n @classmethod\\n def __subclasshook__(cls, c):\\n if cls is asynciterable:\\n if any(\"__aiter__\" in b.__dict__ for b in c.__mro__):\\n return true\\n return notimplemented\\n class asynciterator(asynciterable):\\n __slots__ = ()\\n @abstractmethod\\n async def __anext__(self):\\n \"\"\"return the next item or raise stopasynciteration when exhausted.\"\"\"\\n raise stopasynciteration\\n def __aiter__(self):\\n return self\\n @classmethod\\n def __subclasshook__(cls, c):\\n if cls is asynciterator:\\n if (any(\"__anext__\" in b.__dict__ for b in c.__mro__) and\\n any(\"__aiter__\" in b.__dict__ for b in c.__mro__)):\\n return true\\n return notimplemented\\n class iterable(metaclass=abcmeta):\\n __slots__ = ()\\n @abstractmethod\\n def __iter__(self):\\n while false:\\n yield none\\n @classmethod\\n def __subclasshook__(cls, c):\\n if cls is iterable:\\n if any(\"__iter__\" in b.__dict__ for b in c.__mro__):\\n return true\\n return notimplemented\\n class iterator(iterable):\\n __slots__ = ()\\n @abstractmethod\\n def __next__(self):\\n \\'return the next item from the iterator. when exhausted, raise stopiteration\\'\\n raise stopiteration\\n def __iter__(self):\\n return self\\n @classmethod\\n def __subclasshook__(cls, c):\\n if cls is iterator:\\n if (any(\"__next__\" in b.__dict__ for b in c.__mro__) and\\n any(\"__iter__\" in b.__dict__ for b in c.__mro__)):\\n return true\\n return notimplemented\\n iterator.register(bytes_iterator)\\n iterator.register(bytearray_iterator)\\n #iterator.register(callable_iterator)\\n iterator.register(dict_keyiterator)\\n iterator.register(dict_valueiterator)\\n iterator.register(dict_itemiterator)\\n iterator.register(list_iterator)\\n iterator.register(list_reverseiterator)\\n iterator.register(range_iterator)\\n iterator.register(set_iterator)\\n iterator.register(str_iterator)\\n iterator.register(tuple_iterator)\\n iterator.register(zip_iterator)\\n class generator(iterator):\\n __slots__ = ()\\n def __next__(self):\\n \"\"\"return the next item from the generator.\\n when exhausted, raise stopiteration.\\n \"\"\"\\n return self.send(none)\\n @abstractmethod\\n def send(self, value):\\n \"\"\"send a value into the generator.\\n return next yielded value or raise stopiteration.\\n \"\"\"\\n raise stopiteration\\n @abstractmethod\\n def throw(self, typ, val=none, tb=none):\\n \"\"\"raise an exception in the generator.\\n return next yielded value or raise stopiteration.\\n \"\"\"\\n if val is none:\\n if tb is none:\\n raise typ\\n val = typ()\\n if tb is not none:\\n val = val.with_traceback(tb)\\n raise val\\n def close(self):\\n \"\"\"raise generatorexit inside generator.\\n \"\"\"\\n try:\\n self.throw(generatorexit)\\n except (generatorexit, stopiteration):\\n pass\\n else:\\n raise runtimeerror(\"generator ignored generatorexit\")\\n @classmethod\\n def __subclasshook__(cls, c):\\n if cls is generator:\\n mro = c.__mro__\\n for method in (\\'__iter__\\', \\'__next__\\', \\'send\\', \\'throw\\', \\'close\\'):\\n for base in mro:\\n if method in base.__dict__:\\n break\\n else:\\n return notimplemented\\n return true\\n return notimplemented\\n generator.register(generator)\\n class sized(metaclass=abcmeta):\\n __slots__ = ()\\n @abstractmethod\\n def __len__(self):\\n return 0\\n @classmethod\\n def __subclasshook__(cls, c):\\n if cls is sized:\\n if any(\"__len__\" in b.__dict__ for b in c.__mro__):\\n return true\\n return notimplemented\\n class container(metaclass=abcmeta):\\n __slots__ = ()\\n @abstractmethod\\n def __contains__(self, x):\\n return false\\n @classmethod\\n def __subclasshook__(cls, c):\\n if cls is container:\\n if any(\"__contains__\" in b.__dict__ for b in c.__mro__):\\n return true\\n return notimplemented\\n class callable(metaclass=abcmeta):\\n __slots__ = ()\\n @abstractmethod\\n def __call__(self, *args, **kwds):\\n return false\\n @classmethod\\n def __subclasshook__(cls, c):\\n if cls is callable:\\n if any(\"__call__\" in b.__dict__ for b in c.__mro__):\\n return true\\n return notimplemented\\n ### sets ###\\n class set(sized, iterable, container):\\n \"\"\"a set is a finite, iterable container.\\n this class provides concrete generic implementations of all\\n methods except for __contains__, __iter__ and __len__.\\n to override the comparisons (presumably for speed, as the\\n semantics are fixed), redefine __le__ and __ge__,\\n then the other operations will automatically follow suit.\\n \"\"\"\\n __slots__ = ()\\n def __le__(self, other):\\n if not isinstance(other, set):\\n return notimplemented\\n if len(self) > len(other):\\n return false\\n for elem in self:\\n if elem not in other:\\n return false\\n return true\\n def __lt__(self, other):\\n if not isinstance(other, set):\\n return notimplemented\\n return len(self) < len(other) and self.__le__(other)\\n def __gt__(self, other):\\n if not isinstance(other, set):\\n return notimplemented\\n return len(self) > len(other) and self.__ge__(other)\\n def __ge__(self, other):\\n if not isinstance(other, set):\\n return notimplemented\\n if len(self) < len(other):\\n return false\\n for elem in other:\\n if elem not in self:\\n return false\\n return true\\n def __eq__(self, other):\\n if not isinstance(other, set):\\n return notimplemented\\n return len(self) == len(other) and self.__le__(other)\\n @classmethod\\n def _from_iterable(cls, it):\\n \\'\\'\\'construct an instance of the class from any iterable input.\\n must override this method if the class constructor signature\\n does not accept an iterable for an input.\\n \\'\\'\\'\\n return cls(it)\\n def __and__(self, other):\\n if not isinstance(other, iterable):\\n return notimplemented\\n return self._from_iterable(value for value in other if value in self)\\n __rand__ = __and__\\n def isdisjoint(self, other):\\n \\'return true if two sets have a null intersection.\\'\\n for value in other:\\n if value in self:\\n return false\\n return true\\n def __or__(self, other):\\n if not isinstance(other, iterable):\\n return notimplemented\\n chain = (e for s in (self, other) for e in s)\\n return self._from_iterable(chain)\\n __ror__ = __or__\\n def __sub__(self, other):\\n if not isinstance(other, set):\\n if not isinstance(other, iterable):\\n return notimplemented\\n other = self._from_iterable(other)\\n return self._from_iterable(value for value in self\\n if value not in other)\\n def __rsub__(self, other):\\n if not isinstance(other, set):\\n if not isinstance(other, iterable):\\n return notimplemented\\n other = self._from_iterable(other)\\n return self._from_iterable(value for value in other\\n if value not in self)\\n def __xor__(self, other):\\n if not isinstance(other, set):\\n if not isinstance(other, iterable):\\n return notimplemented\\n other = self._from_iterable(other)\\n return (self - other) | (other - self)\\n __rxor__ = __xor__\\n def _hash(self):\\n \"\"\"compute the hash value of a set.\\n note that we don\\'t define __hash__: not all sets are hashable.\\n but if you define a hashable set type, its __hash__ should\\n call this function.\\n this must be compatible __eq__.\\n all sets ought to compare equal if they contain the same\\n elements, regardless of how they are implemented, and\\n regardless of the order of the elements; so there\\'s not much\\n freedom for __eq__ or __hash__.  we match the algorithm used\\n by the built-in frozenset type.\\n \"\"\"\\n max = sys.maxsize\\n mask = 2 * max + 1\\n n = len(self)\\n h = 1927868237 * (n + 1)\\n h &= mask\\n for x in self:\\n hx = hash(x)\\n h ^= (hx ^ (hx << 16) ^ 89869747)  * 3644798167\\n h &= mask\\n h = h * 69069 + 907133923\\n h &= mask\\n if h > max:\\n h -= mask + 1\\n if h == -1:\\n h = 590923713\\n return h\\n set.register(frozenset)\\n class mutableset(set):\\n \"\"\"a mutable set is a finite, iterable container.\\n this class provides concrete generic implementations of all\\n methods except for __contains__, __iter__, __len__,\\n add(), and discard().\\n to override the comparisons (presumably for speed, as the\\n semantics are fixed), all you have to do is redefine __le__ and\\n then the other operations will automatically follow suit.\\n \"\"\"\\n __slots__ = ()\\n @abstractmethod\\n def add(self, value):\\n \"\"\"add an element.\"\"\"\\n raise notimplementederror\\n @abstractmethod\\n def discard(self, value):\\n \"\"\"remove an element.  do not raise an exception if absent.\"\"\"\\n raise notimplementederror\\n def remove(self, value):\\n \"\"\"remove an element. if not a member, raise a keyerror.\"\"\"\\n if value not in self:\\n raise keyerror(value)\\n self.discard(value)\\n def pop(self):\\n \"\"\"return the popped value.  raise keyerror if empty.\"\"\"\\n it = iter(self)\\n try:\\n value = next(it)\\n except stopiteration:\\n raise keyerror\\n self.discard(value)\\n return value\\n def clear(self):\\n \"\"\"this is slow (creates n new iterators!) but effective.\"\"\"\\n try:\\n while true:\\n self.pop()\\n except keyerror:\\n pass\\n def __ior__(self, it):\\n for value in it:\\n self.add(value)\\n return self\\n def __iand__(self, it):\\n for value in (self - it):\\n self.discard(value)\\n return self\\n def __ixor__(self, it):\\n if it is self:\\n self.clear()\\n else:\\n if not isinstance(it, set):\\n it = self._from_iterable(it)\\n for value in it:\\n if value in self:\\n self.discard(value)\\n else:\\n self.add(value)\\n return self\\n def __isub__(self, it):\\n if it is self:\\n self.clear()\\n else:\\n for value in it:\\n self.discard(value)\\n return self\\n mutableset.register(set)\\n ### mappings ###\\n class mapping(sized, iterable, container):\\n __slots__ = ()\\n \"\"\"a mapping is a generic container for associating key/value\\n pairs.\\n this class provides concrete generic implementations of all\\n methods except for __getitem__, __iter__, and __len__.\\n \"\"\"\\n @abstractmethod\\n def __getitem__(self, key):\\n raise keyerror\\n def get(self, key, default=none):\\n \\'d.get(k[,d]) -> d[k] if k in d, else d.  d defaults to none.\\'\\n try:\\n return self[key]\\n except keyerror:\\n return default\\n def __contains__(self, key):\\n try:\\n self[key]\\n except keyerror:\\n return false\\n else:\\n return true\\n def keys(self):\\n \"d.keys() -> a set-like object providing a view on d\\'s keys\"\\n return keysview(self)\\n def items(self):\\n \"d.items() -> a set-like object providing a view on d\\'s items\"\\n return itemsview(self)\\n def values(self):\\n \"d.values() -> an object providing a view on d\\'s values\"\\n return valuesview(self)\\n def __eq__(self, other):\\n if not isinstance(other, mapping):\\n return notimplemented\\n return dict(self.items()) == dict(other.items())\\n mapping.register(mappingproxy)\\n class mappingview(sized):\\n __slots__ = \\'_mapping\\',\\n def __init__(self, mapping):\\n self._mapping = mapping\\n def __len__(self):\\n return len(self._mapping)\\n def __repr__(self):\\n return \\'{0.__class__.__name__}({0._mapping!r})\\'.format(self)\\n class keysview(mappingview, set):\\n __slots__ = ()\\n @classmethod\\n def _from_iterable(self, it):\\n return set(it)\\n def __contains__(self, key):\\n return key in self._mapping\\n def __iter__(self):\\n yield from self._mapping\\n keysview.register(dict_keys)\\n class itemsview(mappingview, set):\\n __slots__ = ()\\n @classmethod\\n def _from_iterable(self, it):\\n return set(it)\\n def __contains__(self, item):\\n key, value = item\\n try:\\n v = self._mapping[key]\\n except keyerror:\\n return false\\n else:\\n return v == value\\n def __iter__(self):\\n for key in self._mapping:\\n yield (key, self._mapping[key])\\n itemsview.register(dict_items)\\n class valuesview(mappingview):\\n __slots__ = ()\\n def __contains__(self, value):\\n for key in self._mapping:\\n if value == self._mapping[key]:\\n return true\\n return false\\n def __iter__(self):\\n for key in self._mapping:\\n yield self._mapping[key]\\n valuesview.register(dict_values)\\n class mutablemapping(mapping):\\n __slots__ = ()\\n \"\"\"a mutablemapping is a generic container for associating\\n key/value pairs.\\n this class provides concrete generic implementations of all\\n methods except for __getitem__, __setitem__, __delitem__,\\n __iter__, and __len__.\\n \"\"\"\\n @abstractmethod\\n def __setitem__(self, key, value):\\n raise keyerror\\n @abstractmethod\\n def __delitem__(self, key):\\n raise keyerror\\n __marker = object()\\n def pop(self, key, default=__marker):\\n \\'\\'\\'d.pop(k[,d]) -> v, remove specified key and return the corresponding value.\\n if key is not found, d is returned if given, otherwise keyerror is raised.\\n \\'\\'\\'\\n try:\\n value = self[key]\\n except keyerror:\\n if default is self.__marker:\\n raise\\n return default\\n else:\\n del self[key]\\n return value\\n def popitem(self):\\n \\'\\'\\'d.popitem() -> (k, v), remove and return some (key, value) pair\\n as a 2-tuple; but raise keyerror if d is empty.\\n \\'\\'\\'\\n try:\\n key = next(iter(self))\\n except stopiteration:\\n raise keyerror\\n value = self[key]\\n del self[key]\\n return key, value\\n def clear(self):\\n \\'d.clear() -> none.  remove all items from d.\\'\\n try:\\n while true:\\n self.popitem()\\n except keyerror:\\n pass\\n def update(*args, **kwds):\\n \\'\\'\\' d.update([e, ]**f) -> none.  update d from mapping/iterable e and f.\\n if e present and has a .keys() method, does:     for k in e: d[k] = e[k]\\n if e present and lacks .keys() method, does:     for (k, v) in e: d[k] = v\\n in either case, this is followed by: for k, v in f.items(): d[k] = v\\n \\'\\'\\'\\n if not args:\\n raise typeerror(\"descriptor \\'update\\' of \\'mutablemapping\\' object \"\\n \"needs an argument\")\\n self, *args = args\\n if len(args) > 1:\\n raise typeerror(\\'update expected at most 1 arguments, got %d\\' %\\n len(args))\\n if args:\\n other = args[0]\\n if isinstance(other, mapping):\\n for key in other:\\n self[key] = other[key]\\n elif hasattr(other, \"keys\"):\\n for key in other.keys():\\n self[key] = other[key]\\n else:\\n for key, value in other:\\n self[key] = value\\n for key, value in kwds.items():\\n self[key] = value\\n def setdefault(self, key, default=none):\\n \\'d.setdefault(k[,d]) -> d.get(k,d), also set d[k]=d if k not in d\\'\\n try:\\n return self[key]\\n except keyerror:\\n self[key] = default\\n return default\\n mutablemapping.register(dict)\\n ### sequences ###\\n class sequence(sized, iterable, container):\\n \"\"\"all the operations on a read-only sequence.\\n concrete subclasses must override __new__ or __init__,\\n __getitem__, and __len__.\\n \"\"\"\\n __slots__ = ()\\n @abstractmethod\\n def __getitem__(self, index):\\n raise indexerror\\n def __iter__(self):\\n i = 0\\n try:\\n while true:\\n v = self[i]\\n yield v\\n i += 1\\n except indexerror:\\n return\\n def __contains__(self, value):\\n for v in self:\\n if v == value:\\n return true\\n return false\\n def __reversed__(self):\\n for i in reversed(range(len(self))):\\n yield self[i]\\n def index(self, value, start=0, stop=none):\\n \\'\\'\\'s.index(value, [start, [stop]]) -> integer -- return first index of value.\\n raises valueerror if the value is not present.\\n \\'\\'\\'\\n if start is not none and start < 0:\\n start = max(len(self) + start, 0)\\n if stop is not none and stop < 0:\\n stop += len(self)\\n i = start\\n while stop is none or i < stop:\\n try:\\n if self[i] == value:\\n return i\\n except indexerror:\\n break\\n i += 1\\n raise valueerror\\n def count(self, value):\\n \\'s.count(value) -> integer -- return number of occurrences of value\\'\\n return sum(1 for v in self if v == value)\\n sequence.register(tuple)\\n sequence.register(str)\\n sequence.register(range)\\n sequence.register(memoryview)\\n class bytestring(sequence):\\n \"\"\"this unifies bytes and bytearray.\\n xxx should add all their methods.\\n \"\"\"\\n __slots__ = ()\\n bytestring.register(bytes)\\n bytestring.register(bytearray)\\n class mutablesequence(sequence):\\n __slots__ = ()\\n \"\"\"all the operations on a read-write sequence.\\n concrete subclasses must provide __new__ or __init__,\\n __getitem__, __setitem__, __delitem__, __len__, and insert().\\n \"\"\"\\n @abstractmethod\\n def __setitem__(self, index, value):\\n raise indexerror\\n @abstractmethod\\n def __delitem__(self, index):\\n raise indexerror\\n @abstractmethod\\n def insert(self, index, value):\\n \\'s.insert(index, value) -- insert value before index\\'\\n raise indexerror\\n def append(self, value):\\n \\'s.append(value) -- append value to the end of the sequence\\'\\n self.insert(len(self), value)\\n def clear(self):\\n \\'s.clear() -> none -- remove all items from s\\'\\n try:\\n while true:\\n self.pop()\\n except indexerror:\\n pass\\n def reverse(self):\\n \\'s.reverse() -- reverse *in place*\\'\\n n = len(self)\\n for i in range(n//2):\\n self[i], self[n-i-1] = self[n-i-1], self[i]\\n def extend(self, values):\\n \\'s.extend(iterable) -- extend sequence by appending elements from the iterable\\'\\n for v in values:\\n self.append(v)\\n def pop(self, index=-1):\\n \\'\\'\\'s.pop([index]) -> item -- remove and return item at index (default last).\\n raise indexerror if list is empty or index is out of range.\\n \\'\\'\\'\\n v = self[index]\\n del self[index]\\n return v\\n def remove(self, value):\\n \\'\\'\\'s.remove(value) -- remove first occurrence of value.\\n raise valueerror if the value is not present.\\n \\'\\'\\'\\n del self[self.index(value)]\\n def __iadd__(self, values):\\n self.extend(values)\\n return self\\n mutablesequence.register(list)\\n mutablesequence.register(bytearray)  # multiply inheriting, see bytestring\\n \"\"\"generic (shallow and deep) copying operations.\\n interface summary:\\n import copy\\n x = copy.copy(y)        # make a shallow copy of y\\n x = copy.deepcopy(y)    # make a deep copy of y\\n for module specific errors, copy.error is raised.\\n the difference between shallow and deep copying is only relevant for\\n compound objects (objects that contain other objects, like lists or\\n class instances).\\n - a shallow copy constructs a new compound object and then (to the\\n extent possible) inserts *the same objects* into it that the\\n original contains.\\n - a deep copy constructs a new compound object and then, recursively,\\n inserts *copies* into it of the objects found in the original.\\n two problems often exist with deep copy operations that don\\'t exist\\n with shallow copy operations:\\n a) recursive objects (compound objects that, directly or indirectly,\\n contain a reference to themselves) may cause a recursive loop\\n b) because deep copy copies *everything* it may copy too much, e.g.\\n administrative data structures that should be shared even between\\n copies\\n python\\'s deep copy operation avoids these problems by:\\n a) keeping a table of objects already copied during the current\\n copying pass\\n b) letting user-defined classes override the copying operation or the\\n set of components copied\\n this version does not copy types like module, class, function, method,\\n nor stack trace, stack frame, nor file, socket, window, nor array, nor\\n any similar types.\\n classes can use the same interfaces to control copying that they use\\n to control pickling: they can define methods called __getinitargs__(),\\n __getstate__() and __setstate__().  see the documentation for module\\n \"pickle\" for information on these methods.\\n \"\"\"\\n import types\\n import weakref\\n from copyreg import dispatch_table\\n import builtins\\n class error(exception):\\n pass\\n error = error   # backward compatibility\\n try:\\n from org.python.core import pystringmap\\n except importerror:\\n pystringmap = none\\n __all__ = [\"error\", \"copy\", \"deepcopy\"]\\n def copy(x):\\n \"\"\"shallow copy operation on arbitrary python objects.\\n see the module\\'s __doc__ string for more info.\\n \"\"\"\\n cls = type(x)\\n copier = _copy_dispatch.get(cls)\\n if copier:\\n return copier(x)\\n try:\\n issc = issubclass(cls, type)\\n except typeerror: # cls is not a class\\n issc = false\\n if issc:\\n # treat it as a regular class:\\n return _copy_immutable(x)\\n copier = getattr(cls, \"__copy__\", none)\\n if copier:\\n return copier(x)\\n reductor = dispatch_table.get(cls)\\n if reductor:\\n rv = reductor(x)\\n else:\\n reductor = getattr(x, \"__reduce_ex__\", none)\\n if reductor:\\n rv = reductor(4)\\n else:\\n reductor = getattr(x, \"__reduce__\", none)\\n if reductor:\\n rv = reductor()\\n else:\\n raise error(\"un(shallow)copyable object of type %s\" % cls)\\n return _reconstruct(x, rv, 0)\\n _copy_dispatch = d = {}\\n def _copy_immutable(x):\\n return x\\n for t in (type(none), int, float, bool, str, tuple,\\n bytes, frozenset, type, range,\\n types.builtinfunctiontype, type(ellipsis),\\n types.functiontype, weakref.ref):\\n d[t] = _copy_immutable\\n t = getattr(types, \"codetype\", none)\\n if t is not none:\\n d[t] = _copy_immutable\\n for name in (\"complex\", \"unicode\"):\\n t = getattr(builtins, name, none)\\n if t is not none:\\n d[t] = _copy_immutable\\n def _copy_with_constructor(x):\\n return type(x)(x)\\n for t in (list, dict, set):\\n d[t] = _copy_with_constructor\\n def _copy_with_copy_method(x):\\n return x.copy()\\n if pystringmap is not none:\\n d[pystringmap] = _copy_with_copy_method\\n del d\\n def deepcopy(x, memo=none, _nil=[]):\\n \"\"\"deep copy operation on arbitrary python objects.\\n see the module\\'s __doc__ string for more info.\\n \"\"\"\\n if memo is none:\\n memo = {}\\n d = id(x)\\n y = memo.get(d, _nil)\\n if y is not _nil:\\n return y\\n cls = type(x)\\n copier = _deepcopy_dispatch.get(cls)\\n if copier:\\n y = copier(x, memo)\\n else:\\n try:\\n issc = issubclass(cls, type)\\n except typeerror: # cls is not a class (old boost; see sf #502085)\\n issc = 0\\n if issc:\\n y = _deepcopy_atomic(x, memo)\\n else:\\n copier = getattr(x, \"__deepcopy__\", none)\\n if copier:\\n y = copier(memo)\\n else:\\n reductor = dispatch_table.get(cls)\\n if reductor:\\n rv = reductor(x)\\n else:\\n reductor = getattr(x, \"__reduce_ex__\", none)\\n if reductor:\\n rv = reductor(4)\\n else:\\n reductor = getattr(x, \"__reduce__\", none)\\n if reductor:\\n rv = reductor()\\n else:\\n raise error(\\n \"un(deep)copyable object of type %s\" % cls)\\n y = _reconstruct(x, rv, 1, memo)\\n # if is its own copy, don\\'t memoize.\\n if y is not x:\\n memo[d] = y\\n _keep_alive(x, memo) # make sure x lives at least as long as d\\n return y\\n _deepcopy_dispatch = d = {}\\n def _deepcopy_atomic(x, memo):\\n return x\\n d[type(none)] = _deepcopy_atomic\\n d[type(ellipsis)] = _deepcopy_atomic\\n d[int] = _deepcopy_atomic\\n d[float] = _deepcopy_atomic\\n d[bool] = _deepcopy_atomic\\n try:\\n d[complex] = _deepcopy_atomic\\n except nameerror:\\n pass\\n d[bytes] = _deepcopy_atomic\\n d[str] = _deepcopy_atomic\\n try:\\n d[types.codetype] = _deepcopy_atomic\\n except attributeerror:\\n pass\\n d[type] = _deepcopy_atomic\\n d[types.builtinfunctiontype] = _deepcopy_atomic\\n d[types.functiontype] = _deepcopy_atomic\\n d[weakref.ref] = _deepcopy_atomic\\n def _deepcopy_list(x, memo):\\n y = []\\n memo[id(x)] = y\\n for a in x:\\n y.append(deepcopy(a, memo))\\n return y\\n d[list] = _deepcopy_list\\n def _deepcopy_tuple(x, memo):\\n y = [deepcopy(a, memo) for a in x]\\n # we\\'re not going to put the tuple in the memo, but it\\'s still important we\\n # check for it, in case the tuple contains recursive mutable structures.\\n try:\\n return memo[id(x)]\\n except keyerror:\\n pass\\n for k, j in zip(x, y):\\n if k is not j:\\n y = tuple(y)\\n break\\n else:\\n y = x\\n return y\\n d[tuple] = _deepcopy_tuple\\n def _deepcopy_dict(x, memo):\\n y = {}\\n memo[id(x)] = y\\n for key, value in x.items():\\n y[deepcopy(key, memo)] = deepcopy(value, memo)\\n return y\\n d[dict] = _deepcopy_dict\\n if pystringmap is not none:\\n d[pystringmap] = _deepcopy_dict\\n def _deepcopy_method(x, memo): # copy instance methods\\n return type(x)(x.__func__, deepcopy(x.__self__, memo))\\n _deepcopy_dispatch[types.methodtype] = _deepcopy_method\\n def _keep_alive(x, memo):\\n \"\"\"keeps a reference to the object x in the memo.\\n because we remember objects by their id, we have\\n to assure that possibly temporary objects are kept\\n alive by referencing them.\\n we store a reference at the id of the memo, which should\\n normally not be used unless someone tries to deepcopy\\n the memo itself...\\n \"\"\"\\n try:\\n memo[id(memo)].append(x)\\n except keyerror:\\n # aha, this is the first one :-)\\n memo[id(memo)]=[x]\\n def _reconstruct(x, info, deep, memo=none):\\n if isinstance(info, str):\\n return x\\n assert isinstance(info, tuple)\\n if memo is none:\\n memo = {}\\n n = len(info)\\n assert n in (2, 3, 4, 5)\\n callable, args = info[:2]\\n if n > 2:\\n state = info[2]\\n else:\\n state = none\\n if n > 3:\\n listiter = info[3]\\n else:\\n listiter = none\\n if n > 4:\\n dictiter = info[4]\\n else:\\n dictiter = none\\n if deep:\\n args = deepcopy(args, memo)\\n y = callable(*args)\\n memo[id(x)] = y\\n if state is not none:\\n if deep:\\n state = deepcopy(state, memo)\\n if hasattr(y, \\'__setstate__\\'):\\n y.__setstate__(state)\\n else:\\n if isinstance(state, tuple) and len(state) == 2:\\n state, slotstate = state\\n else:\\n slotstate = none\\n if state is not none:\\n y.__dict__.update(state)\\n if slotstate is not none:\\n for key, value in slotstate.items():\\n setattr(y, key, value)\\n if listiter is not none:\\n for item in listiter:\\n if deep:\\n item = deepcopy(item, memo)\\n y.append(item)\\n if dictiter is not none:\\n for key, value in dictiter:\\n if deep:\\n key = deepcopy(key, memo)\\n value = deepcopy(value, memo)\\n y[key] = value\\n return y\\n del d\\n del types\\n # helper for instance creation without calling __init__\\n class _emptyclass:\\n pass\\n \"\"\"helper to provide extensibility for pickle.\\n this is only useful to add pickle support for extension types defined in\\n c, not for instances of user-defined classes.\\n \"\"\"\\n __all__ = [\"pickle\", \"constructor\",\\n \"add_extension\", \"remove_extension\", \"clear_extension_cache\"]\\n dispatch_table = {}\\n def pickle(ob_type, pickle_function, constructor_ob=none):\\n if not callable(pickle_function):\\n raise typeerror(\"reduction functions must be callable\")\\n dispatch_table[ob_type] = pickle_function\\n # the constructor_ob function is a vestige of safe for unpickling.\\n # there is no reason for the caller to pass it anymore.\\n if constructor_ob is not none:\\n constructor(constructor_ob)\\n def constructor(object):\\n if not callable(object):\\n raise typeerror(\"constructors must be callable\")\\n # example: provide pickling support for complex numbers.\\n try:\\n complex\\n except nameerror:\\n pass\\n else:\\n def pickle_complex(c):\\n return complex, (c.real, c.imag)\\n pickle(complex, pickle_complex, complex)\\n # support for pickling new-style objects\\n def _reconstructor(cls, base, state):\\n if base is object:\\n obj = object.__new__(cls)\\n else:\\n obj = base.__new__(cls, state)\\n if base.__init__ != object.__init__:\\n base.__init__(obj, state)\\n return obj\\n _heaptype = 1<<9\\n # python code for object.__reduce_ex__ for protocols 0 and 1\\n def _reduce_ex(self, proto):\\n assert proto < 2\\n for base in self.__class__.__mro__:\\n if hasattr(base, \\'__flags__\\') and not base.__flags__ & _heaptype:\\n break\\n else:\\n base = object # not really reachable\\n if base is object:\\n state = none\\n else:\\n if base is self.__class__:\\n raise typeerror(\"can\\'t pickle %s objects\" % base.__name__)\\n state = base(self)\\n args = (self.__class__, base, state)\\n try:\\n getstate = self.__getstate__\\n except attributeerror:\\n if getattr(self, \"__slots__\", none):\\n raise typeerror(\"a class that defines __slots__ without \"\\n \"defining __getstate__ cannot be pickled\")\\n try:\\n dict = self.__dict__\\n except attributeerror:\\n dict = none\\n else:\\n dict = getstate()\\n if dict:\\n return _reconstructor, args, dict\\n else:\\n return _reconstructor, args\\n # helper for __reduce_ex__ protocol 2\\n def __newobj__(cls, *args):\\n return cls.__new__(cls, *args)\\n def __newobj_ex__(cls, args, kwargs):\\n \"\"\"used by pickle protocol 4, instead of __newobj__ to allow classes with\\n keyword-only arguments to be pickled correctly.\\n \"\"\"\\n return cls.__new__(cls, *args, **kwargs)\\n def _slotnames(cls):\\n \"\"\"return a list of slot names for a given class.\\n this needs to find slots defined by the class and its bases, so we\\n can\\'t simply return the __slots__ attribute.  we must walk down\\n the method resolution order and concatenate the __slots__ of each\\n class found there.  (this assumes classes don\\'t modify their\\n __slots__ attribute to misrepresent their slots after the class is\\n defined.)\\n \"\"\"\\n # get the value from a cache in the class if possible\\n names = cls.__dict__.get(\"__slotnames__\")\\n if names is not none:\\n return names\\n # not cached -- calculate the value\\n names = []\\n if not hasattr(cls, \"__slots__\"):\\n # this class has no slots\\n pass\\n else:\\n # slots found -- gather slot names from all base classes\\n for c in cls.__mro__:\\n if \"__slots__\" in c.__dict__:\\n slots = c.__dict__[\\'__slots__\\']\\n # if class has a single slot, it can be given as a string\\n if isinstance(slots, str):\\n slots = (slots,)\\n for name in slots:\\n # special descriptors\\n if name in (\"__dict__\", \"__weakref__\"):\\n continue\\n # mangled names\\n elif name.startswith(\\'__\\') and not name.endswith(\\'__\\'):\\n names.append(\\'_%s%s\\' % (c.__name__, name))\\n else:\\n names.append(name)\\n # cache the outcome in the class if at all possible\\n try:\\n cls.__slotnames__ = names\\n except:\\n pass # but don\\'t die if we can\\'t\\n return names\\n # a registry of extension codes.  this is an ad-hoc compression\\n # mechanism.  whenever a global reference to <module>, <name> is about\\n # to be pickled, the (<module>, <name>) tuple is looked up here to see\\n # if it is a registered extension code for it.  extension codes are\\n # universal, so that the meaning of a pickle does not depend on\\n # context.  (there are also some codes reserved for local use that\\n # don\\'t have this restriction.)  codes are positive ints; 0 is\\n # reserved.\\n _extension_registry = {}                # key -> code\\n _inverted_registry = {}                 # code -> key\\n _extension_cache = {}                   # code -> object\\n # don\\'t ever rebind those names:  pickling grabs a reference to them when\\n # it\\'s initialized, and won\\'t see a rebinding.\\n def add_extension(module, name, code):\\n \"\"\"register an extension code.\"\"\"\\n code = int(code)\\n if not 1 <= code <= 0x7fffffff:\\n raise valueerror(\"code out of range\")\\n key = (module, name)\\n if (_extension_registry.get(key) == code and\\n _inverted_registry.get(code) == key):\\n return # redundant registrations are benign\\n if key in _extension_registry:\\n raise valueerror(\"key %s is already registered with code %s\" %\\n (key, _extension_registry[key]))\\n if code in _inverted_registry:\\n raise valueerror(\"code %s is already in use for key %s\" %\\n (code, _inverted_registry[code]))\\n _extension_registry[key] = code\\n _inverted_registry[code] = key\\n def remove_extension(module, name, code):\\n \"\"\"unregister an extension code.  for testing only.\"\"\"\\n key = (module, name)\\n if (_extension_registry.get(key) != code or\\n _inverted_registry.get(code) != key):\\n raise valueerror(\"key %s is not registered with code %s\" %\\n (key, code))\\n del _extension_registry[key]\\n del _inverted_registry[code]\\n if code in _extension_cache:\\n del _extension_cache[code]\\n def clear_extension_cache():\\n _extension_cache.clear()\\n # standard extension code assignments\\n # reserved ranges\\n # first  last count  purpose\\n #     1   127   127  reserved for python standard library\\n #   128   191    64  reserved for zope\\n #   192   239    48  reserved for 3rd parties\\n #   240   255    16  reserved for private use (will never be assigned)\\n #   256   inf   inf  reserved for future assignment\\n # extension codes are assigned by the python software foundation.\\n \"\"\"drop-in replacement for the thread module.\\n meant to be used as a brain-dead substitute so that threaded code does\\n not need to be rewritten for when the thread module is not present.\\n suggested usage is::\\n try:\\n import _thread\\n except importerror:\\n import _dummy_thread as _thread\\n \"\"\"\\n # exports only things specified by thread documentation;\\n # skipping obsolete synonyms allocate(), start_new(), exit_thread().\\n __all__ = [\\'error\\', \\'start_new_thread\\', \\'exit\\', \\'get_ident\\', \\'allocate_lock\\',\\n \\'interrupt_main\\', \\'locktype\\']\\n # a dummy value\\n timeout_max = 2**31\\n # note: this module can be imported early in the extension building process,\\n # and so top level imports of other modules should be avoided.  instead, all\\n # imports are done when needed on a function-by-function basis.  since threads\\n # are disabled, the import lock should not be an issue anyway (??).\\n error = runtimeerror\\n def start_new_thread(function, args, kwargs={}):\\n \"\"\"dummy implementation of _thread.start_new_thread().\\n compatibility is maintained by making sure that ``args`` is a\\n tuple and ``kwargs`` is a dictionary.  if an exception is raised\\n and it is systemexit (which can be done by _thread.exit()) it is\\n caught and nothing is done; all other exceptions are printed out\\n by using traceback.print_exc().\\n if the executed function calls interrupt_main the keyboardinterrupt will be\\n raised when the function returns.\\n \"\"\"\\n if type(args) != type(tuple()):\\n raise typeerror(\"2nd arg must be a tuple\")\\n if type(kwargs) != type(dict()):\\n raise typeerror(\"3rd arg must be a dict\")\\n global _main\\n _main = false\\n try:\\n function(*args, **kwargs)\\n except systemexit:\\n pass\\n except:\\n import traceback\\n traceback.print_exc()\\n _main = true\\n global _interrupt\\n if _interrupt:\\n _interrupt = false\\n raise keyboardinterrupt\\n def exit():\\n \"\"\"dummy implementation of _thread.exit().\"\"\"\\n raise systemexit\\n def get_ident():\\n \"\"\"dummy implementation of _thread.get_ident().\\n since this module should only be used when _threadmodule is not\\n available, it is safe to assume that the current process is the\\n only thread.  thus a constant can be safely returned.\\n \"\"\"\\n return -1\\n def allocate_lock():\\n \"\"\"dummy implementation of _thread.allocate_lock().\"\"\"\\n return locktype()\\n def stack_size(size=none):\\n \"\"\"dummy implementation of _thread.stack_size().\"\"\"\\n if size is not none:\\n raise error(\"setting thread stack size not supported\")\\n return 0\\n def _set_sentinel():\\n \"\"\"dummy implementation of _thread._set_sentinel().\"\"\"\\n return locktype()\\n class locktype(object):\\n \"\"\"class implementing dummy implementation of _thread.locktype.\\n compatibility is maintained by maintaining self.locked_status\\n which is a boolean that stores the state of the lock.  pickling of\\n the lock, though, should not be done since if the _thread module is\\n then used with an unpickled ``lock()`` from here problems could\\n occur from this class not having atomic methods.\\n \"\"\"\\n def __init__(self):\\n self.locked_status = false\\n def acquire(self, waitflag=none, timeout=-1):\\n \"\"\"dummy implementation of acquire().\\n for blocking calls, self.locked_status is automatically set to\\n true and returned appropriately based on value of\\n ``waitflag``.  if it is non-blocking, then the value is\\n actually checked and not set if it is already acquired.  this\\n is all done so that threading.condition\\'s assert statements\\n aren\\'t triggered and throw a little fit.\\n \"\"\"\\n if waitflag is none or waitflag:\\n self.locked_status = true\\n return true\\n else:\\n if not self.locked_status:\\n self.locked_status = true\\n return true\\n else:\\n if timeout > 0:\\n import time\\n time.sleep(timeout)\\n return false\\n __enter__ = acquire\\n def __exit__(self, typ, val, tb):\\n self.release()\\n def release(self):\\n \"\"\"release the dummy lock.\"\"\"\\n # xxx perhaps shouldn\\'t actually bother to test?  could lead\\n #     to problems for complex, threaded code.\\n if not self.locked_status:\\n raise error\\n self.locked_status = false\\n return true\\n def locked(self):\\n return self.locked_status\\n def __repr__(self):\\n return \"<%s %s.%s object at %s>\" % (\\n \"locked\" if self.locked_status else \"unlocked\",\\n self.__class__.__module__,\\n self.__class__.__qualname__,\\n hex(id(self))\\n )\\n # used to signal that interrupt_main was called in a \"thread\"\\n _interrupt = false\\n # true when not executing in a \"thread\"\\n _main = true\\n def interrupt_main():\\n \"\"\"set _interrupt flag to true to have start_new_thread raise\\n keyboardinterrupt upon exiting.\"\"\"\\n if _main:\\n raise keyboardinterrupt\\n else:\\n global _interrupt\\n _interrupt = true\\n \"\"\"filename matching with shell patterns.\\n fnmatch(filename, pattern) matches according to the local convention.\\n fnmatchcase(filename, pattern) always takes case in account.\\n the functions operate by translating the pattern into a regular\\n expression.  they cache the compiled regular expressions for speed.\\n the function translate(pattern) returns a regular expression\\n corresponding to pattern.  (it does not compile it.)\\n \"\"\"\\n import os\\n import posixpath\\n import re\\n import functools\\n __all__ = [\"filter\", \"fnmatch\", \"fnmatchcase\", \"translate\"]\\n def fnmatch(name, pat):\\n \"\"\"test whether filename matches pattern.\\n patterns are unix shell style:\\n *       matches everything\\n ?       matches any single character\\n [seq]   matches any character in seq\\n [!seq]  matches any char not in seq\\n an initial period in filename is not special.\\n both filename and pattern are first case-normalized\\n if the operating system requires it.\\n if you don\\'t want this, use fnmatchcase(filename, pattern).\\n \"\"\"\\n name = os.path.normcase(name)\\n pat = os.path.normcase(pat)\\n return fnmatchcase(name, pat)\\n @functools.lru_cache(maxsize=256, typed=true)\\n def _compile_pattern(pat):\\n if isinstance(pat, bytes):\\n pat_str = str(pat, \\'iso-8859-1\\')\\n res_str = translate(pat_str)\\n res = bytes(res_str, \\'iso-8859-1\\')\\n else:\\n res = translate(pat)\\n return re.compile(res).match\\n def filter(names, pat):\\n \"\"\"return the subset of the list names that match pat.\"\"\"\\n result = []\\n pat = os.path.normcase(pat)\\n match = _compile_pattern(pat)\\n if os.path is posixpath:\\n # normcase on posix is nop. optimize it away from the loop.\\n for name in names:\\n if match(name):\\n result.append(name)\\n else:\\n for name in names:\\n if match(os.path.normcase(name)):\\n result.append(name)\\n return result\\n def fnmatchcase(name, pat):\\n \"\"\"test whether filename matches pattern, including case.\\n this is a version of fnmatch() which doesn\\'t case-normalize\\n its arguments.\\n \"\"\"\\n match = _compile_pattern(pat)\\n return match(name) is not none\\n def translate(pat):\\n \"\"\"translate a shell pattern to a regular expression.\\n there is no way to quote meta-characters.\\n \"\"\"\\n i, n = 0, len(pat)\\n res = \\'\\'\\n while i < n:\\n c = pat[i]\\n i = i+1\\n if c == \\'*\\':\\n res = res + \\'.*\\'\\n elif c == \\'?\\':\\n res = res + \\'.\\'\\n elif c == \\'[\\':\\n j = i\\n if j < n and pat[j] == \\'!\\':\\n j = j+1\\n if j < n and pat[j] == \\']\\':\\n j = j+1\\n while j < n and pat[j] != \\']\\':\\n j = j+1\\n if j >= n:\\n res = res + \\'\\\\\\\\[\\'\\n else:\\n stuff = pat[i:j].replace(\\'\\\\\\\\\\',\\'\\\\\\\\\\\\\\\\\\')\\n i = j+1\\n if stuff[0] == \\'!\\':\\n stuff = \\'^\\' + stuff[1:]\\n elif stuff[0] == \\'^\\':\\n stuff = \\'\\\\\\\\\\' + stuff\\n res = \\'%s[%s]\\' % (res, stuff)\\n else:\\n res = res + re.escape(c)\\n return res + \\'\\\\z(?ms)\\'\\n \"\"\"functools.py - tools for working with functions and callable objects\\n \"\"\"\\n # python module wrapper for _functools c module\\n # to allow utilities written in python to be added\\n # to the functools module.\\n # written by nick coghlan <ncoghlan at gmail.com>,\\n # raymond hettinger <python at rcn.com>,\\n # and ukasz langa <lukasz at langa.pl>.\\n #   copyright (c) 2006-2013 python software foundation.\\n # see c source code for _functools credits/copyright\\n __all__ = [\\'update_wrapper\\', \\'wraps\\', \\'wrapper_assignments\\', \\'wrapper_updates\\',\\n \\'total_ordering\\', \\'cmp_to_key\\', \\'lru_cache\\', \\'reduce\\', \\'partial\\',\\n \\'partialmethod\\', \\'singledispatch\\']\\n try:\\n from _functools import reduce\\n except importerror:\\n pass\\n from abc import get_cache_token\\n from collections import namedtuple\\n from types import mappingproxytype\\n from weakref import weakkeydictionary\\n try:\\n from _thread import rlock\\n except importerror:\\n class rlock:\\n \\'dummy reentrant lock for builds without threads\\'\\n def __enter__(self): pass\\n def __exit__(self, exctype, excinst, exctb): pass\\n ################################################################################\\n ### update_wrapper() and wraps() decorator\\n ################################################################################\\n # update_wrapper() and wraps() are tools to help write\\n # wrapper functions that can handle naive introspection\\n wrapper_assignments = (\\'__module__\\', \\'__name__\\', \\'__qualname__\\', \\'__doc__\\',\\n \\'__annotations__\\')\\n wrapper_updates = (\\'__dict__\\',)\\n def update_wrapper(wrapper,\\n wrapped,\\n assigned = wrapper_assignments,\\n updated = wrapper_updates):\\n \"\"\"update a wrapper function to look like the wrapped function\\n wrapper is the function to be updated\\n wrapped is the original function\\n assigned is a tuple naming the attributes assigned directly\\n from the wrapped function to the wrapper function (defaults to\\n functools.wrapper_assignments)\\n updated is a tuple naming the attributes of the wrapper that\\n are updated with the corresponding attribute from the wrapped\\n function (defaults to functools.wrapper_updates)\\n \"\"\"\\n for attr in assigned:\\n try:\\n value = getattr(wrapped, attr)\\n except attributeerror:\\n pass\\n else:\\n setattr(wrapper, attr, value)\\n for attr in updated:\\n getattr(wrapper, attr).update(getattr(wrapped, attr, {}))\\n # issue #17482: set __wrapped__ last so we don\\'t inadvertently copy it\\n # from the wrapped function when updating __dict__\\n wrapper.__wrapped__ = wrapped\\n # return the wrapper so this can be used as a decorator via partial()\\n return wrapper\\n def wraps(wrapped,\\n assigned = wrapper_assignments,\\n updated = wrapper_updates):\\n \"\"\"decorator factory to apply update_wrapper() to a wrapper function\\n returns a decorator that invokes update_wrapper() with the decorated\\n function as the wrapper argument and the arguments to wraps() as the\\n remaining arguments. default arguments are as for update_wrapper().\\n this is a convenience function to simplify applying partial() to\\n update_wrapper().\\n \"\"\"\\n return partial(update_wrapper, wrapped=wrapped,\\n assigned=assigned, updated=updated)\\n ################################################################################\\n ### total_ordering class decorator\\n ################################################################################\\n # the total ordering functions all invoke the root magic method directly\\n # rather than using the corresponding operator.  this avoids possible\\n # infinite recursion that could occur when the operator dispatch logic\\n # detects a notimplemented result and then calls a reflected method.\\n def _gt_from_lt(self, other, notimplemented=notimplemented):\\n \\'return a > b.  computed by @total_ordering from (not a < b) and (a != b).\\'\\n op_result = self.__lt__(other)\\n if op_result is notimplemented:\\n return op_result\\n return not op_result and self != other\\n def _le_from_lt(self, other, notimplemented=notimplemented):\\n \\'return a <= b.  computed by @total_ordering from (a < b) or (a == b).\\'\\n op_result = self.__lt__(other)\\n return op_result or self == other\\n def _ge_from_lt(self, other, notimplemented=notimplemented):\\n \\'return a >= b.  computed by @total_ordering from (not a < b).\\'\\n op_result = self.__lt__(other)\\n if op_result is notimplemented:\\n return op_result\\n return not op_result\\n def _ge_from_le(self, other, notimplemented=notimplemented):\\n \\'return a >= b.  computed by @total_ordering from (not a <= b) or (a == b).\\'\\n op_result = self.__le__(other)\\n if op_result is notimplemented:\\n return op_result\\n return not op_result or self == other\\n def _lt_from_le(self, other, notimplemented=notimplemented):\\n \\'return a < b.  computed by @total_ordering from (a <= b) and (a != b).\\'\\n op_result = self.__le__(other)\\n if op_result is notimplemented:\\n return op_result\\n return op_result and self != other\\n def _gt_from_le(self, other, notimplemented=notimplemented):\\n \\'return a > b.  computed by @total_ordering from (not a <= b).\\'\\n op_result = self.__le__(other)\\n if op_result is notimplemented:\\n return op_result\\n return not op_result\\n def _lt_from_gt(self, other, notimplemented=notimplemented):\\n \\'return a < b.  computed by @total_ordering from (not a > b) and (a != b).\\'\\n op_result = self.__gt__(other)\\n if op_result is notimplemented:\\n return op_result\\n return not op_result and self != other\\n def _ge_from_gt(self, other, notimplemented=notimplemented):\\n \\'return a >= b.  computed by @total_ordering from (a > b) or (a == b).\\'\\n op_result = self.__gt__(other)\\n return op_result or self == other\\n def _le_from_gt(self, other, notimplemented=notimplemented):\\n \\'return a <= b.  computed by @total_ordering from (not a > b).\\'\\n op_result = self.__gt__(other)\\n if op_result is notimplemented:\\n return op_result\\n return not op_result\\n def _le_from_ge(self, other, notimplemented=notimplemented):\\n \\'return a <= b.  computed by @total_ordering from (not a >= b) or (a == b).\\'\\n op_result = self.__ge__(other)\\n if op_result is notimplemented:\\n return op_result\\n return not op_result or self == other\\n def _gt_from_ge(self, other, notimplemented=notimplemented):\\n \\'return a > b.  computed by @total_ordering from (a >= b) and (a != b).\\'\\n op_result = self.__ge__(other)\\n if op_result is notimplemented:\\n return op_result\\n return op_result and self != other\\n def _lt_from_ge(self, other, notimplemented=notimplemented):\\n \\'return a < b.  computed by @total_ordering from (not a >= b).\\'\\n op_result = self.__ge__(other)\\n if op_result is notimplemented:\\n return op_result\\n return not op_result\\n _convert = {\\n \\'__lt__\\': [(\\'__gt__\\', _gt_from_lt),\\n (\\'__le__\\', _le_from_lt),\\n (\\'__ge__\\', _ge_from_lt)],\\n \\'__le__\\': [(\\'__ge__\\', _ge_from_le),\\n (\\'__lt__\\', _lt_from_le),\\n (\\'__gt__\\', _gt_from_le)],\\n \\'__gt__\\': [(\\'__lt__\\', _lt_from_gt),\\n (\\'__ge__\\', _ge_from_gt),\\n (\\'__le__\\', _le_from_gt)],\\n \\'__ge__\\': [(\\'__le__\\', _le_from_ge),\\n (\\'__gt__\\', _gt_from_ge),\\n (\\'__lt__\\', _lt_from_ge)]\\n }\\n def total_ordering(cls):\\n \"\"\"class decorator that fills in missing ordering methods\"\"\"\\n # find user-defined comparisons (not those inherited from object).\\n roots = [op for op in _convert if getattr(cls, op, none) is not getattr(object, op, none)]\\n if not roots:\\n raise valueerror(\\'must define at least one ordering operation: < > <= >=\\')\\n root = max(roots)       # prefer __lt__ to __le__ to __gt__ to __ge__\\n for opname, opfunc in _convert[root]:\\n if opname not in roots:\\n opfunc.__name__ = opname\\n setattr(cls, opname, opfunc)\\n return cls\\n ################################################################################\\n ### cmp_to_key() function converter\\n ################################################################################\\n def cmp_to_key(mycmp):\\n \"\"\"convert a cmp= function into a key= function\"\"\"\\n class k(object):\\n __slots__ = [\\'obj\\']\\n def __init__(self, obj):\\n self.obj = obj\\n def __lt__(self, other):\\n return mycmp(self.obj, other.obj) < 0\\n def __gt__(self, other):\\n return mycmp(self.obj, other.obj) > 0\\n def __eq__(self, other):\\n return mycmp(self.obj, other.obj) == 0\\n def __le__(self, other):\\n return mycmp(self.obj, other.obj) <= 0\\n def __ge__(self, other):\\n return mycmp(self.obj, other.obj) >= 0\\n __hash__ = none\\n return k\\n try:\\n from _functools import cmp_to_key\\n except importerror:\\n pass\\n ################################################################################\\n ### partial() argument application\\n ################################################################################\\n # purely functional, no descriptor behaviour\\n def partial(func, *args, **keywords):\\n \"\"\"new function with partial application of the given arguments\\n and keywords.\\n \"\"\"\\n if hasattr(func, \\'func\\'):\\n args = func.args + args\\n tmpkw = func.keywords.copy()\\n tmpkw.update(keywords)\\n keywords = tmpkw\\n del tmpkw\\n func = func.func\\n def newfunc(*fargs, **fkeywords):\\n newkeywords = keywords.copy()\\n newkeywords.update(fkeywords)\\n return func(*(args + fargs), **newkeywords)\\n newfunc.func = func\\n newfunc.args = args\\n newfunc.keywords = keywords\\n return newfunc\\n try:\\n from _functools import partial\\n except importerror:\\n pass\\n # descriptor version\\n class partialmethod(object):\\n \"\"\"method descriptor with partial application of the given arguments\\n and keywords.\\n supports wrapping existing descriptors and handles non-descriptor\\n callables as instance methods.\\n \"\"\"\\n def __init__(self, func, *args, **keywords):\\n if not callable(func) and not hasattr(func, \"__get__\"):\\n raise typeerror(\"{!r} is not callable or a descriptor\"\\n .format(func))\\n # func could be a descriptor like classmethod which isn\\'t callable,\\n # so we can\\'t inherit from partial (it verifies func is callable)\\n if isinstance(func, partialmethod):\\n # flattening is mandatory in order to place cls/self before all\\n # other arguments\\n # it\\'s also more efficient since only one function will be called\\n self.func = func.func\\n self.args = func.args + args\\n self.keywords = func.keywords.copy()\\n self.keywords.update(keywords)\\n else:\\n self.func = func\\n self.args = args\\n self.keywords = keywords\\n def __repr__(self):\\n args = \", \".join(map(repr, self.args))\\n keywords = \", \".join(\"{}={!r}\".format(k, v)\\n for k, v in self.keywords.items())\\n format_string = \"{module}.{cls}({func}, {args}, {keywords})\"\\n return format_string.format(module=self.__class__.__module__,\\n cls=self.__class__.__qualname__,\\n func=self.func,\\n args=args,\\n keywords=keywords)\\n def _make_unbound_method(self):\\n def _method(*args, **keywords):\\n call_keywords = self.keywords.copy()\\n call_keywords.update(keywords)\\n cls_or_self, *rest = args\\n call_args = (cls_or_self,) + self.args + tuple(rest)\\n return self.func(*call_args, **call_keywords)\\n _method.__isabstractmethod__ = self.__isabstractmethod__\\n _method._partialmethod = self\\n return _method\\n def __get__(self, obj, cls):\\n get = getattr(self.func, \"__get__\", none)\\n result = none\\n if get is not none:\\n new_func = get(obj, cls)\\n if new_func is not self.func:\\n # assume __get__ returning something new indicates the\\n # creation of an appropriate callable\\n result = partial(new_func, *self.args, **self.keywords)\\n try:\\n result.__self__ = new_func.__self__\\n except attributeerror:\\n pass\\n if result is none:\\n # if the underlying descriptor didn\\'t do anything, treat this\\n # like an instance method\\n result = self._make_unbound_method().__get__(obj, cls)\\n return result\\n @property\\n def __isabstractmethod__(self):\\n return getattr(self.func, \"__isabstractmethod__\", false)\\n ################################################################################\\n ### lru cache function decorator\\n ################################################################################\\n _cacheinfo = namedtuple(\"cacheinfo\", [\"hits\", \"misses\", \"maxsize\", \"currsize\"])\\n class _hashedseq(list):\\n \"\"\" this class guarantees that hash() will be called no more than once\\n per element.  this is important because the lru_cache() will hash\\n the key multiple times on a cache miss.\\n \"\"\"\\n __slots__ = \\'hashvalue\\'\\n def __init__(self, tup, hash=hash):\\n self[:] = tup\\n self.hashvalue = hash(tup)\\n def __hash__(self):\\n return self.hashvalue\\n def _make_key(args, kwds, typed,\\n kwd_mark = (object(),),\\n fasttypes = {int, str, frozenset, type(none)},\\n sorted=sorted, tuple=tuple, type=type, len=len):\\n \"\"\"make a cache key from optionally typed positional and keyword arguments\\n the key is constructed in a way that is flat as possible rather than\\n as a nested structure that would take more memory.\\n if there is only a single argument and its data type is known to cache\\n its hash value, then that argument is returned without a wrapper.  this\\n saves space and improves lookup speed.\\n \"\"\"\\n key = args\\n if kwds:\\n sorted_items = sorted(kwds.items())\\n key += kwd_mark\\n for item in sorted_items:\\n key += item\\n if typed:\\n key += tuple(type(v) for v in args)\\n if kwds:\\n key += tuple(type(v) for k, v in sorted_items)\\n elif len(key) == 1 and type(key[0]) in fasttypes:\\n return key[0]\\n return _hashedseq(key)\\n def lru_cache(maxsize=128, typed=false):\\n \"\"\"least-recently-used cache decorator.\\n if *maxsize* is set to none, the lru features are disabled and the cache\\n can grow without bound.\\n if *typed* is true, arguments of different types will be cached separately.\\n for example, f(3.0) and f(3) will be treated as distinct calls with\\n distinct results.\\n arguments to the cached function must be hashable.\\n view the cache statistics named tuple (hits, misses, maxsize, currsize)\\n with f.cache_info().  clear the cache and statistics with f.cache_clear().\\n access the underlying function with f.__wrapped__.\\n see:  http://en.wikipedia.org/wiki/cache_algorithms#least_recently_used\\n \"\"\"\\n # users should only access the lru_cache through its public api:\\n #       cache_info, cache_clear, and f.__wrapped__\\n # the internals of the lru_cache are encapsulated for thread safety and\\n # to allow the implementation to change (including a possible c version).\\n # early detection of an erroneous call to @lru_cache without any arguments\\n # resulting in the inner function being passed to maxsize instead of an\\n # integer or none.\\n if maxsize is not none and not isinstance(maxsize, int):\\n raise typeerror(\\'expected maxsize to be an integer or none\\')\\n def decorating_function(user_function):\\n wrapper = _lru_cache_wrapper(user_function, maxsize, typed, _cacheinfo)\\n return update_wrapper(wrapper, user_function)\\n return decorating_function\\n def _lru_cache_wrapper(user_function, maxsize, typed, _cacheinfo):\\n # constants shared by all lru cache instances:\\n sentinel = object()          # unique object used to signal cache misses\\n make_key = _make_key         # build a key from the function arguments\\n prev, next, key, result = 0, 1, 2, 3   # names for the link fields\\n cache = {}\\n hits = misses = 0\\n full = false\\n cache_get = cache.get    # bound method to lookup a key or return none\\n lock = rlock()           # because linkedlist updates aren\\'t threadsafe\\n root = []                # root of the circular doubly linked list\\n root[:] = [root, root, none, none]     # initialize by pointing to self\\n if maxsize == 0:\\n def wrapper(*args, **kwds):\\n # no caching -- just a statistics update after a successful call\\n nonlocal misses\\n result = user_function(*args, **kwds)\\n misses += 1\\n return result\\n elif maxsize is none:\\n def wrapper(*args, **kwds):\\n # simple caching without ordering or size limit\\n nonlocal hits, misses\\n key = make_key(args, kwds, typed)\\n result = cache_get(key, sentinel)\\n if result is not sentinel:\\n hits += 1\\n return result\\n result = user_function(*args, **kwds)\\n cache[key] = result\\n misses += 1\\n return result\\n else:\\n def wrapper(*args, **kwds):\\n # size limited caching that tracks accesses by recency\\n nonlocal root, hits, misses, full\\n key = make_key(args, kwds, typed)\\n with lock:\\n link = cache_get(key)\\n if link is not none:\\n # move the link to the front of the circular queue\\n link_prev, link_next, _key, result = link\\n link_prev[next] = link_next\\n link_next[prev] = link_prev\\n last = root[prev]\\n last[next] = root[prev] = link\\n link[prev] = last\\n link[next] = root\\n hits += 1\\n return result\\n result = user_function(*args, **kwds)\\n with lock:\\n if key in cache:\\n # getting here means that this same key was added to the\\n # cache while the lock was released.  since the link\\n # update is already done, we need only return the\\n # computed result and update the count of misses.\\n pass\\n elif full:\\n # use the old root to store the new key and result.\\n oldroot = root\\n oldroot[key] = key\\n oldroot[result] = result\\n # empty the oldest link and make it the new root.\\n # keep a reference to the old key and old result to\\n # prevent their ref counts from going to zero during the\\n # update. that will prevent potentially arbitrary object\\n # clean-up code (i.e. __del__) from running while we\\'re\\n # still adjusting the links.\\n root = oldroot[next]\\n oldkey = root[key]\\n oldresult = root[result]\\n root[key] = root[result] = none\\n # now update the cache dictionary.\\n del cache[oldkey]\\n # save the potentially reentrant cache[key] assignment\\n # for last, after the root and links have been put in\\n # a consistent state.\\n cache[key] = oldroot\\n else:\\n # put result in a new link at the front of the queue.\\n last = root[prev]\\n link = [last, root, key, result]\\n last[next] = root[prev] = cache[key] = link\\n full = (len(cache) >= maxsize)\\n misses += 1\\n return result\\n def cache_info():\\n \"\"\"report cache statistics\"\"\"\\n with lock:\\n return _cacheinfo(hits, misses, maxsize, len(cache))\\n def cache_clear():\\n \"\"\"clear the cache and cache statistics\"\"\"\\n nonlocal hits, misses, full\\n with lock:\\n cache.clear()\\n root[:] = [root, root, none, none]\\n hits = misses = 0\\n full = false\\n wrapper.cache_info = cache_info\\n wrapper.cache_clear = cache_clear\\n return wrapper\\n try:\\n from _functools import _lru_cache_wrapper\\n except importerror:\\n pass\\n ################################################################################\\n ### singledispatch() - single-dispatch generic function decorator\\n ################################################################################\\n def _c3_merge(sequences):\\n \"\"\"merges mros in *sequences* to a single mro using the c3 algorithm.\\n adapted from http://www.python.org/download/releases/2.3/mro/.\\n \"\"\"\\n result = []\\n while true:\\n sequences = [s for s in sequences if s]   # purge empty sequences\\n if not sequences:\\n return result\\n for s1 in sequences:   # find merge candidates among seq heads\\n candidate = s1[0]\\n for s2 in sequences:\\n if candidate in s2[1:]:\\n candidate = none\\n break      # reject the current head, it appears later\\n else:\\n break\\n if candidate is none:\\n raise runtimeerror(\"inconsistent hierarchy\")\\n result.append(candidate)\\n # remove the chosen candidate\\n for seq in sequences:\\n if seq[0] == candidate:\\n del seq[0]\\n def _c3_mro(cls, abcs=none):\\n \"\"\"computes the method resolution order using extended c3 linearization.\\n if no *abcs* are given, the algorithm works exactly like the built-in c3\\n linearization used for method resolution.\\n if given, *abcs* is a list of abstract base classes that should be inserted\\n into the resulting mro. unrelated abcs are ignored and don\\'t end up in the\\n result. the algorithm inserts abcs where their functionality is introduced,\\n i.e. issubclass(cls, abc) returns true for the class itself but returns\\n false for all its direct base classes. implicit abcs for a given class\\n (either registered or inferred from the presence of a special method like\\n __len__) are inserted directly after the last abc explicitly listed in the\\n mro of said class. if two implicit abcs end up next to each other in the\\n resulting mro, their ordering depends on the order of types in *abcs*.\\n \"\"\"\\n for i, base in enumerate(reversed(cls.__bases__)):\\n if hasattr(base, \\'__abstractmethods__\\'):\\n boundary = len(cls.__bases__) - i\\n break   # bases up to the last explicit abc are considered first.\\n else:\\n boundary = 0\\n abcs = list(abcs) if abcs else []\\n explicit_bases = list(cls.__bases__[:boundary])\\n abstract_bases = []\\n other_bases = list(cls.__bases__[boundary:])\\n for base in abcs:\\n if issubclass(cls, base) and not any(\\n issubclass(b, base) for b in cls.__bases__\\n ):\\n # if *cls* is the class that introduces behaviour described by\\n # an abc *base*, insert said abc to its mro.\\n abstract_bases.append(base)\\n for base in abstract_bases:\\n abcs.remove(base)\\n explicit_c3_mros = [_c3_mro(base, abcs=abcs) for base in explicit_bases]\\n abstract_c3_mros = [_c3_mro(base, abcs=abcs) for base in abstract_bases]\\n other_c3_mros = [_c3_mro(base, abcs=abcs) for base in other_bases]\\n return _c3_merge(\\n [[cls]] +\\n explicit_c3_mros + abstract_c3_mros + other_c3_mros +\\n [explicit_bases] + [abstract_bases] + [other_bases]\\n )\\n def _compose_mro(cls, types):\\n \"\"\"calculates the method resolution order for a given class *cls*.\\n includes relevant abstract base classes (with their respective bases) from\\n the *types* iterable. uses a modified c3 linearization algorithm.\\n \"\"\"\\n bases = set(cls.__mro__)\\n # remove entries which are already present in the __mro__ or unrelated.\\n def is_related(typ):\\n return (typ not in bases and hasattr(typ, \\'__mro__\\')\\n and issubclass(cls, typ))\\n types = [n for n in types if is_related(n)]\\n # remove entries which are strict bases of other entries (they will end up\\n # in the mro anyway.\\n def is_strict_base(typ):\\n for other in types:\\n if typ != other and typ in other.__mro__:\\n return true\\n return false\\n types = [n for n in types if not is_strict_base(n)]\\n # subclasses of the abcs in *types* which are also implemented by\\n # *cls* can be used to stabilize abc ordering.\\n type_set = set(types)\\n mro = []\\n for typ in types:\\n found = []\\n for sub in typ.__subclasses__():\\n if sub not in bases and issubclass(cls, sub):\\n found.append([s for s in sub.__mro__ if s in type_set])\\n if not found:\\n mro.append(typ)\\n continue\\n # favor subclasses with the biggest number of useful bases\\n found.sort(key=len, reverse=true)\\n for sub in found:\\n for subcls in sub:\\n if subcls not in mro:\\n mro.append(subcls)\\n return _c3_mro(cls, abcs=mro)\\n def _find_impl(cls, registry):\\n \"\"\"returns the best matching implementation from *registry* for type *cls*.\\n where there is no registered implementation for a specific type, its method\\n resolution order is used to find a more generic implementation.\\n note: if *registry* does not contain an implementation for the base\\n *object* type, this function may return none.\\n \"\"\"\\n mro = _compose_mro(cls, registry.keys())\\n match = none\\n for t in mro:\\n if match is not none:\\n # if *match* is an implicit abc but there is another unrelated,\\n # equally matching implicit abc, refuse the temptation to guess.\\n if (t in registry and t not in cls.__mro__\\n and match not in cls.__mro__\\n and not issubclass(match, t)):\\n raise runtimeerror(\"ambiguous dispatch: {} or {}\".format(\\n match, t))\\n break\\n if t in registry:\\n match = t\\n return registry.get(match)\\n def singledispatch(func):\\n \"\"\"single-dispatch generic function decorator.\\n transforms a function into a generic function, which can have different\\n behaviours depending upon the type of its first argument. the decorated\\n function acts as the default implementation, and additional\\n implementations can be registered using the register() attribute of the\\n generic function.\\n \"\"\"\\n registry = {}\\n dispatch_cache = weakkeydictionary()\\n cache_token = none\\n def dispatch(cls):\\n \"\"\"generic_func.dispatch(cls) -> <function implementation>\\n runs the dispatch algorithm to return the best available implementation\\n for the given *cls* registered on *generic_func*.\\n \"\"\"\\n nonlocal cache_token\\n if cache_token is not none:\\n current_token = get_cache_token()\\n if cache_token != current_token:\\n dispatch_cache.clear()\\n cache_token = current_token\\n try:\\n impl = dispatch_cache[cls]\\n except keyerror:\\n try:\\n impl = registry[cls]\\n except keyerror:\\n impl = _find_impl(cls, registry)\\n dispatch_cache[cls] = impl\\n return impl\\n def register(cls, func=none):\\n \"\"\"generic_func.register(cls, func) -> func\\n registers a new implementation for the given *cls* on a *generic_func*.\\n \"\"\"\\n nonlocal cache_token\\n if func is none:\\n return lambda f: register(cls, f)\\n registry[cls] = func\\n if cache_token is none and hasattr(cls, \\'__abstractmethods__\\'):\\n cache_token = get_cache_token()\\n dispatch_cache.clear()\\n return func\\n def wrapper(*args, **kw):\\n return dispatch(args[0].__class__)(*args, **kw)\\n registry[object] = func\\n wrapper.register = register\\n wrapper.dispatch = dispatch\\n wrapper.registry = mappingproxytype(registry)\\n wrapper._clear_cache = dispatch_cache.clear\\n update_wrapper(wrapper, func)\\n return wrapper\\n \"\"\"record of phased-in incompatible language changes.\\n each line is of the form:\\n featurename = \"_feature(\" optionalrelease \",\" mandatoryrelease \",\"\\n compilerflag \")\"\\n where, normally, optionalrelease < mandatoryrelease, and both are 5-tuples\\n of the same form as sys.version_info:\\n (py_major_version, # the 2 in 2.1.0a3; an int\\n py_minor_version, # the 1; an int\\n py_micro_version, # the 0; an int\\n py_release_level, # \"alpha\", \"beta\", \"candidate\" or \"final\"; string\\n py_release_serial # the 3; an int\\n )\\n optionalrelease records the first release in which\\n from __future__ import featurename\\n was accepted.\\n in the case of mandatoryreleases that have not yet occurred,\\n mandatoryrelease predicts the release in which the feature will become part\\n of the language.\\n else mandatoryrelease records when the feature became part of the language;\\n in releases at or after that, modules no longer need\\n from __future__ import featurename\\n to use the feature in question, but may continue to use such imports.\\n mandatoryrelease may also be none, meaning that a planned feature got\\n dropped.\\n instances of class _feature have two corresponding methods,\\n .getoptionalrelease() and .getmandatoryrelease().\\n compilerflag is the (bitfield) flag that should be passed in the fourth\\n argument to the builtin function compile() to enable the feature in\\n dynamically compiled code.  this flag is stored in the .compiler_flag\\n attribute on _future instances.  these values must match the appropriate\\n #defines of co_xxx flags in include/compile.h.\\n no feature line is ever to be deleted from this file.\\n \"\"\"\\n all_feature_names = [\\n \"nested_scopes\",\\n \"generators\",\\n \"division\",\\n \"absolute_import\",\\n \"with_statement\",\\n \"print_function\",\\n \"unicode_literals\",\\n \"barry_as_flufl\",\\n \"generator_stop\",\\n ]\\n __all__ = [\"all_feature_names\"] + all_feature_names\\n # the co_xxx symbols are defined here under the same names used by\\n # compile.h, so that an editor search will find them here.  however,\\n # they\\'re not exported in __all__, because they don\\'t really belong to\\n # this module.\\n co_nested            = 0x0010   # nested_scopes\\n co_generator_allowed = 0        # generators (obsolete, was 0x1000)\\n co_future_division   = 0x2000   # division\\n co_future_absolute_import = 0x4000 # perform absolute imports by default\\n co_future_with_statement  = 0x8000   # with statement\\n co_future_print_function  = 0x10000   # print function\\n co_future_unicode_literals = 0x20000 # unicode string literals\\n co_future_barry_as_bdfl = 0x40000\\n co_future_generator_stop  = 0x80000 # stopiteration becomes runtimeerror in generators\\n class _feature:\\n def __init__(self, optionalrelease, mandatoryrelease, compiler_flag):\\n self.optional = optionalrelease\\n self.mandatory = mandatoryrelease\\n self.compiler_flag = compiler_flag\\n def getoptionalrelease(self):\\n \"\"\"return first release in which this feature was recognized.\\n this is a 5-tuple, of the same form as sys.version_info.\\n \"\"\"\\n return self.optional\\n def getmandatoryrelease(self):\\n \"\"\"return release in which this feature will become mandatory.\\n this is a 5-tuple, of the same form as sys.version_info, or, if\\n the feature was dropped, is none.\\n \"\"\"\\n return self.mandatory\\n def __repr__(self):\\n return \"_feature\" + repr((self.optional,\\n self.mandatory,\\n self.compiler_flag))\\n nested_scopes = _feature((2, 1, 0, \"beta\",  1),\\n (2, 2, 0, \"alpha\", 0),\\n co_nested)\\n generators = _feature((2, 2, 0, \"alpha\", 1),\\n (2, 3, 0, \"final\", 0),\\n co_generator_allowed)\\n division = _feature((2, 2, 0, \"alpha\", 2),\\n (3, 0, 0, \"alpha\", 0),\\n co_future_division)\\n absolute_import = _feature((2, 5, 0, \"alpha\", 1),\\n (3, 0, 0, \"alpha\", 0),\\n co_future_absolute_import)\\n with_statement = _feature((2, 5, 0, \"alpha\", 1),\\n (2, 6, 0, \"alpha\", 0),\\n co_future_with_statement)\\n print_function = _feature((2, 6, 0, \"alpha\", 2),\\n (3, 0, 0, \"alpha\", 0),\\n co_future_print_function)\\n unicode_literals = _feature((2, 6, 0, \"alpha\", 2),\\n (3, 0, 0, \"alpha\", 0),\\n co_future_unicode_literals)\\n barry_as_flufl = _feature((3, 1, 0, \"alpha\", 2),\\n (3, 9, 0, \"alpha\", 0),\\n co_future_barry_as_bdfl)\\n generator_stop = _feature((3, 5, 0, \"beta\", 1),\\n (3, 7, 0, \"alpha\", 0),\\n co_future_generator_stop)\\n \"\"\"\\n path operations common to more than one os\\n do not use directly.  the os specific modules import the appropriate\\n functions from this module themselves.\\n \"\"\"\\n import os\\n import stat\\n __all__ = [\\'commonprefix\\', \\'exists\\', \\'getatime\\', \\'getctime\\', \\'getmtime\\',\\n \\'getsize\\', \\'isdir\\', \\'isfile\\', \\'samefile\\', \\'sameopenfile\\',\\n \\'samestat\\']\\n # does a path exist?\\n # this is false for dangling symbolic links on systems that support them.\\n def exists(path):\\n \"\"\"test whether a path exists.  returns false for broken symbolic links\"\"\"\\n try:\\n os.stat(path)\\n except oserror:\\n return false\\n return true\\n # this follows symbolic links, so both islink() and isdir() can be true\\n # for the same path on systems that support symlinks\\n def isfile(path):\\n \"\"\"test whether a path is a regular file\"\"\"\\n try:\\n st = os.stat(path)\\n except oserror:\\n return false\\n return stat.s_isreg(st.st_mode)\\n # is a path a directory?\\n # this follows symbolic links, so both islink() and isdir()\\n # can be true for the same path on systems that support symlinks\\n def isdir(s):\\n \"\"\"return true if the pathname refers to an existing directory.\"\"\"\\n try:\\n st = os.stat(s)\\n except oserror:\\n return false\\n return stat.s_isdir(st.st_mode)\\n def getsize(filename):\\n \"\"\"return the size of a file, reported by os.stat().\"\"\"\\n return os.stat(filename).st_size\\n def getmtime(filename):\\n \"\"\"return the last modification time of a file, reported by os.stat().\"\"\"\\n return os.stat(filename).st_mtime\\n def getatime(filename):\\n \"\"\"return the last access time of a file, reported by os.stat().\"\"\"\\n return os.stat(filename).st_atime\\n def getctime(filename):\\n \"\"\"return the metadata change time of a file, reported by os.stat().\"\"\"\\n return os.stat(filename).st_ctime\\n # return the longest prefix of all list elements.\\n def commonprefix(m):\\n \"given a list of pathnames, returns the longest common leading component\"\\n if not m: return \\'\\'\\n s1 = min(m)\\n s2 = max(m)\\n for i, c in enumerate(s1):\\n if c != s2[i]:\\n return s1[:i]\\n return s1\\n # are two stat buffers (obtained from stat, fstat or lstat)\\n # describing the same file?\\n def samestat(s1, s2):\\n \"\"\"test whether two stat buffers reference the same file\"\"\"\\n return (s1.st_ino == s2.st_ino and\\n s1.st_dev == s2.st_dev)\\n # are two filenames really pointing to the same file?\\n def samefile(f1, f2):\\n \"\"\"test whether two pathnames reference the same actual file\"\"\"\\n s1 = os.stat(f1)\\n s2 = os.stat(f2)\\n return samestat(s1, s2)\\n # are two open files really referencing the same file?\\n # (not necessarily the same file descriptor!)\\n def sameopenfile(fp1, fp2):\\n \"\"\"test whether two open file objects reference the same file\"\"\"\\n s1 = os.fstat(fp1)\\n s2 = os.fstat(fp2)\\n return samestat(s1, s2)\\n # split a path in root and extension.\\n # the extension is everything starting at the last dot in the last\\n # pathname component; the root is everything before that.\\n # it is always true that root + ext == p.\\n # generic implementation of splitext, to be parametrized with\\n # the separators\\n def _splitext(p, sep, altsep, extsep):\\n \"\"\"split the extension from a pathname.\\n extension is everything from the last dot to the end, ignoring\\n leading dots.  returns \"(root, ext)\"; ext may be empty.\"\"\"\\n # note: this code must work for text and bytes strings.\\n sepindex = p.rfind(sep)\\n if altsep:\\n altsepindex = p.rfind(altsep)\\n sepindex = max(sepindex, altsepindex)\\n dotindex = p.rfind(extsep)\\n if dotindex > sepindex:\\n # skip all leading dots\\n filenameindex = sepindex + 1\\n while filenameindex < dotindex:\\n if p[filenameindex:filenameindex+1] != extsep:\\n return p[:dotindex], p[dotindex:]\\n filenameindex += 1\\n return p, p[:0]\\n def _check_arg_types(funcname, *args):\\n hasstr = hasbytes = false\\n for s in args:\\n if isinstance(s, str):\\n hasstr = true\\n elif isinstance(s, bytes):\\n hasbytes = true\\n else:\\n raise typeerror(\\'%s() argument must be str or bytes, not %r\\' %\\n (funcname, s.__class__.__name__)) from none\\n if hasstr and hasbytes:\\n raise typeerror(\"can\\'t mix strings and bytes in path components\") from none\\n #.  copyright (c) 2005-2010   gregory p. smith (greg@krypto.org)\\n #  licensed to psf under a contributor agreement.\\n #\\n __doc__ = \"\"\"hashlib module - a common interface to many hash functions.\\n new(name, data=b\\'\\') - returns a new hash object implementing the\\n given hash function; initializing the hash\\n using the given binary data.\\n named constructor functions are also available, these are faster\\n than using new(name):\\n md5(), sha1(), sha224(), sha256(), sha384(), and sha512()\\n more algorithms may be available on your platform but the above are guaranteed\\n to exist.  see the algorithms_guaranteed and algorithms_available attributes\\n to find out what algorithm names can be passed to new().\\n note: if you want the adler32 or crc32 hash functions they are available in\\n the zlib module.\\n choose your hash function wisely.  some have known collision weaknesses.\\n sha384 and sha512 will be slow on 32 bit platforms.\\n hash objects have these methods:\\n - update(arg): update the hash object with the bytes in arg. repeated calls\\n are equivalent to a single call with the concatenation of all\\n the arguments.\\n - digest():    return the digest of the bytes passed to the update() method\\n so far.\\n - hexdigest(): like digest() except the digest is returned as a unicode\\n object of double length, containing only hexadecimal digits.\\n - copy():      return a copy (clone) of the hash object. this can be used to\\n efficiently compute the digests of strings that share a common\\n initial substring.\\n for example, to obtain the digest of the string \\'nobody inspects the\\n spammish repetition\\':\\n >>> import hashlib\\n >>> m = hashlib.md5()\\n >>> m.update(b\"nobody inspects\")\\n >>> m.update(b\" the spammish repetition\")\\n >>> m.digest()\\n b\\'\\\\\\\\xbbd\\\\\\\\x9c\\\\\\\\x83\\\\\\\\xdd\\\\\\\\x1e\\\\\\\\xa5\\\\\\\\xc9\\\\\\\\xd9\\\\\\\\xde\\\\\\\\xc9\\\\\\\\xa1\\\\\\\\x8d\\\\\\\\xf0\\\\\\\\xff\\\\\\\\xe9\\'\\n more condensed:\\n >>> hashlib.sha224(b\"nobody inspects the spammish repetition\").hexdigest()\\n \\'a4337bc45a8fc544c03f52dc550cd6e1e87021bc896588bd79e901e2\\'\\n \"\"\"\\n # this tuple and __get_builtin_constructor() must be modified if a new\\n # always available algorithm is added.\\n __always_supported = (\\'md5\\', \\'sha1\\', \\'sha224\\', \\'sha256\\', \\'sha384\\', \\'sha512\\')\\n algorithms_guaranteed = set(__always_supported)\\n algorithms_available = set(__always_supported)\\n __all__ = __always_supported + (\\'new\\', \\'algorithms_guaranteed\\',\\n \\'algorithms_available\\', \\'pbkdf2_hmac\\')\\n __builtin_constructor_cache = {}\\n def __get_builtin_constructor(name):\\n cache = __builtin_constructor_cache\\n constructor = cache.get(name)\\n if constructor is not none:\\n return constructor\\n try:\\n if name in (\\'sha1\\', \\'sha1\\'):\\n import _sha1\\n cache[\\'sha1\\'] = cache[\\'sha1\\'] = _sha1.sha1\\n elif name in (\\'md5\\', \\'md5\\'):\\n import _md5\\n cache[\\'md5\\'] = cache[\\'md5\\'] = _md5.md5\\n elif name in (\\'sha256\\', \\'sha256\\', \\'sha224\\', \\'sha224\\'):\\n import _sha256\\n cache[\\'sha224\\'] = cache[\\'sha224\\'] = _sha256.sha224\\n cache[\\'sha256\\'] = cache[\\'sha256\\'] = _sha256.sha256\\n elif name in (\\'sha512\\', \\'sha512\\', \\'sha384\\', \\'sha384\\'):\\n import _sha512\\n cache[\\'sha384\\'] = cache[\\'sha384\\'] = _sha512.sha384\\n cache[\\'sha512\\'] = cache[\\'sha512\\'] = _sha512.sha512\\n except importerror:\\n pass  # no extension module, this hash is unsupported.\\n constructor = cache.get(name)\\n if constructor is not none:\\n return constructor\\n raise valueerror(\\'unsupported hash type \\' + name)\\n def __get_openssl_constructor(name):\\n try:\\n f = getattr(_hashlib, \\'openssl_\\' + name)\\n # allow the c module to raise valueerror.  the function will be\\n # defined but the hash not actually available thanks to openssl.\\n f()\\n # use the c function directly (very fast)\\n return f\\n except (attributeerror, valueerror):\\n return __get_builtin_constructor(name)\\n def __py_new(name, data=b\\'\\'):\\n \"\"\"new(name, data=b\\'\\') - return a new hashing object using the named algorithm;\\n optionally initialized with data (which must be bytes).\\n \"\"\"\\n return __get_builtin_constructor(name)(data)\\n def __hash_new(name, data=b\\'\\'):\\n \"\"\"new(name, data=b\\'\\') - return a new hashing object using the named algorithm;\\n optionally initialized with data (which must be bytes).\\n \"\"\"\\n try:\\n return _hashlib.new(name, data)\\n except valueerror:\\n # if the _hashlib module (openssl) doesn\\'t support the named\\n # hash, try using our builtin implementations.\\n # this allows for sha224/256 and sha384/512 support even though\\n # the openssl library prior to 0.9.8 doesn\\'t provide them.\\n return __get_builtin_constructor(name)(data)\\n try:\\n import _hashlib\\n new = __hash_new\\n __get_hash = __get_openssl_constructor\\n algorithms_available = algorithms_available.union(\\n _hashlib.openssl_md_meth_names)\\n except importerror:\\n new = __py_new\\n __get_hash = __get_builtin_constructor\\n try:\\n # openssl\\'s pkcs5_pbkdf2_hmac requires openssl 1.0+ with hmac and sha\\n from _hashlib import pbkdf2_hmac\\n except importerror:\\n _trans_5c = bytes((x ^ 0x5c) for x in range(256))\\n _trans_36 = bytes((x ^ 0x36) for x in range(256))\\n def pbkdf2_hmac(hash_name, password, salt, iterations, dklen=none):\\n \"\"\"password based key derivation function 2 (pkcs #5 v2.0)\\n this python implementations based on the hmac module about as fast\\n as openssl\\'s pkcs5_pbkdf2_hmac for short passwords and much faster\\n for long passwords.\\n \"\"\"\\n if not isinstance(hash_name, str):\\n raise typeerror(hash_name)\\n if not isinstance(password, (bytes, bytearray)):\\n password = bytes(memoryview(password))\\n if not isinstance(salt, (bytes, bytearray)):\\n salt = bytes(memoryview(salt))\\n # fast inline hmac implementation\\n inner = new(hash_name)\\n outer = new(hash_name)\\n blocksize = getattr(inner, \\'block_size\\', 64)\\n if len(password) > blocksize:\\n password = new(hash_name, password).digest()\\n password = password + b\\'\\\\x00\\' * (blocksize - len(password))\\n inner.update(password.translate(_trans_36))\\n outer.update(password.translate(_trans_5c))\\n def prf(msg, inner=inner, outer=outer):\\n # pbkdf2_hmac uses the password as key. we can re-use the same\\n # digest objects and just update copies to skip initialization.\\n icpy = inner.copy()\\n ocpy = outer.copy()\\n icpy.update(msg)\\n ocpy.update(icpy.digest())\\n return ocpy.digest()\\n if iterations < 1:\\n raise valueerror(iterations)\\n if dklen is none:\\n dklen = outer.digest_size\\n if dklen < 1:\\n raise valueerror(dklen)\\n dkey = b\\'\\'\\n loop = 1\\n from_bytes = int.from_bytes\\n while len(dkey) < dklen:\\n prev = prf(salt + loop.to_bytes(4, \\'big\\'))\\n # endianess doesn\\'t matter here as long to / from use the same\\n rkey = int.from_bytes(prev, \\'big\\')\\n for i in range(iterations - 1):\\n prev = prf(prev)\\n # rkey = rkey ^ prev\\n rkey ^= from_bytes(prev, \\'big\\')\\n loop += 1\\n dkey += rkey.to_bytes(inner.digest_size, \\'big\\')\\n return dkey[:dklen]\\n for __func_name in __always_supported:\\n # try them all, some may not work due to the openssl\\n # version not supporting that algorithm.\\n try:\\n globals()[__func_name] = __get_hash(__func_name)\\n except valueerror:\\n import logging\\n logging.exception(\\'code for hash %s was not found.\\', __func_name)\\n # cleanup locals()\\n del __always_supported, __func_name, __get_hash\\n del __py_new, __hash_new, __get_openssl_constructor\\n \"\"\"heap queue algorithm (a.k.a. priority queue).\\n heaps are arrays for which a[k] <= a[2*k+1] and a[k] <= a[2*k+2] for\\n all k, counting elements from 0.  for the sake of comparison,\\n non-existing elements are considered to be infinite.  the interesting\\n property of a heap is that a[0] is always its smallest element.\\n usage:\\n heap = []            # creates an empty heap\\n heappush(heap, item) # pushes a new item on the heap\\n item = heappop(heap) # pops the smallest item from the heap\\n item = heap[0]       # smallest item on the heap without popping it\\n heapify(x)           # transforms list into a heap, in-place, in linear time\\n item = heapreplace(heap, item) # pops and returns smallest item, and adds\\n # new item; the heap size is unchanged\\n our api differs from textbook heap algorithms as follows:\\n - we use 0-based indexing.  this makes the relationship between the\\n index for a node and the indexes for its children slightly less\\n obvious, but is more suitable since python uses 0-based indexing.\\n - our heappop() method returns the smallest item, not the largest.\\n these two make it possible to view the heap as a regular python list\\n without surprises: heap[0] is the smallest item, and heap.sort()\\n maintains the heap invariant!\\n \"\"\"\\n # original code by kevin o\\'connor, augmented by tim peters and raymond hettinger\\n __about__ = \"\"\"heap queues\\n [explanation by franois pinard]\\n heaps are arrays for which a[k] <= a[2*k+1] and a[k] <= a[2*k+2] for\\n all k, counting elements from 0.  for the sake of comparison,\\n non-existing elements are considered to be infinite.  the interesting\\n property of a heap is that a[0] is always its smallest element.\\n the strange invariant above is meant to be an efficient memory\\n representation for a tournament.  the numbers below are `k\\', not a[k]:\\n 0\\n 1                                 2\\n 3               4                5               6\\n 7       8       9       10      11      12      13      14\\n 15 16   17 18   19 20   21 22   23 24   25 26   27 28   29 30\\n in the tree above, each cell `k\\' is topping `2*k+1\\' and `2*k+2\\'.  in\\n a usual binary tournament we see in sports, each cell is the winner\\n over the two cells it tops, and we can trace the winner down the tree\\n to see all opponents s/he had.  however, in many computer applications\\n of such tournaments, we do not need to trace the history of a winner.\\n to be more memory efficient, when a winner is promoted, we try to\\n replace it by something else at a lower level, and the rule becomes\\n that a cell and the two cells it tops contain three different items,\\n but the top cell \"wins\" over the two topped cells.\\n if this heap invariant is protected at all time, index 0 is clearly\\n the overall winner.  the simplest algorithmic way to remove it and\\n find the \"next\" winner is to move some loser (let\\'s say cell 30 in the\\n diagram above) into the 0 position, and then percolate this new 0 down\\n the tree, exchanging values, until the invariant is re-established.\\n this is clearly logarithmic on the total number of items in the tree.\\n by iterating over all items, you get an o(n ln n) sort.\\n a nice feature of this sort is that you can efficiently insert new\\n items while the sort is going on, provided that the inserted items are\\n not \"better\" than the last 0\\'th element you extracted.  this is\\n especially useful in simulation contexts, where the tree holds all\\n incoming events, and the \"win\" condition means the smallest scheduled\\n time.  when an event schedule other events for execution, they are\\n scheduled into the future, so they can easily go into the heap.  so, a\\n heap is a good structure for implementing schedulers (this is what i\\n used for my midi sequencer :-).\\n various structures for implementing schedulers have been extensively\\n studied, and heaps are good for this, as they are reasonably speedy,\\n the speed is almost constant, and the worst case is not much different\\n than the average case.  however, there are other representations which\\n are more efficient overall, yet the worst cases might be terrible.\\n heaps are also very useful in big disk sorts.  you most probably all\\n know that a big sort implies producing \"runs\" (which are pre-sorted\\n sequences, which size is usually related to the amount of cpu memory),\\n followed by a merging passes for these runs, which merging is often\\n very cleverly organised[1].  it is very important that the initial\\n sort produces the longest runs possible.  tournaments are a good way\\n to that.  if, using all the memory available to hold a tournament, you\\n replace and percolate items that happen to fit the current run, you\\'ll\\n produce runs which are twice the size of the memory for random input,\\n and much better for input fuzzily ordered.\\n moreover, if you output the 0\\'th item on disk and get an input which\\n may not fit in the current tournament (because the value \"wins\" over\\n the last output value), it cannot fit in the heap, so the size of the\\n heap decreases.  the freed memory could be cleverly reused immediately\\n for progressively building a second heap, which grows at exactly the\\n same rate the first heap is melting.  when the first heap completely\\n vanishes, you switch heaps and start a new run.  clever and quite\\n effective!\\n in a word, heaps are useful memory structures to know.  i use them in\\n a few applications, and i think it is good to keep a `heap\\' module\\n around. :-)\\n --------------------\\n [1] the disk balancing algorithms which are current, nowadays, are\\n more annoying than clever, and this is a consequence of the seeking\\n capabilities of the disks.  on devices which cannot seek, like big\\n tape drives, the story was quite different, and one had to be very\\n clever to ensure (far in advance) that each tape movement will be the\\n most effective possible (that is, will best participate at\\n \"progressing\" the merge).  some tapes were even able to read\\n backwards, and this was also used to avoid the rewinding time.\\n believe me, real good tape sorts were quite spectacular to watch!\\n from all times, sorting has always been a great art! :-)\\n \"\"\"\\n __all__ = [\\'heappush\\', \\'heappop\\', \\'heapify\\', \\'heapreplace\\', \\'merge\\',\\n \\'nlargest\\', \\'nsmallest\\', \\'heappushpop\\']\\n def heappush(heap, item):\\n \"\"\"push item onto heap, maintaining the heap invariant.\"\"\"\\n heap.append(item)\\n _siftdown(heap, 0, len(heap)-1)\\n def heappop(heap):\\n \"\"\"pop the smallest item off the heap, maintaining the heap invariant.\"\"\"\\n lastelt = heap.pop()    # raises appropriate indexerror if heap is empty\\n if heap:\\n returnitem = heap[0]\\n heap[0] = lastelt\\n _siftup(heap, 0)\\n return returnitem\\n return lastelt\\n def heapreplace(heap, item):\\n \"\"\"pop and return the current smallest value, and add the new item.\\n this is more efficient than heappop() followed by heappush(), and can be\\n more appropriate when using a fixed-size heap.  note that the value\\n returned may be larger than item!  that constrains reasonable uses of\\n this routine unless written as part of a conditional replacement:\\n if item > heap[0]:\\n item = heapreplace(heap, item)\\n \"\"\"\\n returnitem = heap[0]    # raises appropriate indexerror if heap is empty\\n heap[0] = item\\n _siftup(heap, 0)\\n return returnitem\\n def heappushpop(heap, item):\\n \"\"\"fast version of a heappush followed by a heappop.\"\"\"\\n if heap and heap[0] < item:\\n item, heap[0] = heap[0], item\\n _siftup(heap, 0)\\n return item\\n def heapify(x):\\n \"\"\"transform list into a heap, in-place, in o(len(x)) time.\"\"\"\\n n = len(x)\\n # transform bottom-up.  the largest index there\\'s any point to looking at\\n # is the largest with a child index in-range, so must have 2*i + 1 < n,\\n # or i < (n-1)/2.  if n is even = 2*j, this is (2*j-1)/2 = j-1/2 so\\n # j-1 is the largest, which is n//2 - 1.  if n is odd = 2*j+1, this is\\n # (2*j+1-1)/2 = j so j-1 is the largest, and that\\'s again n//2-1.\\n for i in reversed(range(n//2)):\\n _siftup(x, i)\\n def _heappop_max(heap):\\n \"\"\"maxheap version of a heappop.\"\"\"\\n lastelt = heap.pop()    # raises appropriate indexerror if heap is empty\\n if heap:\\n returnitem = heap[0]\\n heap[0] = lastelt\\n _siftup_max(heap, 0)\\n return returnitem\\n return lastelt\\n def _heapreplace_max(heap, item):\\n \"\"\"maxheap version of a heappop followed by a heappush.\"\"\"\\n returnitem = heap[0]    # raises appropriate indexerror if heap is empty\\n heap[0] = item\\n _siftup_max(heap, 0)\\n return returnitem\\n def _heapify_max(x):\\n \"\"\"transform list into a maxheap, in-place, in o(len(x)) time.\"\"\"\\n n = len(x)\\n for i in reversed(range(n//2)):\\n _siftup_max(x, i)\\n # \\'heap\\' is a heap at all indices >= startpos, except possibly for pos.  pos\\n # is the index of a leaf with a possibly out-of-order value.  restore the\\n # heap invariant.\\n def _siftdown(heap, startpos, pos):\\n newitem = heap[pos]\\n # follow the path to the root, moving parents down until finding a place\\n # newitem fits.\\n while pos > startpos:\\n parentpos = (pos - 1) >> 1\\n parent = heap[parentpos]\\n if newitem < parent:\\n heap[pos] = parent\\n pos = parentpos\\n continue\\n break\\n heap[pos] = newitem\\n # the child indices of heap index pos are already heaps, and we want to make\\n # a heap at index pos too.  we do this by bubbling the smaller child of\\n # pos up (and so on with that child\\'s children, etc) until hitting a leaf,\\n # then using _siftdown to move the oddball originally at index pos into place.\\n #\\n # we *could* break out of the loop as soon as we find a pos where newitem <=\\n # both its children, but turns out that\\'s not a good idea, and despite that\\n # many books write the algorithm that way.  during a heap pop, the last array\\n # element is sifted in, and that tends to be large, so that comparing it\\n # against values starting from the root usually doesn\\'t pay (= usually doesn\\'t\\n # get us out of the loop early).  see knuth, volume 3, where this is\\n # explained and quantified in an exercise.\\n #\\n # cutting the # of comparisons is important, since these routines have no\\n # way to extract \"the priority\" from an array element, so that intelligence\\n # is likely to be hiding in custom comparison methods, or in array elements\\n # storing (priority, record) tuples.  comparisons are thus potentially\\n # expensive.\\n #\\n # on random arrays of length 1000, making this change cut the number of\\n # comparisons made by heapify() a little, and those made by exhaustive\\n # heappop() a lot, in accord with theory.  here are typical results from 3\\n # runs (3 just to demonstrate how small the variance is):\\n #\\n # compares needed by heapify     compares needed by 1000 heappops\\n # --------------------------     --------------------------------\\n # 1837 cut to 1663               14996 cut to 8680\\n # 1855 cut to 1659               14966 cut to 8678\\n # 1847 cut to 1660               15024 cut to 8703\\n #\\n # building the heap by using heappush() 1000 times instead required\\n # 2198, 2148, and 2219 compares:  heapify() is more efficient, when\\n # you can use it.\\n #\\n # the total compares needed by list.sort() on the same lists were 8627,\\n # 8627, and 8632 (this should be compared to the sum of heapify() and\\n # heappop() compares):  list.sort() is (unsurprisingly!) more efficient\\n # for sorting.\\n def _siftup(heap, pos):\\n endpos = len(heap)\\n startpos = pos\\n newitem = heap[pos]\\n # bubble up the smaller child until hitting a leaf.\\n childpos = 2*pos + 1    # leftmost child position\\n while childpos < endpos:\\n # set childpos to index of smaller child.\\n rightpos = childpos + 1\\n if rightpos < endpos and not heap[childpos] < heap[rightpos]:\\n childpos = rightpos\\n # move the smaller child up.\\n heap[pos] = heap[childpos]\\n pos = childpos\\n childpos = 2*pos + 1\\n # the leaf at pos is empty now.  put newitem there, and bubble it up\\n # to its final resting place (by sifting its parents down).\\n heap[pos] = newitem\\n _siftdown(heap, startpos, pos)\\n def _siftdown_max(heap, startpos, pos):\\n \\'maxheap variant of _siftdown\\'\\n newitem = heap[pos]\\n # follow the path to the root, moving parents down until finding a place\\n # newitem fits.\\n while pos > startpos:\\n parentpos = (pos - 1) >> 1\\n parent = heap[parentpos]\\n if parent < newitem:\\n heap[pos] = parent\\n pos = parentpos\\n continue\\n break\\n heap[pos] = newitem\\n def _siftup_max(heap, pos):\\n \\'maxheap variant of _siftup\\'\\n endpos = len(heap)\\n startpos = pos\\n newitem = heap[pos]\\n # bubble up the larger child until hitting a leaf.\\n childpos = 2*pos + 1    # leftmost child position\\n while childpos < endpos:\\n # set childpos to index of larger child.\\n rightpos = childpos + 1\\n if rightpos < endpos and not heap[rightpos] < heap[childpos]:\\n childpos = rightpos\\n # move the larger child up.\\n heap[pos] = heap[childpos]\\n pos = childpos\\n childpos = 2*pos + 1\\n # the leaf at pos is empty now.  put newitem there, and bubble it up\\n # to its final resting place (by sifting its parents down).\\n heap[pos] = newitem\\n _siftdown_max(heap, startpos, pos)\\n def merge(*iterables, key=none, reverse=false):\\n \\'\\'\\'merge multiple sorted inputs into a single sorted output.\\n similar to sorted(itertools.chain(*iterables)) but returns a generator,\\n does not pull the data into memory all at once, and assumes that each of\\n the input streams is already sorted (smallest to largest).\\n >>> list(merge([1,3,5,7], [0,2,4,8], [5,10,15,20], [], [25]))\\n [0, 1, 2, 3, 4, 5, 5, 7, 8, 10, 15, 20, 25]\\n if *key* is not none, applies a key function to each element to determine\\n its sort order.\\n >>> list(merge([\\'dog\\', \\'horse\\'], [\\'cat\\', \\'fish\\', \\'kangaroo\\'], key=len))\\n [\\'dog\\', \\'cat\\', \\'fish\\', \\'horse\\', \\'kangaroo\\']\\n \\'\\'\\'\\n h = []\\n h_append = h.append\\n if reverse:\\n _heapify = _heapify_max\\n _heappop = _heappop_max\\n _heapreplace = _heapreplace_max\\n direction = -1\\n else:\\n _heapify = heapify\\n _heappop = heappop\\n _heapreplace = heapreplace\\n direction = 1\\n if key is none:\\n for order, it in enumerate(map(iter, iterables)):\\n try:\\n next = it.__next__\\n h_append([next(), order * direction, next])\\n except stopiteration:\\n pass\\n _heapify(h)\\n while len(h) > 1:\\n try:\\n while true:\\n value, order, next = s = h[0]\\n yield value\\n s[0] = next()           # raises stopiteration when exhausted\\n _heapreplace(h, s)      # restore heap condition\\n except stopiteration:\\n _heappop(h)                 # remove empty iterator\\n if h:\\n # fast case when only a single iterator remains\\n value, order, next = h[0]\\n yield value\\n yield from next.__self__\\n return\\n for order, it in enumerate(map(iter, iterables)):\\n try:\\n next = it.__next__\\n value = next()\\n h_append([key(value), order * direction, value, next])\\n except stopiteration:\\n pass\\n _heapify(h)\\n while len(h) > 1:\\n try:\\n while true:\\n key_value, order, value, next = s = h[0]\\n yield value\\n value = next()\\n s[0] = key(value)\\n s[2] = value\\n _heapreplace(h, s)\\n except stopiteration:\\n _heappop(h)\\n if h:\\n key_value, order, value, next = h[0]\\n yield value\\n yield from next.__self__\\n # algorithm notes for nlargest() and nsmallest()\\n # ==============================================\\n #\\n # make a single pass over the data while keeping the k most extreme values\\n # in a heap.  memory consumption is limited to keeping k values in a list.\\n #\\n # measured performance for random inputs:\\n #\\n #                                   number of comparisons\\n #    n inputs     k-extreme values  (average of 5 trials)   % more than min()\\n # -------------   ----------------  ---------------------   -----------------\\n #      1,000           100                  3,317               231.7%\\n #     10,000           100                 14,046                40.5%\\n #    100,000           100                105,749                 5.7%\\n #  1,000,000           100              1,007,751                 0.8%\\n # 10,000,000           100             10,009,401                 0.1%\\n #\\n # theoretical number of comparisons for k smallest of n random inputs:\\n #\\n # step   comparisons                  action\\n # ----   --------------------------   ---------------------------\\n #  1     1.66 * k                     heapify the first k-inputs\\n #  2     n - k                        compare remaining elements to top of heap\\n #  3     k * (1 + lg2(k)) * ln(n/k)   replace the topmost value on the heap\\n #  4     k * lg2(k) - (k/2)           final sort of the k most extreme values\\n #\\n # combining and simplifying for a rough estimate gives:\\n #\\n #        comparisons = n + k * (log(k, 2) * log(n/k) + log(k, 2) + log(n/k))\\n #\\n # computing the number of comparisons for step 3:\\n # -----------------------------------------------\\n # * for the i-th new value from the iterable, the probability of being in the\\n #   k most extreme values is k/i.  for example, the probability of the 101st\\n #   value seen being in the 100 most extreme values is 100/101.\\n # * if the value is a new extreme value, the cost of inserting it into the\\n #   heap is 1 + log(k, 2).\\n # * the probability times the cost gives:\\n #            (k/i) * (1 + log(k, 2))\\n # * summing across the remaining n-k elements gives:\\n #            sum((k/i) * (1 + log(k, 2)) for i in range(k+1, n+1))\\n # * this reduces to:\\n #            (h(n) - h(k)) * k * (1 + log(k, 2))\\n # * where h(n) is the n-th harmonic number estimated by:\\n #            gamma = 0.5772156649\\n #            h(n) = log(n, e) + gamma + 1 / (2 * n)\\n #   http://en.wikipedia.org/wiki/harmonic_series_(mathematics)#rate_of_divergence\\n # * substituting the h(n) formula:\\n #            comparisons = k * (1 + log(k, 2)) * (log(n/k, e) + (1/n - 1/k) / 2)\\n #\\n # worst-case for step 3:\\n # ----------------------\\n # in the worst case, the input data is reversed sorted so that every new element\\n # must be inserted in the heap:\\n #\\n #             comparisons = 1.66 * k + log(k, 2) * (n - k)\\n #\\n # alternative algorithms\\n # ----------------------\\n # other algorithms were not used because they:\\n # 1) took much more auxiliary memory,\\n # 2) made multiple passes over the data.\\n # 3) made more comparisons in common cases (small k, large n, semi-random input).\\n # see the more detailed comparison of approach at:\\n # http://code.activestate.com/recipes/577573-compare-algorithms-for-heapqsmallest\\n def nsmallest(n, iterable, key=none):\\n \"\"\"find the n smallest elements in a dataset.\\n equivalent to:  sorted(iterable, key=key)[:n]\\n \"\"\"\\n # short-cut for n==1 is to use min()\\n if n == 1:\\n it = iter(iterable)\\n sentinel = object()\\n if key is none:\\n result = min(it, default=sentinel)\\n else:\\n result = min(it, default=sentinel, key=key)\\n return [] if result is sentinel else [result]\\n # when n>=size, it\\'s faster to use sorted()\\n try:\\n size = len(iterable)\\n except (typeerror, attributeerror):\\n pass\\n else:\\n if n >= size:\\n return sorted(iterable, key=key)[:n]\\n # when key is none, use simpler decoration\\n if key is none:\\n it = iter(iterable)\\n # put the range(n) first so that zip() doesn\\'t\\n # consume one too many elements from the iterator\\n result = [(elem, i) for i, elem in zip(range(n), it)]\\n if not result:\\n return result\\n _heapify_max(result)\\n top = result[0][0]\\n order = n\\n _heapreplace = _heapreplace_max\\n for elem in it:\\n if elem < top:\\n _heapreplace(result, (elem, order))\\n top = result[0][0]\\n order += 1\\n result.sort()\\n return [r[0] for r in result]\\n # general case, slowest method\\n it = iter(iterable)\\n result = [(key(elem), i, elem) for i, elem in zip(range(n), it)]\\n if not result:\\n return result\\n _heapify_max(result)\\n top = result[0][0]\\n order = n\\n _heapreplace = _heapreplace_max\\n for elem in it:\\n k = key(elem)\\n if k < top:\\n _heapreplace(result, (k, order, elem))\\n top = result[0][0]\\n order += 1\\n result.sort()\\n return [r[2] for r in result]\\n def nlargest(n, iterable, key=none):\\n \"\"\"find the n largest elements in a dataset.\\n equivalent to:  sorted(iterable, key=key, reverse=true)[:n]\\n \"\"\"\\n # short-cut for n==1 is to use max()\\n if n == 1:\\n it = iter(iterable)\\n sentinel = object()\\n if key is none:\\n result = max(it, default=sentinel)\\n else:\\n result = max(it, default=sentinel, key=key)\\n return [] if result is sentinel else [result]\\n # when n>=size, it\\'s faster to use sorted()\\n try:\\n size = len(iterable)\\n except (typeerror, attributeerror):\\n pass\\n else:\\n if n >= size:\\n return sorted(iterable, key=key, reverse=true)[:n]\\n # when key is none, use simpler decoration\\n if key is none:\\n it = iter(iterable)\\n result = [(elem, i) for i, elem in zip(range(0, -n, -1), it)]\\n if not result:\\n return result\\n heapify(result)\\n top = result[0][0]\\n order = -n\\n _heapreplace = heapreplace\\n for elem in it:\\n if top < elem:\\n _heapreplace(result, (elem, order))\\n top = result[0][0]\\n order -= 1\\n result.sort(reverse=true)\\n return [r[0] for r in result]\\n # general case, slowest method\\n it = iter(iterable)\\n result = [(key(elem), i, elem) for i, elem in zip(range(0, -n, -1), it)]\\n if not result:\\n return result\\n heapify(result)\\n top = result[0][0]\\n order = -n\\n _heapreplace = heapreplace\\n for elem in it:\\n k = key(elem)\\n if top < k:\\n _heapreplace(result, (k, order, elem))\\n top = result[0][0]\\n order -= 1\\n result.sort(reverse=true)\\n return [r[2] for r in result]\\n # if available, use c implementation\\n try:\\n from _heapq import *\\n except importerror:\\n pass\\n try:\\n from _heapq import _heapreplace_max\\n except importerror:\\n pass\\n try:\\n from _heapq import _heapify_max\\n except importerror:\\n pass\\n try:\\n from _heapq import _heappop_max\\n except importerror:\\n pass\\n if __name__ == \"__main__\":\\n import doctest\\n print(doctest.testmod())\\n \"\"\"hmac (keyed-hashing for message authentication) python module.\\n implements the hmac algorithm as described by rfc 2104.\\n \"\"\"\\n import warnings as _warnings\\n from _operator import _compare_digest as compare_digest\\n import hashlib as _hashlib\\n trans_5c = bytes((x ^ 0x5c) for x in range(256))\\n trans_36 = bytes((x ^ 0x36) for x in range(256))\\n # the size of the digests returned by hmac depends on the underlying\\n # hashing module used.  use digest_size from the instance of hmac instead.\\n digest_size = none\\n class hmac:\\n \"\"\"rfc 2104 hmac class.  also complies with rfc 4231.\\n this supports the api for cryptographic hash functions (pep 247).\\n \"\"\"\\n blocksize = 64  # 512-bit hmac; can be changed in subclasses.\\n def __init__(self, key, msg = none, digestmod = none):\\n \"\"\"create a new hmac object.\\n key:       key for the keyed hash object.\\n msg:       initial input for the hash, if provided.\\n digestmod: a module supporting pep 247.  *or*\\n a hashlib constructor returning a new hash object. *or*\\n a hash name suitable for hashlib.new().\\n defaults to hashlib.md5.\\n implicit default to hashlib.md5 is deprecated and will be\\n removed in python 3.6.\\n note: key and msg must be a bytes or bytearray objects.\\n \"\"\"\\n if not isinstance(key, (bytes, bytearray)):\\n raise typeerror(\"key: expected bytes or bytearray, but got %r\" % type(key).__name__)\\n if digestmod is none:\\n _warnings.warn(\"hmac() without an explicit digestmod argument \"\\n \"is deprecated.\", pendingdeprecationwarning, 2)\\n digestmod = _hashlib.md5\\n if callable(digestmod):\\n self.digest_cons = digestmod\\n elif isinstance(digestmod, str):\\n self.digest_cons = lambda d=b\\'\\': _hashlib.new(digestmod, d)\\n else:\\n self.digest_cons = lambda d=b\\'\\': digestmod.new(d)\\n self.outer = self.digest_cons()\\n self.inner = self.digest_cons()\\n self.digest_size = self.inner.digest_size\\n if hasattr(self.inner, \\'block_size\\'):\\n blocksize = self.inner.block_size\\n if blocksize < 16:\\n _warnings.warn(\\'block_size of %d seems too small; using our \\'\\n \\'default of %d.\\' % (blocksize, self.blocksize),\\n runtimewarning, 2)\\n blocksize = self.blocksize\\n else:\\n _warnings.warn(\\'no block_size attribute on given digest object; \\'\\n \\'assuming %d.\\' % (self.blocksize),\\n runtimewarning, 2)\\n blocksize = self.blocksize\\n # self.blocksize is the default blocksize. self.block_size is\\n # effective block size as well as the public api attribute.\\n self.block_size = blocksize\\n if len(key) > blocksize:\\n key = self.digest_cons(key).digest()\\n key = key + bytes(blocksize - len(key))\\n self.outer.update(key.translate(trans_5c))\\n self.inner.update(key.translate(trans_36))\\n if msg is not none:\\n self.update(msg)\\n @property\\n def name(self):\\n return \"hmac-\" + self.inner.name\\n def update(self, msg):\\n \"\"\"update this hashing object with the string msg.\\n \"\"\"\\n self.inner.update(msg)\\n def copy(self):\\n \"\"\"return a separate copy of this hashing object.\\n an update to this copy won\\'t affect the original object.\\n \"\"\"\\n # call __new__ directly to avoid the expensive __init__.\\n other = self.__class__.__new__(self.__class__)\\n other.digest_cons = self.digest_cons\\n other.digest_size = self.digest_size\\n other.inner = self.inner.copy()\\n other.outer = self.outer.copy()\\n return other\\n def _current(self):\\n \"\"\"return a hash object for the current state.\\n to be used only internally with digest() and hexdigest().\\n \"\"\"\\n h = self.outer.copy()\\n h.update(self.inner.digest())\\n return h\\n def digest(self):\\n \"\"\"return the hash value of this hashing object.\\n this returns a string containing 8-bit data.  the object is\\n not altered in any way by this function; you can continue\\n updating the object after calling this function.\\n \"\"\"\\n h = self._current()\\n return h.digest()\\n def hexdigest(self):\\n \"\"\"like digest(), but returns a string of hexadecimal digits instead.\\n \"\"\"\\n h = self._current()\\n return h.hexdigest()\\n def new(key, msg = none, digestmod = none):\\n \"\"\"create a new hashing object and return it.\\n key: the starting key for the hash.\\n msg: if available, will immediately be hashed into the object\\'s starting\\n state.\\n you can now feed arbitrary strings into the object using its update()\\n method, and can ask for the hash value at any time by calling its digest()\\n method.\\n \"\"\"\\n return hmac(key, msg, digestmod)\\n \"\"\"this module provides the components needed to build your own __import__\\n function.  undocumented functions are obsolete.\\n in most cases it is preferred you consider using the importlib module\\'s\\n functionality over this module.\\n \"\"\"\\n # (probably) need to stay in _imp\\n from _imp import (lock_held, acquire_lock, release_lock,\\n get_frozen_object, is_frozen_package,\\n init_frozen, is_builtin, is_frozen,\\n _fix_co_filename)\\n try:\\n from _imp import create_dynamic\\n except importerror:\\n # platform doesn\\'t support dynamic loading.\\n create_dynamic = none\\n from importlib._bootstrap import _err_msg, _exec, _load, _builtin_from_name\\n from importlib._bootstrap_external import sourcelessfileloader\\n from importlib import machinery\\n from importlib import util\\n import importlib\\n import os\\n import sys\\n import tokenize\\n import types\\n import warnings\\n warnings.warn(\"the imp module is deprecated in favour of importlib; \"\\n \"see the module\\'s documentation for alternative uses\",\\n pendingdeprecationwarning, stacklevel=2)\\n # deprecated\\n search_error = 0\\n py_source = 1\\n py_compiled = 2\\n c_extension = 3\\n py_resource = 4\\n pkg_directory = 5\\n c_builtin = 6\\n py_frozen = 7\\n py_coderesource = 8\\n imp_hook = 9\\n def new_module(name):\\n \"\"\"**deprecated**\\n create a new module.\\n the module is not entered into sys.modules.\\n \"\"\"\\n return types.moduletype(name)\\n def get_magic():\\n \"\"\"**deprecated**\\n return the magic number for .pyc files.\\n \"\"\"\\n return util.magic_number\\n def get_tag():\\n \"\"\"return the magic tag for .pyc files.\"\"\"\\n return sys.implementation.cache_tag\\n def cache_from_source(path, debug_override=none):\\n \"\"\"**deprecated**\\n given the path to a .py file, return the path to its .pyc file.\\n the .py file does not need to exist; this simply returns the path to the\\n .pyc file calculated as if the .py file were imported.\\n if debug_override is not none, then it must be a boolean and is used in\\n place of sys.flags.optimize.\\n if sys.implementation.cache_tag is none then notimplementederror is raised.\\n \"\"\"\\n with warnings.catch_warnings():\\n warnings.simplefilter(\\'ignore\\')\\n return util.cache_from_source(path, debug_override)\\n def source_from_cache(path):\\n \"\"\"**deprecated**\\n given the path to a .pyc. file, return the path to its .py file.\\n the .pyc file does not need to exist; this simply returns the path to\\n the .py file calculated to correspond to the .pyc file.  if path does\\n not conform to pep 3147 format, valueerror will be raised. if\\n sys.implementation.cache_tag is none then notimplementederror is raised.\\n \"\"\"\\n return util.source_from_cache(path)\\n def get_suffixes():\\n \"\"\"**deprecated**\"\"\"\\n extensions = [(s, \\'rb\\', c_extension) for s in machinery.extension_suffixes]\\n source = [(s, \\'r\\', py_source) for s in machinery.source_suffixes]\\n bytecode = [(s, \\'rb\\', py_compiled) for s in machinery.bytecode_suffixes]\\n return extensions + source + bytecode\\n class nullimporter:\\n \"\"\"**deprecated**\\n null import object.\\n \"\"\"\\n def __init__(self, path):\\n if path == \\'\\':\\n raise importerror(\\'empty pathname\\', path=\\'\\')\\n elif os.path.isdir(path):\\n raise importerror(\\'existing directory\\', path=path)\\n def find_module(self, fullname):\\n \"\"\"always returns none.\"\"\"\\n return none\\n class _hackedgetdata:\\n \"\"\"compatibility support for \\'file\\' arguments of various load_*()\\n functions.\"\"\"\\n def __init__(self, fullname, path, file=none):\\n super().__init__(fullname, path)\\n self.file = file\\n def get_data(self, path):\\n \"\"\"gross hack to contort loader to deal w/ load_*()\\'s bad api.\"\"\"\\n if self.file and path == self.path:\\n if not self.file.closed:\\n file = self.file\\n else:\\n self.file = file = open(self.path, \\'r\\')\\n with file:\\n # technically should be returning bytes, but\\n # sourceloader.get_code() just passed what is returned to\\n # compile() which can handle str. and converting to bytes would\\n # require figuring out the encoding to decode to and\\n # tokenize.detect_encoding() only accepts bytes.\\n return file.read()\\n else:\\n return super().get_data(path)\\n class _loadsourcecompatibility(_hackedgetdata, machinery.sourcefileloader):\\n \"\"\"compatibility support for implementing load_source().\"\"\"\\n def load_source(name, pathname, file=none):\\n loader = _loadsourcecompatibility(name, pathname, file)\\n spec = util.spec_from_file_location(name, pathname, loader=loader)\\n if name in sys.modules:\\n module = _exec(spec, sys.modules[name])\\n else:\\n module = _load(spec)\\n # to allow reloading to potentially work, use a non-hacked loader which\\n # won\\'t rely on a now-closed file object.\\n module.__loader__ = machinery.sourcefileloader(name, pathname)\\n module.__spec__.loader = module.__loader__\\n return module\\n class _loadcompiledcompatibility(_hackedgetdata, sourcelessfileloader):\\n \"\"\"compatibility support for implementing load_compiled().\"\"\"\\n def load_compiled(name, pathname, file=none):\\n \"\"\"**deprecated**\"\"\"\\n loader = _loadcompiledcompatibility(name, pathname, file)\\n spec = util.spec_from_file_location(name, pathname, loader=loader)\\n if name in sys.modules:\\n module = _exec(spec, sys.modules[name])\\n else:\\n module = _load(spec)\\n # to allow reloading to potentially work, use a non-hacked loader which\\n # won\\'t rely on a now-closed file object.\\n module.__loader__ = sourcelessfileloader(name, pathname)\\n module.__spec__.loader = module.__loader__\\n return module\\n def load_package(name, path):\\n \"\"\"**deprecated**\"\"\"\\n if os.path.isdir(path):\\n extensions = (machinery.source_suffixes[:] +\\n machinery.bytecode_suffixes[:])\\n for extension in extensions:\\n path = os.path.join(path, \\'__init__\\'+extension)\\n if os.path.exists(path):\\n break\\n else:\\n raise valueerror(\\'{!r} is not a package\\'.format(path))\\n spec = util.spec_from_file_location(name, path,\\n submodule_search_locations=[])\\n if name in sys.modules:\\n return _exec(spec, sys.modules[name])\\n else:\\n return _load(spec)\\n def load_module(name, file, filename, details):\\n \"\"\"**deprecated**\\n load a module, given information returned by find_module().\\n the module name must include the full package name, if any.\\n \"\"\"\\n suffix, mode, type_ = details\\n if mode and (not mode.startswith((\\'r\\', \\'u\\')) or \\'+\\' in mode):\\n raise valueerror(\\'invalid file open mode {!r}\\'.format(mode))\\n elif file is none and type_ in {py_source, py_compiled}:\\n msg = \\'file object required for import (type code {})\\'.format(type_)\\n raise valueerror(msg)\\n elif type_ == py_source:\\n return load_source(name, filename, file)\\n elif type_ == py_compiled:\\n return load_compiled(name, filename, file)\\n elif type_ == c_extension and load_dynamic is not none:\\n if file is none:\\n with open(filename, \\'rb\\') as opened_file:\\n return load_dynamic(name, filename, opened_file)\\n else:\\n return load_dynamic(name, filename, file)\\n elif type_ == pkg_directory:\\n return load_package(name, filename)\\n elif type_ == c_builtin:\\n return init_builtin(name)\\n elif type_ == py_frozen:\\n return init_frozen(name)\\n else:\\n msg =  \"don\\'t know how to import {} (type code {})\".format(name, type_)\\n raise importerror(msg, name=name)\\n def find_module(name, path=none):\\n \"\"\"**deprecated**\\n search for a module.\\n if path is omitted or none, search for a built-in, frozen or special\\n module and continue search in sys.path. the module name cannot\\n contain \\'.\\'; to search for a submodule of a package, pass the\\n submodule name and the package\\'s __path__.\\n \"\"\"\\n if not isinstance(name, str):\\n raise typeerror(\"\\'name\\' must be a str, not {}\".format(type(name)))\\n elif not isinstance(path, (type(none), list)):\\n # backwards-compatibility\\n raise runtimeerror(\"\\'list\\' must be none or a list, \"\\n \"not {}\".format(type(name)))\\n if path is none:\\n if is_builtin(name):\\n return none, none, (\\'\\', \\'\\', c_builtin)\\n elif is_frozen(name):\\n return none, none, (\\'\\', \\'\\', py_frozen)\\n else:\\n path = sys.path\\n for entry in path:\\n package_directory = os.path.join(entry, name)\\n for suffix in [\\'.py\\', machinery.bytecode_suffixes[0]]:\\n package_file_name = \\'__init__\\' + suffix\\n file_path = os.path.join(package_directory, package_file_name)\\n if os.path.isfile(file_path):\\n return none, package_directory, (\\'\\', \\'\\', pkg_directory)\\n for suffix, mode, type_ in get_suffixes():\\n file_name = name + suffix\\n file_path = os.path.join(entry, file_name)\\n if os.path.isfile(file_path):\\n break\\n else:\\n continue\\n break  # break out of outer loop when breaking out of inner loop.\\n else:\\n raise importerror(_err_msg.format(name), name=name)\\n encoding = none\\n if \\'b\\' not in mode:\\n with open(file_path, \\'rb\\') as file:\\n encoding = tokenize.detect_encoding(file.readline)[0]\\n file = open(file_path, mode, encoding=encoding)\\n return file, file_path, (suffix, mode, type_)\\n def reload(module):\\n \"\"\"**deprecated**\\n reload the module and return it.\\n the module must have been successfully imported before.\\n \"\"\"\\n return importlib.reload(module)\\n def init_builtin(name):\\n \"\"\"**deprecated**\\n load and return a built-in module by name, or none is such module doesn\\'t\\n exist\\n \"\"\"\\n try:\\n return _builtin_from_name(name)\\n except importerror:\\n return none\\n if create_dynamic:\\n def load_dynamic(name, path, file=none):\\n \"\"\"**deprecated**\\n load an extension module.\\n \"\"\"\\n import importlib.machinery\\n loader = importlib.machinery.extensionfileloader(name, path)\\n # issue #24748: skip the sys.modules check in _load_module_shim;\\n # always load new extension\\n spec = importlib.machinery.modulespec(\\n name=name, loader=loader, origin=path)\\n return _load(spec)\\n else:\\n load_dynamic = none\\n \"\"\"the io module provides the python interfaces to stream handling. the\\n builtin open function is defined in this module.\\n at the top of the i/o hierarchy is the abstract base class iobase. it\\n defines the basic interface to a stream. note, however, that there is no\\n separation between reading and writing to streams; implementations are\\n allowed to raise an oserror if they do not support a given operation.\\n extending iobase is rawiobase which deals simply with the reading and\\n writing of raw bytes to a stream. fileio subclasses rawiobase to provide\\n an interface to os files.\\n bufferediobase deals with buffering on a raw byte stream (rawiobase). its\\n subclasses, bufferedwriter, bufferedreader, and bufferedrwpair buffer\\n streams that are readable, writable, and both respectively.\\n bufferedrandom provides a buffered interface to random access\\n streams. bytesio is a simple stream of in-memory bytes.\\n another iobase subclass, textiobase, deals with the encoding and decoding\\n of streams into text. textiowrapper, which extends it, is a buffered text\\n interface to a buffered raw stream (`bufferediobase`). finally, stringio\\n is an in-memory stream for text.\\n argument names are not part of the specification, and only the arguments\\n of open() are intended to be used as keyword arguments.\\n data:\\n default_buffer_size\\n an int containing the default buffer size used by the module\\'s buffered\\n i/o classes. open() uses the file\\'s blksize (as obtained by os.stat) if\\n possible.\\n \"\"\"\\n # new i/o library conforming to pep 3116.\\n __author__ = (\"guido van rossum <guido@python.org>, \"\\n \"mike verdone <mike.verdone@gmail.com>, \"\\n \"mark russell <mark.russell@zen.co.uk>, \"\\n \"antoine pitrou <solipsis@pitrou.net>, \"\\n \"amaury forgeot d\\'arc <amauryfa@gmail.com>, \"\\n \"benjamin peterson <benjamin@python.org>\")\\n __all__ = [\"blockingioerror\", \"open\", \"iobase\", \"rawiobase\", \"fileio\",\\n \"bytesio\", \"stringio\", \"bufferediobase\",\\n \"bufferedreader\", \"bufferedwriter\", \"bufferedrwpair\",\\n \"bufferedrandom\", \"textiobase\", \"textiowrapper\",\\n \"unsupportedoperation\", \"seek_set\", \"seek_cur\", \"seek_end\"]\\n import _io\\n import abc\\n from _io import (default_buffer_size, blockingioerror, unsupportedoperation,\\n open, fileio, bytesio, stringio, bufferedreader,\\n bufferedwriter, bufferedrwpair, bufferedrandom,\\n incrementalnewlinedecoder, textiowrapper)\\n openwrapper = _io.open # for compatibility with _pyio\\n # pretend this exception was created here.\\n unsupportedoperation.__module__ = \"io\"\\n # for seek()\\n seek_set = 0\\n seek_cur = 1\\n seek_end = 2\\n # declaring abcs in c is tricky so we do it here.\\n # method descriptions and default implementations are inherited from the c\\n # version however.\\n class iobase(_io._iobase, metaclass=abc.abcmeta):\\n __doc__ = _io._iobase.__doc__\\n class rawiobase(_io._rawiobase, iobase):\\n __doc__ = _io._rawiobase.__doc__\\n class bufferediobase(_io._bufferediobase, iobase):\\n __doc__ = _io._bufferediobase.__doc__\\n class textiobase(_io._textiobase, iobase):\\n __doc__ = _io._textiobase.__doc__\\n rawiobase.register(fileio)\\n for klass in (bytesio, bufferedreader, bufferedwriter, bufferedrandom,\\n bufferedrwpair):\\n bufferediobase.register(klass)\\n for klass in (stringio, textiowrapper):\\n textiobase.register(klass)\\n del klass\\n #! /usr/bin/python3.5\\n \"\"\"keywords (from \"graminit.c\")\\n this file is automatically generated; please don\\'t muck it up!\\n to update the symbols in this file, \\'cd\\' to the top directory of\\n the python source tree after building the interpreter and run:\\n ./python lib/keyword.py\\n \"\"\"\\n __all__ = [\"iskeyword\", \"kwlist\"]\\n kwlist = [\\n #--start keywords--\\n \\'false\\',\\n \\'none\\',\\n \\'true\\',\\n \\'and\\',\\n \\'as\\',\\n \\'assert\\',\\n \\'break\\',\\n \\'class\\',\\n \\'continue\\',\\n \\'def\\',\\n \\'del\\',\\n \\'elif\\',\\n \\'else\\',\\n \\'except\\',\\n \\'finally\\',\\n \\'for\\',\\n \\'from\\',\\n \\'global\\',\\n \\'if\\',\\n \\'import\\',\\n \\'in\\',\\n \\'is\\',\\n \\'lambda\\',\\n \\'nonlocal\\',\\n \\'not\\',\\n \\'or\\',\\n \\'pass\\',\\n \\'raise\\',\\n \\'return\\',\\n \\'try\\',\\n \\'while\\',\\n \\'with\\',\\n \\'yield\\',\\n #--end keywords--\\n ]\\n iskeyword = frozenset(kwlist).__contains__\\n def main():\\n import sys, re\\n args = sys.argv[1:]\\n iptfile = args and args[0] or \"python/graminit.c\"\\n if len(args) > 1: optfile = args[1]\\n else: optfile = \"lib/keyword.py\"\\n # load the output skeleton from the target, taking care to preserve its\\n # newline convention.\\n with open(optfile, newline=\\'\\') as fp:\\n format = fp.readlines()\\n nl = format[0][len(format[0].strip()):] if format else \\'\\\\n\\'\\n # scan the source file for keywords\\n with open(iptfile) as fp:\\n strprog = re.compile(\\'\"([^\"]+)\"\\')\\n lines = []\\n for line in fp:\\n if \\'{1, \"\\' in line:\\n match = strprog.search(line)\\n if match:\\n lines.append(\"        \\'\" + match.group(1) + \"\\',\" + nl)\\n lines.sort()\\n # insert the lines of keywords into the skeleton\\n try:\\n start = format.index(\"#--start keywords--\" + nl) + 1\\n end = format.index(\"#--end keywords--\" + nl)\\n format[start:end] = lines\\n except valueerror:\\n sys.stderr.write(\"target does not contain format markers\\\\n\")\\n sys.exit(1)\\n # write the output file\\n with open(optfile, \\'w\\', newline=\\'\\') as fp:\\n fp.writelines(format)\\n if __name__ == \"__main__\":\\n main()\\n \"\"\"cache lines from python source files.\\n this is intended to read lines from modules imported -- hence if a filename\\n is not found, it will look down the module search path for a file by\\n that name.\\n \"\"\"\\n import functools\\n import sys\\n import os\\n import tokenize\\n __all__ = [\"getline\", \"clearcache\", \"checkcache\"]\\n def getline(filename, lineno, module_globals=none):\\n lines = getlines(filename, module_globals)\\n if 1 <= lineno <= len(lines):\\n return lines[lineno-1]\\n else:\\n return \\'\\'\\n # the cache\\n # the cache. maps filenames to either a thunk which will provide source code,\\n # or a tuple (size, mtime, lines, fullname) once loaded.\\n cache = {}\\n def clearcache():\\n \"\"\"clear the cache entirely.\"\"\"\\n global cache\\n cache = {}\\n def getlines(filename, module_globals=none):\\n \"\"\"get the lines for a python source file from the cache.\\n update the cache if it doesn\\'t contain an entry for this file already.\"\"\"\\n if filename in cache:\\n entry = cache[filename]\\n if len(entry) != 1:\\n return cache[filename][2]\\n try:\\n return updatecache(filename, module_globals)\\n except memoryerror:\\n clearcache()\\n return []\\n def checkcache(filename=none):\\n \"\"\"discard cache entries that are out of date.\\n (this is not checked upon each call!)\"\"\"\\n if filename is none:\\n filenames = list(cache.keys())\\n else:\\n if filename in cache:\\n filenames = [filename]\\n else:\\n return\\n for filename in filenames:\\n entry = cache[filename]\\n if len(entry) == 1:\\n # lazy cache entry, leave it lazy.\\n continue\\n size, mtime, lines, fullname = entry\\n if mtime is none:\\n continue   # no-op for files loaded via a __loader__\\n try:\\n stat = os.stat(fullname)\\n except oserror:\\n del cache[filename]\\n continue\\n if size != stat.st_size or mtime != stat.st_mtime:\\n del cache[filename]\\n def updatecache(filename, module_globals=none):\\n \"\"\"update a cache entry and return its list of lines.\\n if something\\'s wrong, print a message, discard the cache entry,\\n and return an empty list.\"\"\"\\n if filename in cache:\\n if len(cache[filename]) != 1:\\n del cache[filename]\\n if not filename or (filename.startswith(\\'<\\') and filename.endswith(\\'>\\')):\\n return []\\n fullname = filename\\n try:\\n stat = os.stat(fullname)\\n except oserror:\\n basename = filename\\n # realise a lazy loader based lookup if there is one\\n # otherwise try to lookup right now.\\n if lazycache(filename, module_globals):\\n try:\\n data = cache[filename][0]()\\n except (importerror, oserror):\\n pass\\n else:\\n if data is none:\\n # no luck, the pep302 loader cannot find the source\\n # for this module.\\n return []\\n cache[filename] = (\\n len(data), none,\\n [line+\\'\\\\n\\' for line in data.splitlines()], fullname\\n )\\n return cache[filename][2]\\n # try looking through the module search path, which is only useful\\n # when handling a relative filename.\\n if os.path.isabs(filename):\\n return []\\n for dirname in sys.path:\\n try:\\n fullname = os.path.join(dirname, basename)\\n except (typeerror, attributeerror):\\n # not sufficiently string-like to do anything useful with.\\n continue\\n try:\\n stat = os.stat(fullname)\\n break\\n except oserror:\\n pass\\n else:\\n return []\\n try:\\n with tokenize.open(fullname) as fp:\\n lines = fp.readlines()\\n except oserror:\\n return []\\n if lines and not lines[-1].endswith(\\'\\\\n\\'):\\n lines[-1] += \\'\\\\n\\'\\n size, mtime = stat.st_size, stat.st_mtime\\n cache[filename] = size, mtime, lines, fullname\\n return lines\\n def lazycache(filename, module_globals):\\n \"\"\"seed the cache for filename with module_globals.\\n the module loader will be asked for the source only when getlines is\\n called, not immediately.\\n if there is an entry in the cache already, it is not altered.\\n :return: true if a lazy load is registered in the cache,\\n otherwise false. to register such a load a module loader with a\\n get_source method must be found, the filename must be a cachable\\n filename, and the filename must not be already cached.\\n \"\"\"\\n if filename in cache:\\n if len(cache[filename]) == 1:\\n return true\\n else:\\n return false\\n if not filename or (filename.startswith(\\'<\\') and filename.endswith(\\'>\\')):\\n return false\\n # try for a __loader__, if available\\n if module_globals and \\'__loader__\\' in module_globals:\\n name = module_globals.get(\\'__name__\\')\\n loader = module_globals[\\'__loader__\\']\\n get_source = getattr(loader, \\'get_source\\', none)\\n if name and get_source:\\n get_lines = functools.partial(get_source, name)\\n cache[filename] = (get_lines,)\\n return true\\n return false\\n \"\"\"locale support module.\\n the module provides low-level access to the c lib\\'s locale apis and adds high\\n level number formatting apis as well as a locale aliasing engine to complement\\n these.\\n the aliasing engine includes support for many commonly used locale names and\\n maps them to values suitable for passing to the c lib\\'s setlocale() function. it\\n also includes default encodings for all supported locale names.\\n \"\"\"\\n import sys\\n import encodings\\n import encodings.aliases\\n import re\\n import collections\\n from builtins import str as _builtin_str\\n import functools\\n # try importing the _locale module.\\n #\\n # if this fails, fall back on a basic \\'c\\' locale emulation.\\n # yuck:  lc_messages is non-standard:  can\\'t tell whether it exists before\\n # trying the import.  so __all__ is also fiddled at the end of the file.\\n __all__ = [\"getlocale\", \"getdefaultlocale\", \"getpreferredencoding\", \"error\",\\n \"setlocale\", \"resetlocale\", \"localeconv\", \"strcoll\", \"strxfrm\",\\n \"str\", \"atof\", \"atoi\", \"format\", \"format_string\", \"currency\",\\n \"normalize\", \"lc_ctype\", \"lc_collate\", \"lc_time\", \"lc_monetary\",\\n \"lc_numeric\", \"lc_all\", \"char_max\"]\\n def _strcoll(a,b):\\n \"\"\" strcoll(string,string) -> int.\\n compares two strings according to the locale.\\n \"\"\"\\n return (a > b) - (a < b)\\n def _strxfrm(s):\\n \"\"\" strxfrm(string) -> string.\\n returns a string that behaves for cmp locale-aware.\\n \"\"\"\\n return s\\n try:\\n from _locale import *\\n except importerror:\\n # locale emulation\\n char_max = 127\\n lc_all = 6\\n lc_collate = 3\\n lc_ctype = 0\\n lc_messages = 5\\n lc_monetary = 4\\n lc_numeric = 1\\n lc_time = 2\\n error = valueerror\\n def localeconv():\\n \"\"\" localeconv() -> dict.\\n returns numeric and monetary locale-specific parameters.\\n \"\"\"\\n # \\'c\\' locale default values\\n return {\\'grouping\\': [127],\\n \\'currency_symbol\\': \\'\\',\\n \\'n_sign_posn\\': 127,\\n \\'p_cs_precedes\\': 127,\\n \\'n_cs_precedes\\': 127,\\n \\'mon_grouping\\': [],\\n \\'n_sep_by_space\\': 127,\\n \\'decimal_point\\': \\'.\\',\\n \\'negative_sign\\': \\'\\',\\n \\'positive_sign\\': \\'\\',\\n \\'p_sep_by_space\\': 127,\\n \\'int_curr_symbol\\': \\'\\',\\n \\'p_sign_posn\\': 127,\\n \\'thousands_sep\\': \\'\\',\\n \\'mon_thousands_sep\\': \\'\\',\\n \\'frac_digits\\': 127,\\n \\'mon_decimal_point\\': \\'\\',\\n \\'int_frac_digits\\': 127}\\n def setlocale(category, value=none):\\n \"\"\" setlocale(integer,string=none) -> string.\\n activates/queries locale processing.\\n \"\"\"\\n if value not in (none, \\'\\', \\'c\\'):\\n raise error(\\'_locale emulation only supports \"c\" locale\\')\\n return \\'c\\'\\n # these may or may not exist in _locale, so be sure to set them.\\n if \\'strxfrm\\' not in globals():\\n strxfrm = _strxfrm\\n if \\'strcoll\\' not in globals():\\n strcoll = _strcoll\\n _localeconv = localeconv\\n # with this dict, you can override some items of localeconv\\'s return value.\\n # this is useful for testing purposes.\\n _override_localeconv = {}\\n @functools.wraps(_localeconv)\\n def localeconv():\\n d = _localeconv()\\n if _override_localeconv:\\n d.update(_override_localeconv)\\n return d\\n ### number formatting apis\\n # author: martin von loewis\\n # improved by georg brandl\\n # iterate over grouping intervals\\n def _grouping_intervals(grouping):\\n last_interval = none\\n for interval in grouping:\\n # if grouping is -1, we are done\\n if interval == char_max:\\n return\\n # 0: re-use last group ad infinitum\\n if interval == 0:\\n if last_interval is none:\\n raise valueerror(\"invalid grouping\")\\n while true:\\n yield last_interval\\n yield interval\\n last_interval = interval\\n #perform the grouping from right to left\\n def _group(s, monetary=false):\\n conv = localeconv()\\n thousands_sep = conv[monetary and \\'mon_thousands_sep\\' or \\'thousands_sep\\']\\n grouping = conv[monetary and \\'mon_grouping\\' or \\'grouping\\']\\n if not grouping:\\n return (s, 0)\\n if s[-1] == \\' \\':\\n stripped = s.rstrip()\\n right_spaces = s[len(stripped):]\\n s = stripped\\n else:\\n right_spaces = \\'\\'\\n left_spaces = \\'\\'\\n groups = []\\n for interval in _grouping_intervals(grouping):\\n if not s or s[-1] not in \"0123456789\":\\n # only non-digit characters remain (sign, spaces)\\n left_spaces = s\\n s = \\'\\'\\n break\\n groups.append(s[-interval:])\\n s = s[:-interval]\\n if s:\\n groups.append(s)\\n groups.reverse()\\n return (\\n left_spaces + thousands_sep.join(groups) + right_spaces,\\n len(thousands_sep) * (len(groups) - 1)\\n )\\n # strip a given amount of excess padding from the given string\\n def _strip_padding(s, amount):\\n lpos = 0\\n while amount and s[lpos] == \\' \\':\\n lpos += 1\\n amount -= 1\\n rpos = len(s) - 1\\n while amount and s[rpos] == \\' \\':\\n rpos -= 1\\n amount -= 1\\n return s[lpos:rpos+1]\\n _percent_re = re.compile(r\\'%(?:\\\\((?p<key>.*?)\\\\))?\\'\\n r\\'(?p<modifiers>[-#0-9 +*.hll]*?)[eeffggdiouxxcrs%]\\')\\n def format(percent, value, grouping=false, monetary=false, *additional):\\n \"\"\"returns the locale-aware substitution of a %? specifier\\n (percent).\\n additional is for format strings which contain one or more\\n \\'*\\' modifiers.\"\"\"\\n # this is only for one-percent-specifier strings and this should be checked\\n match = _percent_re.match(percent)\\n if not match or len(match.group())!= len(percent):\\n raise valueerror((\"format() must be given exactly one %%char \"\\n \"format specifier, %s not valid\") % repr(percent))\\n return _format(percent, value, grouping, monetary, *additional)\\n def _format(percent, value, grouping=false, monetary=false, *additional):\\n if additional:\\n formatted = percent % ((value,) + additional)\\n else:\\n formatted = percent % value\\n # floats and decimal ints need special action!\\n if percent[-1] in \\'eeffgg\\':\\n seps = 0\\n parts = formatted.split(\\'.\\')\\n if grouping:\\n parts[0], seps = _group(parts[0], monetary=monetary)\\n decimal_point = localeconv()[monetary and \\'mon_decimal_point\\'\\n or \\'decimal_point\\']\\n formatted = decimal_point.join(parts)\\n if seps:\\n formatted = _strip_padding(formatted, seps)\\n elif percent[-1] in \\'diu\\':\\n seps = 0\\n if grouping:\\n formatted, seps = _group(formatted, monetary=monetary)\\n if seps:\\n formatted = _strip_padding(formatted, seps)\\n return formatted\\n def format_string(f, val, grouping=false):\\n \"\"\"formats a string in the same way that the % formatting would use,\\n but takes the current locale into account.\\n grouping is applied if the third parameter is true.\"\"\"\\n percents = list(_percent_re.finditer(f))\\n new_f = _percent_re.sub(\\'%s\\', f)\\n if isinstance(val, collections.mapping):\\n new_val = []\\n for perc in percents:\\n if perc.group()[-1]==\\'%\\':\\n new_val.append(\\'%\\')\\n else:\\n new_val.append(format(perc.group(), val, grouping))\\n else:\\n if not isinstance(val, tuple):\\n val = (val,)\\n new_val = []\\n i = 0\\n for perc in percents:\\n if perc.group()[-1]==\\'%\\':\\n new_val.append(\\'%\\')\\n else:\\n starcount = perc.group(\\'modifiers\\').count(\\'*\\')\\n new_val.append(_format(perc.group(),\\n val[i],\\n grouping,\\n false,\\n *val[i+1:i+1+starcount]))\\n i += (1 + starcount)\\n val = tuple(new_val)\\n return new_f % val\\n def currency(val, symbol=true, grouping=false, international=false):\\n \"\"\"formats val according to the currency settings\\n in the current locale.\"\"\"\\n conv = localeconv()\\n # check for illegal values\\n digits = conv[international and \\'int_frac_digits\\' or \\'frac_digits\\']\\n if digits == 127:\\n raise valueerror(\"currency formatting is not possible using \"\\n \"the \\'c\\' locale.\")\\n s = format(\\'%%.%if\\' % digits, abs(val), grouping, monetary=true)\\n # \\'<\\' and \\'>\\' are markers if the sign must be inserted between symbol and value\\n s = \\'<\\' + s + \\'>\\'\\n if symbol:\\n smb = conv[international and \\'int_curr_symbol\\' or \\'currency_symbol\\']\\n precedes = conv[val<0 and \\'n_cs_precedes\\' or \\'p_cs_precedes\\']\\n separated = conv[val<0 and \\'n_sep_by_space\\' or \\'p_sep_by_space\\']\\n if precedes:\\n s = smb + (separated and \\' \\' or \\'\\') + s\\n else:\\n s = s + (separated and \\' \\' or \\'\\') + smb\\n sign_pos = conv[val<0 and \\'n_sign_posn\\' or \\'p_sign_posn\\']\\n sign = conv[val<0 and \\'negative_sign\\' or \\'positive_sign\\']\\n if sign_pos == 0:\\n s = \\'(\\' + s + \\')\\'\\n elif sign_pos == 1:\\n s = sign + s\\n elif sign_pos == 2:\\n s = s + sign\\n elif sign_pos == 3:\\n s = s.replace(\\'<\\', sign)\\n elif sign_pos == 4:\\n s = s.replace(\\'>\\', sign)\\n else:\\n # the default if nothing specified;\\n # this should be the most fitting sign position\\n s = sign + s\\n return s.replace(\\'<\\', \\'\\').replace(\\'>\\', \\'\\')\\n def str(val):\\n \"\"\"convert float to string, taking the locale into account.\"\"\"\\n return format(\"%.12g\", val)\\n def delocalize(string):\\n \"parses a string as a normalized number according to the locale settings.\"\\n #first, get rid of the grouping\\n ts = localeconv()[\\'thousands_sep\\']\\n if ts:\\n string = string.replace(ts, \\'\\')\\n #next, replace the decimal point with a dot\\n dd = localeconv()[\\'decimal_point\\']\\n if dd:\\n string = string.replace(dd, \\'.\\')\\n return string\\n def atof(string, func=float):\\n \"parses a string as a float according to the locale settings.\"\\n return func(delocalize(string))\\n def atoi(string):\\n \"converts a string to an integer according to the locale settings.\"\\n return int(delocalize(string))\\n def _test():\\n setlocale(lc_all, \"\")\\n #do grouping\\n s1 = format(\"%d\", 123456789,1)\\n print(s1, \"is\", atoi(s1))\\n #standard formatting\\n s1 = str(3.14)\\n print(s1, \"is\", atof(s1))\\n ### locale name aliasing engine\\n # author: marc-andre lemburg, mal@lemburg.com\\n # various tweaks by fredrik lundh <fredrik@pythonware.com>\\n # store away the low-level version of setlocale (it\\'s\\n # overridden below)\\n _setlocale = setlocale\\n def _replace_encoding(code, encoding):\\n if \\'.\\' in code:\\n langname = code[:code.index(\\'.\\')]\\n else:\\n langname = code\\n # convert the encoding to a c lib compatible encoding string\\n norm_encoding = encodings.normalize_encoding(encoding)\\n #print(\\'norm encoding: %r\\' % norm_encoding)\\n norm_encoding = encodings.aliases.aliases.get(norm_encoding.lower(),\\n norm_encoding)\\n #print(\\'aliased encoding: %r\\' % norm_encoding)\\n encoding = norm_encoding\\n norm_encoding = norm_encoding.lower()\\n if norm_encoding in locale_encoding_alias:\\n encoding = locale_encoding_alias[norm_encoding]\\n else:\\n norm_encoding = norm_encoding.replace(\\'_\\', \\'\\')\\n norm_encoding = norm_encoding.replace(\\'-\\', \\'\\')\\n if norm_encoding in locale_encoding_alias:\\n encoding = locale_encoding_alias[norm_encoding]\\n #print(\\'found encoding %r\\' % encoding)\\n return langname + \\'.\\' + encoding\\n def _append_modifier(code, modifier):\\n if modifier == \\'euro\\':\\n if \\'.\\' not in code:\\n return code + \\'.iso8859-15\\'\\n _, _, encoding = code.partition(\\'.\\')\\n if encoding in (\\'iso8859-15\\', \\'utf-8\\'):\\n return code\\n if encoding == \\'iso8859-1\\':\\n return _replace_encoding(code, \\'iso8859-15\\')\\n return code + \\'@\\' + modifier\\n def normalize(localename):\\n \"\"\" returns a normalized locale code for the given locale\\n name.\\n the returned locale code is formatted for use with\\n setlocale().\\n if normalization fails, the original name is returned\\n unchanged.\\n if the given encoding is not known, the function defaults to\\n the default encoding for the locale code just like setlocale()\\n does.\\n \"\"\"\\n # normalize the locale name and extract the encoding and modifier\\n code = localename.lower()\\n if \\':\\' in code:\\n # \\':\\' is sometimes used as encoding delimiter.\\n code = code.replace(\\':\\', \\'.\\')\\n if \\'@\\' in code:\\n code, modifier = code.split(\\'@\\', 1)\\n else:\\n modifier = \\'\\'\\n if \\'.\\' in code:\\n langname, encoding = code.split(\\'.\\')[:2]\\n else:\\n langname = code\\n encoding = \\'\\'\\n # first lookup: fullname (possibly with encoding and modifier)\\n lang_enc = langname\\n if encoding:\\n norm_encoding = encoding.replace(\\'-\\', \\'\\')\\n norm_encoding = norm_encoding.replace(\\'_\\', \\'\\')\\n lang_enc += \\'.\\' + norm_encoding\\n lookup_name = lang_enc\\n if modifier:\\n lookup_name += \\'@\\' + modifier\\n code = locale_alias.get(lookup_name, none)\\n if code is not none:\\n return code\\n #print(\\'first lookup failed\\')\\n if modifier:\\n # second try: fullname without modifier (possibly with encoding)\\n code = locale_alias.get(lang_enc, none)\\n if code is not none:\\n #print(\\'lookup without modifier succeeded\\')\\n if \\'@\\' not in code:\\n return _append_modifier(code, modifier)\\n if code.split(\\'@\\', 1)[1].lower() == modifier:\\n return code\\n #print(\\'second lookup failed\\')\\n if encoding:\\n # third try: langname (without encoding, possibly with modifier)\\n lookup_name = langname\\n if modifier:\\n lookup_name += \\'@\\' + modifier\\n code = locale_alias.get(lookup_name, none)\\n if code is not none:\\n #print(\\'lookup without encoding succeeded\\')\\n if \\'@\\' not in code:\\n return _replace_encoding(code, encoding)\\n code, modifier = code.split(\\'@\\', 1)\\n return _replace_encoding(code, encoding) + \\'@\\' + modifier\\n if modifier:\\n # fourth try: langname (without encoding and modifier)\\n code = locale_alias.get(langname, none)\\n if code is not none:\\n #print(\\'lookup without modifier and encoding succeeded\\')\\n if \\'@\\' not in code:\\n code = _replace_encoding(code, encoding)\\n return _append_modifier(code, modifier)\\n code, defmod = code.split(\\'@\\', 1)\\n if defmod.lower() == modifier:\\n return _replace_encoding(code, encoding) + \\'@\\' + defmod\\n return localename\\n def _parse_localename(localename):\\n \"\"\" parses the locale code for localename and returns the\\n result as tuple (language code, encoding).\\n the localename is normalized and passed through the locale\\n alias engine. a valueerror is raised in case the locale name\\n cannot be parsed.\\n the language code corresponds to rfc 1766.  code and encoding\\n can be none in case the values cannot be determined or are\\n unknown to this implementation.\\n \"\"\"\\n code = normalize(localename)\\n if \\'@\\' in code:\\n # deal with locale modifiers\\n code, modifier = code.split(\\'@\\', 1)\\n if modifier == \\'euro\\' and \\'.\\' not in code:\\n # assume latin-9 for @euro locales. this is bogus,\\n # since some systems may use other encodings for these\\n # locales. also, we ignore other modifiers.\\n return code, \\'iso-8859-15\\'\\n if \\'.\\' in code:\\n return tuple(code.split(\\'.\\')[:2])\\n elif code == \\'c\\':\\n return none, none\\n raise valueerror(\\'unknown locale: %s\\' % localename)\\n def _build_localename(localetuple):\\n \"\"\" builds a locale code from the given tuple (language code,\\n encoding).\\n no aliasing or normalizing takes place.\\n \"\"\"\\n try:\\n language, encoding = localetuple\\n if language is none:\\n language = \\'c\\'\\n if encoding is none:\\n return language\\n else:\\n return language + \\'.\\' + encoding\\n except (typeerror, valueerror):\\n raise typeerror(\\'locale must be none, a string, or an iterable of two strings -- language code, encoding.\\')\\n def getdefaultlocale(envvars=(\\'lc_all\\', \\'lc_ctype\\', \\'lang\\', \\'language\\')):\\n \"\"\" tries to determine the default locale settings and returns\\n them as tuple (language code, encoding).\\n according to posix, a program which has not called\\n setlocale(lc_all, \"\") runs using the portable \\'c\\' locale.\\n calling setlocale(lc_all, \"\") lets it use the default locale as\\n defined by the lang variable. since we don\\'t want to interfere\\n with the current locale setting we thus emulate the behavior\\n in the way described above.\\n to maintain compatibility with other platforms, not only the\\n lang variable is tested, but a list of variables given as\\n envvars parameter. the first found to be defined will be\\n used. envvars defaults to the search path used in gnu gettext;\\n it must always contain the variable name \\'lang\\'.\\n except for the code \\'c\\', the language code corresponds to rfc\\n 1766.  code and encoding can be none in case the values cannot\\n be determined.\\n \"\"\"\\n try:\\n # check if it\\'s supported by the _locale module\\n import _locale\\n code, encoding = _locale._getdefaultlocale()\\n except (importerror, attributeerror):\\n pass\\n else:\\n # make sure the code/encoding values are valid\\n if sys.platform == \"win32\" and code and code[:2] == \"0x\":\\n # map windows language identifier to language name\\n code = windows_locale.get(int(code, 0))\\n # ...add other platform-specific processing here, if\\n # necessary...\\n return code, encoding\\n # fall back on posix behaviour\\n import os\\n lookup = os.environ.get\\n for variable in envvars:\\n localename = lookup(variable,none)\\n if localename:\\n if variable == \\'language\\':\\n localename = localename.split(\\':\\')[0]\\n break\\n else:\\n localename = \\'c\\'\\n return _parse_localename(localename)\\n def getlocale(category=lc_ctype):\\n \"\"\" returns the current setting for the given locale category as\\n tuple (language code, encoding).\\n category may be one of the lc_* value except lc_all. it\\n defaults to lc_ctype.\\n except for the code \\'c\\', the language code corresponds to rfc\\n 1766.  code and encoding can be none in case the values cannot\\n be determined.\\n \"\"\"\\n localename = _setlocale(category)\\n if category == lc_all and \\';\\' in localename:\\n raise typeerror(\\'category lc_all is not supported\\')\\n return _parse_localename(localename)\\n def setlocale(category, locale=none):\\n \"\"\" set the locale for the given category.  the locale can be\\n a string, an iterable of two strings (language code and encoding),\\n or none.\\n iterables are converted to strings using the locale aliasing\\n engine.  locale strings are passed directly to the c lib.\\n category may be given as one of the lc_* values.\\n \"\"\"\\n if locale and not isinstance(locale, _builtin_str):\\n # convert to string\\n locale = normalize(_build_localename(locale))\\n return _setlocale(category, locale)\\n def resetlocale(category=lc_all):\\n \"\"\" sets the locale for category to the default setting.\\n the default setting is determined by calling\\n getdefaultlocale(). category defaults to lc_all.\\n \"\"\"\\n _setlocale(category, _build_localename(getdefaultlocale()))\\n if sys.platform.startswith(\"win\"):\\n # on win32, this will return the ansi code page\\n def getpreferredencoding(do_setlocale = true):\\n \"\"\"return the charset that the user is likely using.\"\"\"\\n import _bootlocale\\n return _bootlocale.getpreferredencoding(false)\\n else:\\n # on unix, if codeset is available, use that.\\n try:\\n codeset\\n except nameerror:\\n # fall back to parsing environment variables :-(\\n def getpreferredencoding(do_setlocale = true):\\n \"\"\"return the charset that the user is likely using,\\n by looking at environment variables.\"\"\"\\n res = getdefaultlocale()[1]\\n if res is none:\\n # lang not set, default conservatively to ascii\\n res = \\'ascii\\'\\n return res\\n else:\\n def getpreferredencoding(do_setlocale = true):\\n \"\"\"return the charset that the user is likely using,\\n according to the system configuration.\"\"\"\\n import _bootlocale\\n if do_setlocale:\\n oldloc = setlocale(lc_ctype)\\n try:\\n setlocale(lc_ctype, \"\")\\n except error:\\n pass\\n result = _bootlocale.getpreferredencoding(false)\\n if do_setlocale:\\n setlocale(lc_ctype, oldloc)\\n return result\\n ### database\\n #\\n # the following data was extracted from the locale.alias file which\\n # comes with x11 and then hand edited removing the explicit encoding\\n # definitions and adding some more aliases. the file is usually\\n # available as /usr/lib/x11/locale/locale.alias.\\n #\\n #\\n # the local_encoding_alias table maps lowercase encoding alias names\\n # to c locale encoding names (case-sensitive). note that normalize()\\n # first looks up the encoding in the encodings.aliases dictionary and\\n # then applies this mapping to find the correct c lib name for the\\n # encoding.\\n #\\n locale_encoding_alias = {\\n # mappings for non-standard encoding names used in locale names\\n \\'437\\':                          \\'c\\',\\n \\'c\\':                            \\'c\\',\\n \\'en\\':                           \\'iso8859-1\\',\\n \\'jis\\':                          \\'jis7\\',\\n \\'jis7\\':                         \\'jis7\\',\\n \\'ajec\\':                         \\'eucjp\\',\\n \\'koi8c\\':                        \\'koi8-c\\',\\n \\'microsoftcp1251\\':              \\'cp1251\\',\\n \\'microsoftcp1255\\':              \\'cp1255\\',\\n \\'microsoftcp1256\\':              \\'cp1256\\',\\n \\'88591\\':                        \\'iso8859-1\\',\\n \\'88592\\':                        \\'iso8859-2\\',\\n \\'88595\\':                        \\'iso8859-5\\',\\n \\'885915\\':                       \\'iso8859-15\\',\\n # mappings from python codec names to c lib encoding names\\n \\'ascii\\':                        \\'iso8859-1\\',\\n \\'latin_1\\':                      \\'iso8859-1\\',\\n \\'iso8859_1\\':                    \\'iso8859-1\\',\\n \\'iso8859_10\\':                   \\'iso8859-10\\',\\n \\'iso8859_11\\':                   \\'iso8859-11\\',\\n \\'iso8859_13\\':                   \\'iso8859-13\\',\\n \\'iso8859_14\\':                   \\'iso8859-14\\',\\n \\'iso8859_15\\':                   \\'iso8859-15\\',\\n \\'iso8859_16\\':                   \\'iso8859-16\\',\\n \\'iso8859_2\\':                    \\'iso8859-2\\',\\n \\'iso8859_3\\':                    \\'iso8859-3\\',\\n \\'iso8859_4\\':                    \\'iso8859-4\\',\\n \\'iso8859_5\\':                    \\'iso8859-5\\',\\n \\'iso8859_6\\':                    \\'iso8859-6\\',\\n \\'iso8859_7\\':                    \\'iso8859-7\\',\\n \\'iso8859_8\\':                    \\'iso8859-8\\',\\n \\'iso8859_9\\':                    \\'iso8859-9\\',\\n \\'iso2022_jp\\':                   \\'jis7\\',\\n \\'shift_jis\\':                    \\'sjis\\',\\n \\'tactis\\':                       \\'tactis\\',\\n \\'euc_jp\\':                       \\'eucjp\\',\\n \\'euc_kr\\':                       \\'euckr\\',\\n \\'utf_8\\':                        \\'utf-8\\',\\n \\'koi8_r\\':                       \\'koi8-r\\',\\n \\'koi8_t\\':                       \\'koi8-t\\',\\n \\'koi8_u\\':                       \\'koi8-u\\',\\n \\'kz1048\\':                       \\'rk1048\\',\\n \\'cp1251\\':                       \\'cp1251\\',\\n \\'cp1255\\':                       \\'cp1255\\',\\n \\'cp1256\\':                       \\'cp1256\\',\\n # xxx this list is still incomplete. if you know more\\n # mappings, please file a bug report. thanks.\\n }\\n for k, v in sorted(locale_encoding_alias.items()):\\n k = k.replace(\\'_\\', \\'\\')\\n locale_encoding_alias.setdefault(k, v)\\n #\\n # the locale_alias table maps lowercase alias names to c locale names\\n # (case-sensitive). encodings are always separated from the locale\\n # name using a dot (\\'.\\'); they should only be given in case the\\n # language name is needed to interpret the given encoding alias\\n # correctly (cjk codes often have this need).\\n #\\n # note that the normalize() function which uses this tables\\n # removes \\'_\\' and \\'-\\' characters from the encoding part of the\\n # locale name before doing the lookup. this saves a lot of\\n # space in the table.\\n #\\n # mal 2004-12-10:\\n # updated alias mapping to most recent locale.alias file\\n # from x.org distribution using makelocalealias.py.\\n #\\n # these are the differences compared to the old mapping (python 2.4\\n # and older):\\n #\\n #    updated \\'bg\\' -> \\'bg_bg.iso8859-5\\' to \\'bg_bg.cp1251\\'\\n #    updated \\'bg_bg\\' -> \\'bg_bg.iso8859-5\\' to \\'bg_bg.cp1251\\'\\n #    updated \\'bulgarian\\' -> \\'bg_bg.iso8859-5\\' to \\'bg_bg.cp1251\\'\\n #    updated \\'cz\\' -> \\'cz_cz.iso8859-2\\' to \\'cs_cz.iso8859-2\\'\\n #    updated \\'cz_cz\\' -> \\'cz_cz.iso8859-2\\' to \\'cs_cz.iso8859-2\\'\\n #    updated \\'czech\\' -> \\'cs_cs.iso8859-2\\' to \\'cs_cz.iso8859-2\\'\\n #    updated \\'dutch\\' -> \\'nl_be.iso8859-1\\' to \\'nl_nl.iso8859-1\\'\\n #    updated \\'et\\' -> \\'et_ee.iso8859-4\\' to \\'et_ee.iso8859-15\\'\\n #    updated \\'et_ee\\' -> \\'et_ee.iso8859-4\\' to \\'et_ee.iso8859-15\\'\\n #    updated \\'fi\\' -> \\'fi_fi.iso8859-1\\' to \\'fi_fi.iso8859-15\\'\\n #    updated \\'fi_fi\\' -> \\'fi_fi.iso8859-1\\' to \\'fi_fi.iso8859-15\\'\\n #    updated \\'iw\\' -> \\'iw_il.iso8859-8\\' to \\'he_il.iso8859-8\\'\\n #    updated \\'iw_il\\' -> \\'iw_il.iso8859-8\\' to \\'he_il.iso8859-8\\'\\n #    updated \\'japanese\\' -> \\'ja_jp.sjis\\' to \\'ja_jp.eucjp\\'\\n #    updated \\'lt\\' -> \\'lt_lt.iso8859-4\\' to \\'lt_lt.iso8859-13\\'\\n #    updated \\'lv\\' -> \\'lv_lv.iso8859-4\\' to \\'lv_lv.iso8859-13\\'\\n #    updated \\'sl\\' -> \\'sl_cs.iso8859-2\\' to \\'sl_si.iso8859-2\\'\\n #    updated \\'slovene\\' -> \\'sl_cs.iso8859-2\\' to \\'sl_si.iso8859-2\\'\\n #    updated \\'th_th\\' -> \\'th_th.tactis\\' to \\'th_th.iso8859-11\\'\\n #    updated \\'zh_cn\\' -> \\'zh_cn.euccn\\' to \\'zh_cn.gb2312\\'\\n #    updated \\'zh_cn.big5\\' -> \\'zh_tw.euctw\\' to \\'zh_tw.big5\\'\\n #    updated \\'zh_tw\\' -> \\'zh_tw.euctw\\' to \\'zh_tw.big5\\'\\n #\\n # mal 2008-05-30:\\n # updated alias mapping to most recent locale.alias file\\n # from x.org distribution using makelocalealias.py.\\n #\\n # these are the differences compared to the old mapping (python 2.5\\n # and older):\\n #\\n #    updated \\'cs_cs.iso88592\\' -> \\'cs_cz.iso8859-2\\' to \\'cs_cs.iso8859-2\\'\\n #    updated \\'serbocroatian\\' -> \\'sh_yu.iso8859-2\\' to \\'sr_cs.iso8859-2\\'\\n #    updated \\'sh\\' -> \\'sh_yu.iso8859-2\\' to \\'sr_cs.iso8859-2\\'\\n #    updated \\'sh_hr.iso88592\\' -> \\'sh_hr.iso8859-2\\' to \\'hr_hr.iso8859-2\\'\\n #    updated \\'sh_sp\\' -> \\'sh_yu.iso8859-2\\' to \\'sr_cs.iso8859-2\\'\\n #    updated \\'sh_yu\\' -> \\'sh_yu.iso8859-2\\' to \\'sr_cs.iso8859-2\\'\\n #    updated \\'sp\\' -> \\'sp_yu.iso8859-5\\' to \\'sr_cs.iso8859-5\\'\\n #    updated \\'sp_yu\\' -> \\'sp_yu.iso8859-5\\' to \\'sr_cs.iso8859-5\\'\\n #    updated \\'sr\\' -> \\'sr_yu.iso8859-5\\' to \\'sr_cs.iso8859-5\\'\\n #    updated \\'sr@cyrillic\\' -> \\'sr_yu.iso8859-5\\' to \\'sr_cs.iso8859-5\\'\\n #    updated \\'sr_sp\\' -> \\'sr_sp.iso8859-2\\' to \\'sr_cs.iso8859-2\\'\\n #    updated \\'sr_yu\\' -> \\'sr_yu.iso8859-5\\' to \\'sr_cs.iso8859-5\\'\\n #    updated \\'sr_yu.cp1251@cyrillic\\' -> \\'sr_yu.cp1251\\' to \\'sr_cs.cp1251\\'\\n #    updated \\'sr_yu.iso88592\\' -> \\'sr_yu.iso8859-2\\' to \\'sr_cs.iso8859-2\\'\\n #    updated \\'sr_yu.iso88595\\' -> \\'sr_yu.iso8859-5\\' to \\'sr_cs.iso8859-5\\'\\n #    updated \\'sr_yu.iso88595@cyrillic\\' -> \\'sr_yu.iso8859-5\\' to \\'sr_cs.iso8859-5\\'\\n #    updated \\'sr_yu.microsoftcp1251@cyrillic\\' -> \\'sr_yu.cp1251\\' to \\'sr_cs.cp1251\\'\\n #    updated \\'sr_yu.utf8@cyrillic\\' -> \\'sr_yu.utf-8\\' to \\'sr_cs.utf-8\\'\\n #    updated \\'sr_yu@cyrillic\\' -> \\'sr_yu.iso8859-5\\' to \\'sr_cs.iso8859-5\\'\\n #\\n # ap 2010-04-12:\\n # updated alias mapping to most recent locale.alias file\\n # from x.org distribution using makelocalealias.py.\\n #\\n # these are the differences compared to the old mapping (python 2.6.5\\n # and older):\\n #\\n #    updated \\'ru\\' -> \\'ru_ru.iso8859-5\\' to \\'ru_ru.utf-8\\'\\n #    updated \\'ru_ru\\' -> \\'ru_ru.iso8859-5\\' to \\'ru_ru.utf-8\\'\\n #    updated \\'serbocroatian\\' -> \\'sr_cs.iso8859-2\\' to \\'sr_rs.utf-8@latin\\'\\n #    updated \\'sh\\' -> \\'sr_cs.iso8859-2\\' to \\'sr_rs.utf-8@latin\\'\\n #    updated \\'sh_yu\\' -> \\'sr_cs.iso8859-2\\' to \\'sr_rs.utf-8@latin\\'\\n #    updated \\'sr\\' -> \\'sr_cs.iso8859-5\\' to \\'sr_rs.utf-8\\'\\n #    updated \\'sr@cyrillic\\' -> \\'sr_cs.iso8859-5\\' to \\'sr_rs.utf-8\\'\\n #    updated \\'sr@latn\\' -> \\'sr_cs.iso8859-2\\' to \\'sr_rs.utf-8@latin\\'\\n #    updated \\'sr_cs.utf8@latn\\' -> \\'sr_cs.utf-8\\' to \\'sr_rs.utf-8@latin\\'\\n #    updated \\'sr_cs@latn\\' -> \\'sr_cs.iso8859-2\\' to \\'sr_rs.utf-8@latin\\'\\n #    updated \\'sr_yu\\' -> \\'sr_cs.iso8859-5\\' to \\'sr_rs.utf-8@latin\\'\\n #    updated \\'sr_yu.utf8@cyrillic\\' -> \\'sr_cs.utf-8\\' to \\'sr_rs.utf-8\\'\\n #    updated \\'sr_yu@cyrillic\\' -> \\'sr_cs.iso8859-5\\' to \\'sr_rs.utf-8\\'\\n #\\n # ss 2013-12-20:\\n # updated alias mapping to most recent locale.alias file\\n # from x.org distribution using makelocalealias.py.\\n #\\n # these are the differences compared to the old mapping (python 3.3.3\\n # and older):\\n #\\n #    updated \\'a3\\' -> \\'a3_az.koi8-c\\' to \\'az_az.koi8-c\\'\\n #    updated \\'a3_az\\' -> \\'a3_az.koi8-c\\' to \\'az_az.koi8-c\\'\\n #    updated \\'a3_az.koi8c\\' -> \\'a3_az.koi8-c\\' to \\'az_az.koi8-c\\'\\n #    updated \\'cs_cs.iso88592\\' -> \\'cs_cs.iso8859-2\\' to \\'cs_cz.iso8859-2\\'\\n #    updated \\'hebrew\\' -> \\'iw_il.iso8859-8\\' to \\'he_il.iso8859-8\\'\\n #    updated \\'hebrew.iso88598\\' -> \\'iw_il.iso8859-8\\' to \\'he_il.iso8859-8\\'\\n #    updated \\'sd\\' -> \\'sd_in@devanagari.utf-8\\' to \\'sd_in.utf-8\\'\\n #    updated \\'sr@latn\\' -> \\'sr_rs.utf-8@latin\\' to \\'sr_cs.utf-8@latin\\'\\n #    updated \\'sr_cs\\' -> \\'sr_rs.utf-8\\' to \\'sr_cs.utf-8\\'\\n #    updated \\'sr_cs.utf8@latn\\' -> \\'sr_rs.utf-8@latin\\' to \\'sr_cs.utf-8@latin\\'\\n #    updated \\'sr_cs@latn\\' -> \\'sr_rs.utf-8@latin\\' to \\'sr_cs.utf-8@latin\\'\\n #\\n # ss 2014-10-01:\\n # updated alias mapping with glibc 2.19 supported locales.\\n locale_alias = {\\n \\'a3\\':                                   \\'az_az.koi8-c\\',\\n \\'a3_az\\':                                \\'az_az.koi8-c\\',\\n \\'a3_az.koic\\':                           \\'az_az.koi8-c\\',\\n \\'aa_dj\\':                                \\'aa_dj.iso8859-1\\',\\n \\'aa_er\\':                                \\'aa_er.utf-8\\',\\n \\'aa_et\\':                                \\'aa_et.utf-8\\',\\n \\'af\\':                                   \\'af_za.iso8859-1\\',\\n \\'af_za\\':                                \\'af_za.iso8859-1\\',\\n \\'am\\':                                   \\'am_et.utf-8\\',\\n \\'am_et\\':                                \\'am_et.utf-8\\',\\n \\'american\\':                             \\'en_us.iso8859-1\\',\\n \\'an_es\\':                                \\'an_es.iso8859-15\\',\\n \\'ar\\':                                   \\'ar_aa.iso8859-6\\',\\n \\'ar_aa\\':                                \\'ar_aa.iso8859-6\\',\\n \\'ar_ae\\':                                \\'ar_ae.iso8859-6\\',\\n \\'ar_bh\\':                                \\'ar_bh.iso8859-6\\',\\n \\'ar_dz\\':                                \\'ar_dz.iso8859-6\\',\\n \\'ar_eg\\':                                \\'ar_eg.iso8859-6\\',\\n \\'ar_in\\':                                \\'ar_in.utf-8\\',\\n \\'ar_iq\\':                                \\'ar_iq.iso8859-6\\',\\n \\'ar_jo\\':                                \\'ar_jo.iso8859-6\\',\\n \\'ar_kw\\':                                \\'ar_kw.iso8859-6\\',\\n \\'ar_lb\\':                                \\'ar_lb.iso8859-6\\',\\n \\'ar_ly\\':                                \\'ar_ly.iso8859-6\\',\\n \\'ar_ma\\':                                \\'ar_ma.iso8859-6\\',\\n \\'ar_om\\':                                \\'ar_om.iso8859-6\\',\\n \\'ar_qa\\':                                \\'ar_qa.iso8859-6\\',\\n \\'ar_sa\\':                                \\'ar_sa.iso8859-6\\',\\n \\'ar_sd\\':                                \\'ar_sd.iso8859-6\\',\\n \\'ar_sy\\':                                \\'ar_sy.iso8859-6\\',\\n \\'ar_tn\\':                                \\'ar_tn.iso8859-6\\',\\n \\'ar_ye\\':                                \\'ar_ye.iso8859-6\\',\\n \\'arabic\\':                               \\'ar_aa.iso8859-6\\',\\n \\'as\\':                                   \\'as_in.utf-8\\',\\n \\'as_in\\':                                \\'as_in.utf-8\\',\\n \\'ast_es\\':                               \\'ast_es.iso8859-15\\',\\n \\'ayc_pe\\':                               \\'ayc_pe.utf-8\\',\\n \\'az\\':                                   \\'az_az.iso8859-9e\\',\\n \\'az_az\\':                                \\'az_az.iso8859-9e\\',\\n \\'az_az.iso88599e\\':                      \\'az_az.iso8859-9e\\',\\n \\'be\\':                                   \\'be_by.cp1251\\',\\n \\'be@latin\\':                             \\'be_by.utf-8@latin\\',\\n \\'be_bg.utf8\\':                           \\'bg_bg.utf-8\\',\\n \\'be_by\\':                                \\'be_by.cp1251\\',\\n \\'be_by@latin\\':                          \\'be_by.utf-8@latin\\',\\n \\'bem_zm\\':                               \\'bem_zm.utf-8\\',\\n \\'ber_dz\\':                               \\'ber_dz.utf-8\\',\\n \\'ber_ma\\':                               \\'ber_ma.utf-8\\',\\n \\'bg\\':                                   \\'bg_bg.cp1251\\',\\n \\'bg_bg\\':                                \\'bg_bg.cp1251\\',\\n \\'bho_in\\':                               \\'bho_in.utf-8\\',\\n \\'bn_bd\\':                                \\'bn_bd.utf-8\\',\\n \\'bn_in\\':                                \\'bn_in.utf-8\\',\\n \\'bo_cn\\':                                \\'bo_cn.utf-8\\',\\n \\'bo_in\\':                                \\'bo_in.utf-8\\',\\n \\'bokmal\\':                               \\'nb_no.iso8859-1\\',\\n \\'bokm\\\\xe5l\\':                            \\'nb_no.iso8859-1\\',\\n \\'br\\':                                   \\'br_fr.iso8859-1\\',\\n \\'br_fr\\':                                \\'br_fr.iso8859-1\\',\\n \\'brx_in\\':                               \\'brx_in.utf-8\\',\\n \\'bs\\':                                   \\'bs_ba.iso8859-2\\',\\n \\'bs_ba\\':                                \\'bs_ba.iso8859-2\\',\\n \\'bulgarian\\':                            \\'bg_bg.cp1251\\',\\n \\'byn_er\\':                               \\'byn_er.utf-8\\',\\n \\'c\\':                                    \\'c\\',\\n \\'c-french\\':                             \\'fr_ca.iso8859-1\\',\\n \\'c.ascii\\':                              \\'c\\',\\n \\'c.en\\':                                 \\'c\\',\\n \\'c.iso88591\\':                           \\'en_us.iso8859-1\\',\\n \\'c.utf8\\':                               \\'en_us.utf-8\\',\\n \\'c_c\\':                                  \\'c\\',\\n \\'c_c.c\\':                                \\'c\\',\\n \\'ca\\':                                   \\'ca_es.iso8859-1\\',\\n \\'ca_ad\\':                                \\'ca_ad.iso8859-1\\',\\n \\'ca_es\\':                                \\'ca_es.iso8859-1\\',\\n \\'ca_es@valencia\\':                       \\'ca_es.iso8859-15@valencia\\',\\n \\'ca_fr\\':                                \\'ca_fr.iso8859-1\\',\\n \\'ca_it\\':                                \\'ca_it.iso8859-1\\',\\n \\'catalan\\':                              \\'ca_es.iso8859-1\\',\\n \\'cextend\\':                              \\'en_us.iso8859-1\\',\\n \\'chinese-s\\':                            \\'zh_cn.euccn\\',\\n \\'chinese-t\\':                            \\'zh_tw.euctw\\',\\n \\'crh_ua\\':                               \\'crh_ua.utf-8\\',\\n \\'croatian\\':                             \\'hr_hr.iso8859-2\\',\\n \\'cs\\':                                   \\'cs_cz.iso8859-2\\',\\n \\'cs_cs\\':                                \\'cs_cz.iso8859-2\\',\\n \\'cs_cz\\':                                \\'cs_cz.iso8859-2\\',\\n \\'csb_pl\\':                               \\'csb_pl.utf-8\\',\\n \\'cv_ru\\':                                \\'cv_ru.utf-8\\',\\n \\'cy\\':                                   \\'cy_gb.iso8859-1\\',\\n \\'cy_gb\\':                                \\'cy_gb.iso8859-1\\',\\n \\'cz\\':                                   \\'cs_cz.iso8859-2\\',\\n \\'cz_cz\\':                                \\'cs_cz.iso8859-2\\',\\n \\'czech\\':                                \\'cs_cz.iso8859-2\\',\\n \\'da\\':                                   \\'da_dk.iso8859-1\\',\\n \\'da_dk\\':                                \\'da_dk.iso8859-1\\',\\n \\'danish\\':                               \\'da_dk.iso8859-1\\',\\n \\'dansk\\':                                \\'da_dk.iso8859-1\\',\\n \\'de\\':                                   \\'de_de.iso8859-1\\',\\n \\'de_at\\':                                \\'de_at.iso8859-1\\',\\n \\'de_be\\':                                \\'de_be.iso8859-1\\',\\n \\'de_ch\\':                                \\'de_ch.iso8859-1\\',\\n \\'de_de\\':                                \\'de_de.iso8859-1\\',\\n \\'de_li.utf8\\':                           \\'de_li.utf-8\\',\\n \\'de_lu\\':                                \\'de_lu.iso8859-1\\',\\n \\'deutsch\\':                              \\'de_de.iso8859-1\\',\\n \\'doi_in\\':                               \\'doi_in.utf-8\\',\\n \\'dutch\\':                                \\'nl_nl.iso8859-1\\',\\n \\'dutch.iso88591\\':                       \\'nl_be.iso8859-1\\',\\n \\'dv_mv\\':                                \\'dv_mv.utf-8\\',\\n \\'dz_bt\\':                                \\'dz_bt.utf-8\\',\\n \\'ee\\':                                   \\'ee_ee.iso8859-4\\',\\n \\'ee_ee\\':                                \\'ee_ee.iso8859-4\\',\\n \\'eesti\\':                                \\'et_ee.iso8859-1\\',\\n \\'el\\':                                   \\'el_gr.iso8859-7\\',\\n \\'el_cy\\':                                \\'el_cy.iso8859-7\\',\\n \\'el_gr\\':                                \\'el_gr.iso8859-7\\',\\n \\'el_gr@euro\\':                           \\'el_gr.iso8859-15\\',\\n \\'en\\':                                   \\'en_us.iso8859-1\\',\\n \\'en_ag\\':                                \\'en_ag.utf-8\\',\\n \\'en_au\\':                                \\'en_au.iso8859-1\\',\\n \\'en_be\\':                                \\'en_be.iso8859-1\\',\\n \\'en_bw\\':                                \\'en_bw.iso8859-1\\',\\n \\'en_ca\\':                                \\'en_ca.iso8859-1\\',\\n \\'en_dk\\':                                \\'en_dk.iso8859-1\\',\\n \\'en_dl.utf8\\':                           \\'en_dl.utf-8\\',\\n \\'en_gb\\':                                \\'en_gb.iso8859-1\\',\\n \\'en_hk\\':                                \\'en_hk.iso8859-1\\',\\n \\'en_ie\\':                                \\'en_ie.iso8859-1\\',\\n \\'en_in\\':                                \\'en_in.iso8859-1\\',\\n \\'en_ng\\':                                \\'en_ng.utf-8\\',\\n \\'en_nz\\':                                \\'en_nz.iso8859-1\\',\\n \\'en_ph\\':                                \\'en_ph.iso8859-1\\',\\n \\'en_sg\\':                                \\'en_sg.iso8859-1\\',\\n \\'en_uk\\':                                \\'en_gb.iso8859-1\\',\\n \\'en_us\\':                                \\'en_us.iso8859-1\\',\\n \\'en_us@euro@euro\\':                      \\'en_us.iso8859-15\\',\\n \\'en_za\\':                                \\'en_za.iso8859-1\\',\\n \\'en_zm\\':                                \\'en_zm.utf-8\\',\\n \\'en_zw\\':                                \\'en_zw.iso8859-1\\',\\n \\'en_zw.utf8\\':                           \\'en_zs.utf-8\\',\\n \\'eng_gb\\':                               \\'en_gb.iso8859-1\\',\\n \\'english\\':                              \\'en_en.iso8859-1\\',\\n \\'english_uk\\':                           \\'en_gb.iso8859-1\\',\\n \\'english_united-states\\':                \\'en_us.iso8859-1\\',\\n \\'english_united-states.437\\':            \\'c\\',\\n \\'english_us\\':                           \\'en_us.iso8859-1\\',\\n \\'eo\\':                                   \\'eo_xx.iso8859-3\\',\\n \\'eo.utf8\\':                              \\'eo.utf-8\\',\\n \\'eo_eo\\':                                \\'eo_eo.iso8859-3\\',\\n \\'eo_us.utf8\\':                           \\'eo_us.utf-8\\',\\n \\'eo_xx\\':                                \\'eo_xx.iso8859-3\\',\\n \\'es\\':                                   \\'es_es.iso8859-1\\',\\n \\'es_ar\\':                                \\'es_ar.iso8859-1\\',\\n \\'es_bo\\':                                \\'es_bo.iso8859-1\\',\\n \\'es_cl\\':                                \\'es_cl.iso8859-1\\',\\n \\'es_co\\':                                \\'es_co.iso8859-1\\',\\n \\'es_cr\\':                                \\'es_cr.iso8859-1\\',\\n \\'es_cu\\':                                \\'es_cu.utf-8\\',\\n \\'es_do\\':                                \\'es_do.iso8859-1\\',\\n \\'es_ec\\':                                \\'es_ec.iso8859-1\\',\\n \\'es_es\\':                                \\'es_es.iso8859-1\\',\\n \\'es_gt\\':                                \\'es_gt.iso8859-1\\',\\n \\'es_hn\\':                                \\'es_hn.iso8859-1\\',\\n \\'es_mx\\':                                \\'es_mx.iso8859-1\\',\\n \\'es_ni\\':                                \\'es_ni.iso8859-1\\',\\n \\'es_pa\\':                                \\'es_pa.iso8859-1\\',\\n \\'es_pe\\':                                \\'es_pe.iso8859-1\\',\\n \\'es_pr\\':                                \\'es_pr.iso8859-1\\',\\n \\'es_py\\':                                \\'es_py.iso8859-1\\',\\n \\'es_sv\\':                                \\'es_sv.iso8859-1\\',\\n \\'es_us\\':                                \\'es_us.iso8859-1\\',\\n \\'es_uy\\':                                \\'es_uy.iso8859-1\\',\\n \\'es_ve\\':                                \\'es_ve.iso8859-1\\',\\n \\'estonian\\':                             \\'et_ee.iso8859-1\\',\\n \\'et\\':                                   \\'et_ee.iso8859-15\\',\\n \\'et_ee\\':                                \\'et_ee.iso8859-15\\',\\n \\'eu\\':                                   \\'eu_es.iso8859-1\\',\\n \\'eu_es\\':                                \\'eu_es.iso8859-1\\',\\n \\'eu_fr\\':                                \\'eu_fr.iso8859-1\\',\\n \\'fa\\':                                   \\'fa_ir.utf-8\\',\\n \\'fa_ir\\':                                \\'fa_ir.utf-8\\',\\n \\'fa_ir.isiri3342\\':                      \\'fa_ir.isiri-3342\\',\\n \\'ff_sn\\':                                \\'ff_sn.utf-8\\',\\n \\'fi\\':                                   \\'fi_fi.iso8859-15\\',\\n \\'fi_fi\\':                                \\'fi_fi.iso8859-15\\',\\n \\'fil_ph\\':                               \\'fil_ph.utf-8\\',\\n \\'finnish\\':                              \\'fi_fi.iso8859-1\\',\\n \\'fo\\':                                   \\'fo_fo.iso8859-1\\',\\n \\'fo_fo\\':                                \\'fo_fo.iso8859-1\\',\\n \\'fr\\':                                   \\'fr_fr.iso8859-1\\',\\n \\'fr_be\\':                                \\'fr_be.iso8859-1\\',\\n \\'fr_ca\\':                                \\'fr_ca.iso8859-1\\',\\n \\'fr_ch\\':                                \\'fr_ch.iso8859-1\\',\\n \\'fr_fr\\':                                \\'fr_fr.iso8859-1\\',\\n \\'fr_lu\\':                                \\'fr_lu.iso8859-1\\',\\n \\'fran\\\\xe7ais\\':                          \\'fr_fr.iso8859-1\\',\\n \\'fre_fr\\':                               \\'fr_fr.iso8859-1\\',\\n \\'french\\':                               \\'fr_fr.iso8859-1\\',\\n \\'french.iso88591\\':                      \\'fr_ch.iso8859-1\\',\\n \\'french_france\\':                        \\'fr_fr.iso8859-1\\',\\n \\'fur_it\\':                               \\'fur_it.utf-8\\',\\n \\'fy_de\\':                                \\'fy_de.utf-8\\',\\n \\'fy_nl\\':                                \\'fy_nl.utf-8\\',\\n \\'ga\\':                                   \\'ga_ie.iso8859-1\\',\\n \\'ga_ie\\':                                \\'ga_ie.iso8859-1\\',\\n \\'galego\\':                               \\'gl_es.iso8859-1\\',\\n \\'galician\\':                             \\'gl_es.iso8859-1\\',\\n \\'gd\\':                                   \\'gd_gb.iso8859-1\\',\\n \\'gd_gb\\':                                \\'gd_gb.iso8859-1\\',\\n \\'ger_de\\':                               \\'de_de.iso8859-1\\',\\n \\'german\\':                               \\'de_de.iso8859-1\\',\\n \\'german.iso88591\\':                      \\'de_ch.iso8859-1\\',\\n \\'german_germany\\':                       \\'de_de.iso8859-1\\',\\n \\'gez_er\\':                               \\'gez_er.utf-8\\',\\n \\'gez_et\\':                               \\'gez_et.utf-8\\',\\n \\'gl\\':                                   \\'gl_es.iso8859-1\\',\\n \\'gl_es\\':                                \\'gl_es.iso8859-1\\',\\n \\'greek\\':                                \\'el_gr.iso8859-7\\',\\n \\'gu_in\\':                                \\'gu_in.utf-8\\',\\n \\'gv\\':                                   \\'gv_gb.iso8859-1\\',\\n \\'gv_gb\\':                                \\'gv_gb.iso8859-1\\',\\n \\'ha_ng\\':                                \\'ha_ng.utf-8\\',\\n \\'he\\':                                   \\'he_il.iso8859-8\\',\\n \\'he_il\\':                                \\'he_il.iso8859-8\\',\\n \\'hebrew\\':                               \\'he_il.iso8859-8\\',\\n \\'hi\\':                                   \\'hi_in.iscii-dev\\',\\n \\'hi_in\\':                                \\'hi_in.iscii-dev\\',\\n \\'hi_in.isciidev\\':                       \\'hi_in.iscii-dev\\',\\n \\'hne\\':                                  \\'hne_in.utf-8\\',\\n \\'hne_in\\':                               \\'hne_in.utf-8\\',\\n \\'hr\\':                                   \\'hr_hr.iso8859-2\\',\\n \\'hr_hr\\':                                \\'hr_hr.iso8859-2\\',\\n \\'hrvatski\\':                             \\'hr_hr.iso8859-2\\',\\n \\'hsb_de\\':                               \\'hsb_de.iso8859-2\\',\\n \\'ht_ht\\':                                \\'ht_ht.utf-8\\',\\n \\'hu\\':                                   \\'hu_hu.iso8859-2\\',\\n \\'hu_hu\\':                                \\'hu_hu.iso8859-2\\',\\n \\'hungarian\\':                            \\'hu_hu.iso8859-2\\',\\n \\'hy_am\\':                                \\'hy_am.utf-8\\',\\n \\'hy_am.armscii8\\':                       \\'hy_am.armscii_8\\',\\n \\'ia\\':                                   \\'ia.utf-8\\',\\n \\'ia_fr\\':                                \\'ia_fr.utf-8\\',\\n \\'icelandic\\':                            \\'is_is.iso8859-1\\',\\n \\'id\\':                                   \\'id_id.iso8859-1\\',\\n \\'id_id\\':                                \\'id_id.iso8859-1\\',\\n \\'ig_ng\\':                                \\'ig_ng.utf-8\\',\\n \\'ik_ca\\':                                \\'ik_ca.utf-8\\',\\n \\'in\\':                                   \\'id_id.iso8859-1\\',\\n \\'in_id\\':                                \\'id_id.iso8859-1\\',\\n \\'is\\':                                   \\'is_is.iso8859-1\\',\\n \\'is_is\\':                                \\'is_is.iso8859-1\\',\\n \\'iso-8859-1\\':                           \\'en_us.iso8859-1\\',\\n \\'iso-8859-15\\':                          \\'en_us.iso8859-15\\',\\n \\'iso8859-1\\':                            \\'en_us.iso8859-1\\',\\n \\'iso8859-15\\':                           \\'en_us.iso8859-15\\',\\n \\'iso_8859_1\\':                           \\'en_us.iso8859-1\\',\\n \\'iso_8859_15\\':                          \\'en_us.iso8859-15\\',\\n \\'it\\':                                   \\'it_it.iso8859-1\\',\\n \\'it_ch\\':                                \\'it_ch.iso8859-1\\',\\n \\'it_it\\':                                \\'it_it.iso8859-1\\',\\n \\'italian\\':                              \\'it_it.iso8859-1\\',\\n \\'iu\\':                                   \\'iu_ca.nunacom-8\\',\\n \\'iu_ca\\':                                \\'iu_ca.nunacom-8\\',\\n \\'iu_ca.nunacom8\\':                       \\'iu_ca.nunacom-8\\',\\n \\'iw\\':                                   \\'he_il.iso8859-8\\',\\n \\'iw_il\\':                                \\'he_il.iso8859-8\\',\\n \\'iw_il.utf8\\':                           \\'iw_il.utf-8\\',\\n \\'ja\\':                                   \\'ja_jp.eucjp\\',\\n \\'ja_jp\\':                                \\'ja_jp.eucjp\\',\\n \\'ja_jp.euc\\':                            \\'ja_jp.eucjp\\',\\n \\'ja_jp.mscode\\':                         \\'ja_jp.sjis\\',\\n \\'ja_jp.pck\\':                            \\'ja_jp.sjis\\',\\n \\'japan\\':                                \\'ja_jp.eucjp\\',\\n \\'japanese\\':                             \\'ja_jp.eucjp\\',\\n \\'japanese-euc\\':                         \\'ja_jp.eucjp\\',\\n \\'japanese.euc\\':                         \\'ja_jp.eucjp\\',\\n \\'jp_jp\\':                                \\'ja_jp.eucjp\\',\\n \\'ka\\':                                   \\'ka_ge.georgian-academy\\',\\n \\'ka_ge\\':                                \\'ka_ge.georgian-academy\\',\\n \\'ka_ge.georgianacademy\\':                \\'ka_ge.georgian-academy\\',\\n \\'ka_ge.georgianps\\':                     \\'ka_ge.georgian-ps\\',\\n \\'ka_ge.georgianrs\\':                     \\'ka_ge.georgian-academy\\',\\n \\'kk_kz\\':                                \\'kk_kz.rk1048\\',\\n \\'kl\\':                                   \\'kl_gl.iso8859-1\\',\\n \\'kl_gl\\':                                \\'kl_gl.iso8859-1\\',\\n \\'km_kh\\':                                \\'km_kh.utf-8\\',\\n \\'kn\\':                                   \\'kn_in.utf-8\\',\\n \\'kn_in\\':                                \\'kn_in.utf-8\\',\\n \\'ko\\':                                   \\'ko_kr.euckr\\',\\n \\'ko_kr\\':                                \\'ko_kr.euckr\\',\\n \\'ko_kr.euc\\':                            \\'ko_kr.euckr\\',\\n \\'kok_in\\':                               \\'kok_in.utf-8\\',\\n \\'korean\\':                               \\'ko_kr.euckr\\',\\n \\'korean.euc\\':                           \\'ko_kr.euckr\\',\\n \\'ks\\':                                   \\'ks_in.utf-8\\',\\n \\'ks_in\\':                                \\'ks_in.utf-8\\',\\n \\'ks_in@devanagari.utf8\\':                \\'ks_in.utf-8@devanagari\\',\\n \\'ku_tr\\':                                \\'ku_tr.iso8859-9\\',\\n \\'kw\\':                                   \\'kw_gb.iso8859-1\\',\\n \\'kw_gb\\':                                \\'kw_gb.iso8859-1\\',\\n \\'ky\\':                                   \\'ky_kg.utf-8\\',\\n \\'ky_kg\\':                                \\'ky_kg.utf-8\\',\\n \\'lb_lu\\':                                \\'lb_lu.utf-8\\',\\n \\'lg_ug\\':                                \\'lg_ug.iso8859-10\\',\\n \\'li_be\\':                                \\'li_be.utf-8\\',\\n \\'li_nl\\':                                \\'li_nl.utf-8\\',\\n \\'lij_it\\':                               \\'lij_it.utf-8\\',\\n \\'lithuanian\\':                           \\'lt_lt.iso8859-13\\',\\n \\'lo\\':                                   \\'lo_la.mulelao-1\\',\\n \\'lo_la\\':                                \\'lo_la.mulelao-1\\',\\n \\'lo_la.cp1133\\':                         \\'lo_la.ibm-cp1133\\',\\n \\'lo_la.ibmcp1133\\':                      \\'lo_la.ibm-cp1133\\',\\n \\'lo_la.mulelao1\\':                       \\'lo_la.mulelao-1\\',\\n \\'lt\\':                                   \\'lt_lt.iso8859-13\\',\\n \\'lt_lt\\':                                \\'lt_lt.iso8859-13\\',\\n \\'lv\\':                                   \\'lv_lv.iso8859-13\\',\\n \\'lv_lv\\':                                \\'lv_lv.iso8859-13\\',\\n \\'mag_in\\':                               \\'mag_in.utf-8\\',\\n \\'mai\\':                                  \\'mai_in.utf-8\\',\\n \\'mai_in\\':                               \\'mai_in.utf-8\\',\\n \\'mg_mg\\':                                \\'mg_mg.iso8859-15\\',\\n \\'mhr_ru\\':                               \\'mhr_ru.utf-8\\',\\n \\'mi\\':                                   \\'mi_nz.iso8859-1\\',\\n \\'mi_nz\\':                                \\'mi_nz.iso8859-1\\',\\n \\'mk\\':                                   \\'mk_mk.iso8859-5\\',\\n \\'mk_mk\\':                                \\'mk_mk.iso8859-5\\',\\n \\'ml\\':                                   \\'ml_in.utf-8\\',\\n \\'ml_in\\':                                \\'ml_in.utf-8\\',\\n \\'mn_mn\\':                                \\'mn_mn.utf-8\\',\\n \\'mni_in\\':                               \\'mni_in.utf-8\\',\\n \\'mr\\':                                   \\'mr_in.utf-8\\',\\n \\'mr_in\\':                                \\'mr_in.utf-8\\',\\n \\'ms\\':                                   \\'ms_my.iso8859-1\\',\\n \\'ms_my\\':                                \\'ms_my.iso8859-1\\',\\n \\'mt\\':                                   \\'mt_mt.iso8859-3\\',\\n \\'mt_mt\\':                                \\'mt_mt.iso8859-3\\',\\n \\'my_mm\\':                                \\'my_mm.utf-8\\',\\n \\'nan_tw@latin\\':                         \\'nan_tw.utf-8@latin\\',\\n \\'nb\\':                                   \\'nb_no.iso8859-1\\',\\n \\'nb_no\\':                                \\'nb_no.iso8859-1\\',\\n \\'nds_de\\':                               \\'nds_de.utf-8\\',\\n \\'nds_nl\\':                               \\'nds_nl.utf-8\\',\\n \\'ne_np\\':                                \\'ne_np.utf-8\\',\\n \\'nhn_mx\\':                               \\'nhn_mx.utf-8\\',\\n \\'niu_nu\\':                               \\'niu_nu.utf-8\\',\\n \\'niu_nz\\':                               \\'niu_nz.utf-8\\',\\n \\'nl\\':                                   \\'nl_nl.iso8859-1\\',\\n \\'nl_aw\\':                                \\'nl_aw.utf-8\\',\\n \\'nl_be\\':                                \\'nl_be.iso8859-1\\',\\n \\'nl_nl\\':                                \\'nl_nl.iso8859-1\\',\\n \\'nn\\':                                   \\'nn_no.iso8859-1\\',\\n \\'nn_no\\':                                \\'nn_no.iso8859-1\\',\\n \\'no\\':                                   \\'no_no.iso8859-1\\',\\n \\'no@nynorsk\\':                           \\'ny_no.iso8859-1\\',\\n \\'no_no\\':                                \\'no_no.iso8859-1\\',\\n \\'no_no.iso88591@bokmal\\':                \\'no_no.iso8859-1\\',\\n \\'no_no.iso88591@nynorsk\\':               \\'no_no.iso8859-1\\',\\n \\'norwegian\\':                            \\'no_no.iso8859-1\\',\\n \\'nr\\':                                   \\'nr_za.iso8859-1\\',\\n \\'nr_za\\':                                \\'nr_za.iso8859-1\\',\\n \\'nso\\':                                  \\'nso_za.iso8859-15\\',\\n \\'nso_za\\':                               \\'nso_za.iso8859-15\\',\\n \\'ny\\':                                   \\'ny_no.iso8859-1\\',\\n \\'ny_no\\':                                \\'ny_no.iso8859-1\\',\\n \\'nynorsk\\':                              \\'nn_no.iso8859-1\\',\\n \\'oc\\':                                   \\'oc_fr.iso8859-1\\',\\n \\'oc_fr\\':                                \\'oc_fr.iso8859-1\\',\\n \\'om_et\\':                                \\'om_et.utf-8\\',\\n \\'om_ke\\':                                \\'om_ke.iso8859-1\\',\\n \\'or\\':                                   \\'or_in.utf-8\\',\\n \\'or_in\\':                                \\'or_in.utf-8\\',\\n \\'os_ru\\':                                \\'os_ru.utf-8\\',\\n \\'pa\\':                                   \\'pa_in.utf-8\\',\\n \\'pa_in\\':                                \\'pa_in.utf-8\\',\\n \\'pa_pk\\':                                \\'pa_pk.utf-8\\',\\n \\'pap_an\\':                               \\'pap_an.utf-8\\',\\n \\'pd\\':                                   \\'pd_us.iso8859-1\\',\\n \\'pd_de\\':                                \\'pd_de.iso8859-1\\',\\n \\'pd_us\\':                                \\'pd_us.iso8859-1\\',\\n \\'ph\\':                                   \\'ph_ph.iso8859-1\\',\\n \\'ph_ph\\':                                \\'ph_ph.iso8859-1\\',\\n \\'pl\\':                                   \\'pl_pl.iso8859-2\\',\\n \\'pl_pl\\':                                \\'pl_pl.iso8859-2\\',\\n \\'polish\\':                               \\'pl_pl.iso8859-2\\',\\n \\'portuguese\\':                           \\'pt_pt.iso8859-1\\',\\n \\'portuguese_brazil\\':                    \\'pt_br.iso8859-1\\',\\n \\'posix\\':                                \\'c\\',\\n \\'posix-utf2\\':                           \\'c\\',\\n \\'pp\\':                                   \\'pp_an.iso8859-1\\',\\n \\'pp_an\\':                                \\'pp_an.iso8859-1\\',\\n \\'ps_af\\':                                \\'ps_af.utf-8\\',\\n \\'pt\\':                                   \\'pt_pt.iso8859-1\\',\\n \\'pt_br\\':                                \\'pt_br.iso8859-1\\',\\n \\'pt_pt\\':                                \\'pt_pt.iso8859-1\\',\\n \\'ro\\':                                   \\'ro_ro.iso8859-2\\',\\n \\'ro_ro\\':                                \\'ro_ro.iso8859-2\\',\\n \\'romanian\\':                             \\'ro_ro.iso8859-2\\',\\n \\'ru\\':                                   \\'ru_ru.utf-8\\',\\n \\'ru_ru\\':                                \\'ru_ru.utf-8\\',\\n \\'ru_ua\\':                                \\'ru_ua.koi8-u\\',\\n \\'rumanian\\':                             \\'ro_ro.iso8859-2\\',\\n \\'russian\\':                              \\'ru_ru.iso8859-5\\',\\n \\'rw\\':                                   \\'rw_rw.iso8859-1\\',\\n \\'rw_rw\\':                                \\'rw_rw.iso8859-1\\',\\n \\'sa_in\\':                                \\'sa_in.utf-8\\',\\n \\'sat_in\\':                               \\'sat_in.utf-8\\',\\n \\'sc_it\\':                                \\'sc_it.utf-8\\',\\n \\'sd\\':                                   \\'sd_in.utf-8\\',\\n \\'sd_in\\':                                \\'sd_in.utf-8\\',\\n \\'sd_in@devanagari.utf8\\':                \\'sd_in.utf-8@devanagari\\',\\n \\'sd_pk\\':                                \\'sd_pk.utf-8\\',\\n \\'se_no\\':                                \\'se_no.utf-8\\',\\n \\'serbocroatian\\':                        \\'sr_rs.utf-8@latin\\',\\n \\'sh\\':                                   \\'sr_rs.utf-8@latin\\',\\n \\'sh_ba.iso88592@bosnia\\':                \\'sr_cs.iso8859-2\\',\\n \\'sh_hr\\':                                \\'sh_hr.iso8859-2\\',\\n \\'sh_hr.iso88592\\':                       \\'hr_hr.iso8859-2\\',\\n \\'sh_sp\\':                                \\'sr_cs.iso8859-2\\',\\n \\'sh_yu\\':                                \\'sr_rs.utf-8@latin\\',\\n \\'shs_ca\\':                               \\'shs_ca.utf-8\\',\\n \\'si\\':                                   \\'si_lk.utf-8\\',\\n \\'si_lk\\':                                \\'si_lk.utf-8\\',\\n \\'sid_et\\':                               \\'sid_et.utf-8\\',\\n \\'sinhala\\':                              \\'si_lk.utf-8\\',\\n \\'sk\\':                                   \\'sk_sk.iso8859-2\\',\\n \\'sk_sk\\':                                \\'sk_sk.iso8859-2\\',\\n \\'sl\\':                                   \\'sl_si.iso8859-2\\',\\n \\'sl_cs\\':                                \\'sl_cs.iso8859-2\\',\\n \\'sl_si\\':                                \\'sl_si.iso8859-2\\',\\n \\'slovak\\':                               \\'sk_sk.iso8859-2\\',\\n \\'slovene\\':                              \\'sl_si.iso8859-2\\',\\n \\'slovenian\\':                            \\'sl_si.iso8859-2\\',\\n \\'so_dj\\':                                \\'so_dj.iso8859-1\\',\\n \\'so_et\\':                                \\'so_et.utf-8\\',\\n \\'so_ke\\':                                \\'so_ke.iso8859-1\\',\\n \\'so_so\\':                                \\'so_so.iso8859-1\\',\\n \\'sp\\':                                   \\'sr_cs.iso8859-5\\',\\n \\'sp_yu\\':                                \\'sr_cs.iso8859-5\\',\\n \\'spanish\\':                              \\'es_es.iso8859-1\\',\\n \\'spanish_spain\\':                        \\'es_es.iso8859-1\\',\\n \\'sq\\':                                   \\'sq_al.iso8859-2\\',\\n \\'sq_al\\':                                \\'sq_al.iso8859-2\\',\\n \\'sq_mk\\':                                \\'sq_mk.utf-8\\',\\n \\'sr\\':                                   \\'sr_rs.utf-8\\',\\n \\'sr@cyrillic\\':                          \\'sr_rs.utf-8\\',\\n \\'sr@latn\\':                              \\'sr_cs.utf-8@latin\\',\\n \\'sr_cs\\':                                \\'sr_cs.utf-8\\',\\n \\'sr_cs.iso88592@latn\\':                  \\'sr_cs.iso8859-2\\',\\n \\'sr_cs@latn\\':                           \\'sr_cs.utf-8@latin\\',\\n \\'sr_me\\':                                \\'sr_me.utf-8\\',\\n \\'sr_rs\\':                                \\'sr_rs.utf-8\\',\\n \\'sr_rs@latn\\':                           \\'sr_rs.utf-8@latin\\',\\n \\'sr_sp\\':                                \\'sr_cs.iso8859-2\\',\\n \\'sr_yu\\':                                \\'sr_rs.utf-8@latin\\',\\n \\'sr_yu.cp1251@cyrillic\\':                \\'sr_cs.cp1251\\',\\n \\'sr_yu.iso88592\\':                       \\'sr_cs.iso8859-2\\',\\n \\'sr_yu.iso88595\\':                       \\'sr_cs.iso8859-5\\',\\n \\'sr_yu.iso88595@cyrillic\\':              \\'sr_cs.iso8859-5\\',\\n \\'sr_yu.microsoftcp1251@cyrillic\\':       \\'sr_cs.cp1251\\',\\n \\'sr_yu.utf8\\':                           \\'sr_rs.utf-8\\',\\n \\'sr_yu.utf8@cyrillic\\':                  \\'sr_rs.utf-8\\',\\n \\'sr_yu@cyrillic\\':                       \\'sr_rs.utf-8\\',\\n \\'ss\\':                                   \\'ss_za.iso8859-1\\',\\n \\'ss_za\\':                                \\'ss_za.iso8859-1\\',\\n \\'st\\':                                   \\'st_za.iso8859-1\\',\\n \\'st_za\\':                                \\'st_za.iso8859-1\\',\\n \\'sv\\':                                   \\'sv_se.iso8859-1\\',\\n \\'sv_fi\\':                                \\'sv_fi.iso8859-1\\',\\n \\'sv_se\\':                                \\'sv_se.iso8859-1\\',\\n \\'sw_ke\\':                                \\'sw_ke.utf-8\\',\\n \\'sw_tz\\':                                \\'sw_tz.utf-8\\',\\n \\'swedish\\':                              \\'sv_se.iso8859-1\\',\\n \\'szl_pl\\':                               \\'szl_pl.utf-8\\',\\n \\'ta\\':                                   \\'ta_in.tscii-0\\',\\n \\'ta_in\\':                                \\'ta_in.tscii-0\\',\\n \\'ta_in.tscii\\':                          \\'ta_in.tscii-0\\',\\n \\'ta_in.tscii0\\':                         \\'ta_in.tscii-0\\',\\n \\'ta_lk\\':                                \\'ta_lk.utf-8\\',\\n \\'te\\':                                   \\'te_in.utf-8\\',\\n \\'te_in\\':                                \\'te_in.utf-8\\',\\n \\'tg\\':                                   \\'tg_tj.koi8-c\\',\\n \\'tg_tj\\':                                \\'tg_tj.koi8-c\\',\\n \\'th\\':                                   \\'th_th.iso8859-11\\',\\n \\'th_th\\':                                \\'th_th.iso8859-11\\',\\n \\'th_th.tactis\\':                         \\'th_th.tis620\\',\\n \\'th_th.tis620\\':                         \\'th_th.tis620\\',\\n \\'thai\\':                                 \\'th_th.iso8859-11\\',\\n \\'ti_er\\':                                \\'ti_er.utf-8\\',\\n \\'ti_et\\':                                \\'ti_et.utf-8\\',\\n \\'tig_er\\':                               \\'tig_er.utf-8\\',\\n \\'tk_tm\\':                                \\'tk_tm.utf-8\\',\\n \\'tl\\':                                   \\'tl_ph.iso8859-1\\',\\n \\'tl_ph\\':                                \\'tl_ph.iso8859-1\\',\\n \\'tn\\':                                   \\'tn_za.iso8859-15\\',\\n \\'tn_za\\':                                \\'tn_za.iso8859-15\\',\\n \\'tr\\':                                   \\'tr_tr.iso8859-9\\',\\n \\'tr_cy\\':                                \\'tr_cy.iso8859-9\\',\\n \\'tr_tr\\':                                \\'tr_tr.iso8859-9\\',\\n \\'ts\\':                                   \\'ts_za.iso8859-1\\',\\n \\'ts_za\\':                                \\'ts_za.iso8859-1\\',\\n \\'tt\\':                                   \\'tt_ru.tatar-cyr\\',\\n \\'tt_ru\\':                                \\'tt_ru.tatar-cyr\\',\\n \\'tt_ru.tatarcyr\\':                       \\'tt_ru.tatar-cyr\\',\\n \\'tt_ru@iqtelif\\':                        \\'tt_ru.utf-8@iqtelif\\',\\n \\'turkish\\':                              \\'tr_tr.iso8859-9\\',\\n \\'ug_cn\\':                                \\'ug_cn.utf-8\\',\\n \\'uk\\':                                   \\'uk_ua.koi8-u\\',\\n \\'uk_ua\\':                                \\'uk_ua.koi8-u\\',\\n \\'univ\\':                                 \\'en_us.utf-8\\',\\n \\'universal\\':                            \\'en_us.utf-8\\',\\n \\'universal.utf8@ucs4\\':                  \\'en_us.utf-8\\',\\n \\'unm_us\\':                               \\'unm_us.utf-8\\',\\n \\'ur\\':                                   \\'ur_pk.cp1256\\',\\n \\'ur_in\\':                                \\'ur_in.utf-8\\',\\n \\'ur_pk\\':                                \\'ur_pk.cp1256\\',\\n \\'uz\\':                                   \\'uz_uz.utf-8\\',\\n \\'uz_uz\\':                                \\'uz_uz.utf-8\\',\\n \\'uz_uz@cyrillic\\':                       \\'uz_uz.utf-8\\',\\n \\'ve\\':                                   \\'ve_za.utf-8\\',\\n \\'ve_za\\':                                \\'ve_za.utf-8\\',\\n \\'vi\\':                                   \\'vi_vn.tcvn\\',\\n \\'vi_vn\\':                                \\'vi_vn.tcvn\\',\\n \\'vi_vn.tcvn\\':                           \\'vi_vn.tcvn\\',\\n \\'vi_vn.tcvn5712\\':                       \\'vi_vn.tcvn\\',\\n \\'vi_vn.viscii\\':                         \\'vi_vn.viscii\\',\\n \\'vi_vn.viscii111\\':                      \\'vi_vn.viscii\\',\\n \\'wa\\':                                   \\'wa_be.iso8859-1\\',\\n \\'wa_be\\':                                \\'wa_be.iso8859-1\\',\\n \\'wae_ch\\':                               \\'wae_ch.utf-8\\',\\n \\'wal_et\\':                               \\'wal_et.utf-8\\',\\n \\'wo_sn\\':                                \\'wo_sn.utf-8\\',\\n \\'xh\\':                                   \\'xh_za.iso8859-1\\',\\n \\'xh_za\\':                                \\'xh_za.iso8859-1\\',\\n \\'yi\\':                                   \\'yi_us.cp1255\\',\\n \\'yi_us\\':                                \\'yi_us.cp1255\\',\\n \\'yo_ng\\':                                \\'yo_ng.utf-8\\',\\n \\'yue_hk\\':                               \\'yue_hk.utf-8\\',\\n \\'zh\\':                                   \\'zh_cn.euccn\\',\\n \\'zh_cn\\':                                \\'zh_cn.gb2312\\',\\n \\'zh_cn.big5\\':                           \\'zh_tw.big5\\',\\n \\'zh_cn.euc\\':                            \\'zh_cn.euccn\\',\\n \\'zh_hk\\':                                \\'zh_hk.big5hkscs\\',\\n \\'zh_hk.big5hk\\':                         \\'zh_hk.big5hkscs\\',\\n \\'zh_sg\\':                                \\'zh_sg.gb2312\\',\\n \\'zh_sg.gbk\\':                            \\'zh_sg.gbk\\',\\n \\'zh_tw\\':                                \\'zh_tw.big5\\',\\n \\'zh_tw.euc\\':                            \\'zh_tw.euctw\\',\\n \\'zh_tw.euctw\\':                          \\'zh_tw.euctw\\',\\n \\'zu\\':                                   \\'zu_za.iso8859-1\\',\\n \\'zu_za\\':                                \\'zu_za.iso8859-1\\',\\n }\\n #\\n # this maps windows language identifiers to locale strings.\\n #\\n # this list has been updated from\\n # http://msdn.microsoft.com/library/default.asp?url=/library/en-us/intl/nls_238z.asp\\n # to include every locale up to windows vista.\\n #\\n # note: this mapping is incomplete.  if your language is missing, please\\n # submit a bug report to the python bug tracker at http://bugs.python.org/\\n # make sure you include the missing language identifier and the suggested\\n # locale code.\\n #\\n windows_locale = {\\n 0x0436: \"af_za\", # afrikaans\\n 0x041c: \"sq_al\", # albanian\\n 0x0484: \"gsw_fr\",# alsatian - france\\n 0x045e: \"am_et\", # amharic - ethiopia\\n 0x0401: \"ar_sa\", # arabic - saudi arabia\\n 0x0801: \"ar_iq\", # arabic - iraq\\n 0x0c01: \"ar_eg\", # arabic - egypt\\n 0x1001: \"ar_ly\", # arabic - libya\\n 0x1401: \"ar_dz\", # arabic - algeria\\n 0x1801: \"ar_ma\", # arabic - morocco\\n 0x1c01: \"ar_tn\", # arabic - tunisia\\n 0x2001: \"ar_om\", # arabic - oman\\n 0x2401: \"ar_ye\", # arabic - yemen\\n 0x2801: \"ar_sy\", # arabic - syria\\n 0x2c01: \"ar_jo\", # arabic - jordan\\n 0x3001: \"ar_lb\", # arabic - lebanon\\n 0x3401: \"ar_kw\", # arabic - kuwait\\n 0x3801: \"ar_ae\", # arabic - united arab emirates\\n 0x3c01: \"ar_bh\", # arabic - bahrain\\n 0x4001: \"ar_qa\", # arabic - qatar\\n 0x042b: \"hy_am\", # armenian\\n 0x044d: \"as_in\", # assamese - india\\n 0x042c: \"az_az\", # azeri - latin\\n 0x082c: \"az_az\", # azeri - cyrillic\\n 0x046d: \"ba_ru\", # bashkir\\n 0x042d: \"eu_es\", # basque - russia\\n 0x0423: \"be_by\", # belarusian\\n 0x0445: \"bn_in\", # begali\\n 0x201a: \"bs_ba\", # bosnian - cyrillic\\n 0x141a: \"bs_ba\", # bosnian - latin\\n 0x047e: \"br_fr\", # breton - france\\n 0x0402: \"bg_bg\", # bulgarian\\n #    0x0455: \"my_mm\", # burmese - not supported\\n 0x0403: \"ca_es\", # catalan\\n 0x0004: \"zh_chs\",# chinese - simplified\\n 0x0404: \"zh_tw\", # chinese - taiwan\\n 0x0804: \"zh_cn\", # chinese - prc\\n 0x0c04: \"zh_hk\", # chinese - hong kong s.a.r.\\n 0x1004: \"zh_sg\", # chinese - singapore\\n 0x1404: \"zh_mo\", # chinese - macao s.a.r.\\n 0x7c04: \"zh_cht\",# chinese - traditional\\n 0x0483: \"co_fr\", # corsican - france\\n 0x041a: \"hr_hr\", # croatian\\n 0x101a: \"hr_ba\", # croatian - bosnia\\n 0x0405: \"cs_cz\", # czech\\n 0x0406: \"da_dk\", # danish\\n 0x048c: \"gbz_af\",# dari - afghanistan\\n 0x0465: \"div_mv\",# divehi - maldives\\n 0x0413: \"nl_nl\", # dutch - the netherlands\\n 0x0813: \"nl_be\", # dutch - belgium\\n 0x0409: \"en_us\", # english - united states\\n 0x0809: \"en_gb\", # english - united kingdom\\n 0x0c09: \"en_au\", # english - australia\\n 0x1009: \"en_ca\", # english - canada\\n 0x1409: \"en_nz\", # english - new zealand\\n 0x1809: \"en_ie\", # english - ireland\\n 0x1c09: \"en_za\", # english - south africa\\n 0x2009: \"en_ja\", # english - jamaica\\n 0x2409: \"en_cb\", # english - caribbean\\n 0x2809: \"en_bz\", # english - belize\\n 0x2c09: \"en_tt\", # english - trinidad\\n 0x3009: \"en_zw\", # english - zimbabwe\\n 0x3409: \"en_ph\", # english - philippines\\n 0x4009: \"en_in\", # english - india\\n 0x4409: \"en_my\", # english - malaysia\\n 0x4809: \"en_in\", # english - singapore\\n 0x0425: \"et_ee\", # estonian\\n 0x0438: \"fo_fo\", # faroese\\n 0x0464: \"fil_ph\",# filipino\\n 0x040b: \"fi_fi\", # finnish\\n 0x040c: \"fr_fr\", # french - france\\n 0x080c: \"fr_be\", # french - belgium\\n 0x0c0c: \"fr_ca\", # french - canada\\n 0x100c: \"fr_ch\", # french - switzerland\\n 0x140c: \"fr_lu\", # french - luxembourg\\n 0x180c: \"fr_mc\", # french - monaco\\n 0x0462: \"fy_nl\", # frisian - netherlands\\n 0x0456: \"gl_es\", # galician\\n 0x0437: \"ka_ge\", # georgian\\n 0x0407: \"de_de\", # german - germany\\n 0x0807: \"de_ch\", # german - switzerland\\n 0x0c07: \"de_at\", # german - austria\\n 0x1007: \"de_lu\", # german - luxembourg\\n 0x1407: \"de_li\", # german - liechtenstein\\n 0x0408: \"el_gr\", # greek\\n 0x046f: \"kl_gl\", # greenlandic - greenland\\n 0x0447: \"gu_in\", # gujarati\\n 0x0468: \"ha_ng\", # hausa - latin\\n 0x040d: \"he_il\", # hebrew\\n 0x0439: \"hi_in\", # hindi\\n 0x040e: \"hu_hu\", # hungarian\\n 0x040f: \"is_is\", # icelandic\\n 0x0421: \"id_id\", # indonesian\\n 0x045d: \"iu_ca\", # inuktitut - syllabics\\n 0x085d: \"iu_ca\", # inuktitut - latin\\n 0x083c: \"ga_ie\", # irish - ireland\\n 0x0410: \"it_it\", # italian - italy\\n 0x0810: \"it_ch\", # italian - switzerland\\n 0x0411: \"ja_jp\", # japanese\\n 0x044b: \"kn_in\", # kannada - india\\n 0x043f: \"kk_kz\", # kazakh\\n 0x0453: \"kh_kh\", # khmer - cambodia\\n 0x0486: \"qut_gt\",# k\\'iche - guatemala\\n 0x0487: \"rw_rw\", # kinyarwanda - rwanda\\n 0x0457: \"kok_in\",# konkani\\n 0x0412: \"ko_kr\", # korean\\n 0x0440: \"ky_kg\", # kyrgyz\\n 0x0454: \"lo_la\", # lao - lao pdr\\n 0x0426: \"lv_lv\", # latvian\\n 0x0427: \"lt_lt\", # lithuanian\\n 0x082e: \"dsb_de\",# lower sorbian - germany\\n 0x046e: \"lb_lu\", # luxembourgish\\n 0x042f: \"mk_mk\", # fyrom macedonian\\n 0x043e: \"ms_my\", # malay - malaysia\\n 0x083e: \"ms_bn\", # malay - brunei darussalam\\n 0x044c: \"ml_in\", # malayalam - india\\n 0x043a: \"mt_mt\", # maltese\\n 0x0481: \"mi_nz\", # maori\\n 0x047a: \"arn_cl\",# mapudungun\\n 0x044e: \"mr_in\", # marathi\\n 0x047c: \"moh_ca\",# mohawk - canada\\n 0x0450: \"mn_mn\", # mongolian - cyrillic\\n 0x0850: \"mn_cn\", # mongolian - prc\\n 0x0461: \"ne_np\", # nepali\\n 0x0414: \"nb_no\", # norwegian - bokmal\\n 0x0814: \"nn_no\", # norwegian - nynorsk\\n 0x0482: \"oc_fr\", # occitan - france\\n 0x0448: \"or_in\", # oriya - india\\n 0x0463: \"ps_af\", # pashto - afghanistan\\n 0x0429: \"fa_ir\", # persian\\n 0x0415: \"pl_pl\", # polish\\n 0x0416: \"pt_br\", # portuguese - brazil\\n 0x0816: \"pt_pt\", # portuguese - portugal\\n 0x0446: \"pa_in\", # punjabi\\n 0x046b: \"quz_bo\",# quechua (bolivia)\\n 0x086b: \"quz_ec\",# quechua (ecuador)\\n 0x0c6b: \"quz_pe\",# quechua (peru)\\n 0x0418: \"ro_ro\", # romanian - romania\\n 0x0417: \"rm_ch\", # romansh\\n 0x0419: \"ru_ru\", # russian\\n 0x243b: \"smn_fi\",# sami finland\\n 0x103b: \"smj_no\",# sami norway\\n 0x143b: \"smj_se\",# sami sweden\\n 0x043b: \"se_no\", # sami northern norway\\n 0x083b: \"se_se\", # sami northern sweden\\n 0x0c3b: \"se_fi\", # sami northern finland\\n 0x203b: \"sms_fi\",# sami skolt\\n 0x183b: \"sma_no\",# sami southern norway\\n 0x1c3b: \"sma_se\",# sami southern sweden\\n 0x044f: \"sa_in\", # sanskrit\\n 0x0c1a: \"sr_sp\", # serbian - cyrillic\\n 0x1c1a: \"sr_ba\", # serbian - bosnia cyrillic\\n 0x081a: \"sr_sp\", # serbian - latin\\n 0x181a: \"sr_ba\", # serbian - bosnia latin\\n 0x045b: \"si_lk\", # sinhala - sri lanka\\n 0x046c: \"ns_za\", # northern sotho\\n 0x0432: \"tn_za\", # setswana - southern africa\\n 0x041b: \"sk_sk\", # slovak\\n 0x0424: \"sl_si\", # slovenian\\n 0x040a: \"es_es\", # spanish - spain\\n 0x080a: \"es_mx\", # spanish - mexico\\n 0x0c0a: \"es_es\", # spanish - spain (modern)\\n 0x100a: \"es_gt\", # spanish - guatemala\\n 0x140a: \"es_cr\", # spanish - costa rica\\n 0x180a: \"es_pa\", # spanish - panama\\n 0x1c0a: \"es_do\", # spanish - dominican republic\\n 0x200a: \"es_ve\", # spanish - venezuela\\n 0x240a: \"es_co\", # spanish - colombia\\n 0x280a: \"es_pe\", # spanish - peru\\n 0x2c0a: \"es_ar\", # spanish - argentina\\n 0x300a: \"es_ec\", # spanish - ecuador\\n 0x340a: \"es_cl\", # spanish - chile\\n 0x380a: \"es_ur\", # spanish - uruguay\\n 0x3c0a: \"es_py\", # spanish - paraguay\\n 0x400a: \"es_bo\", # spanish - bolivia\\n 0x440a: \"es_sv\", # spanish - el salvador\\n 0x480a: \"es_hn\", # spanish - honduras\\n 0x4c0a: \"es_ni\", # spanish - nicaragua\\n 0x500a: \"es_pr\", # spanish - puerto rico\\n 0x540a: \"es_us\", # spanish - united states\\n #    0x0430: \"\", # sutu - not supported\\n 0x0441: \"sw_ke\", # swahili\\n 0x041d: \"sv_se\", # swedish - sweden\\n 0x081d: \"sv_fi\", # swedish - finland\\n 0x045a: \"syr_sy\",# syriac\\n 0x0428: \"tg_tj\", # tajik - cyrillic\\n 0x085f: \"tmz_dz\",# tamazight - latin\\n 0x0449: \"ta_in\", # tamil\\n 0x0444: \"tt_ru\", # tatar\\n 0x044a: \"te_in\", # telugu\\n 0x041e: \"th_th\", # thai\\n 0x0851: \"bo_bt\", # tibetan - bhutan\\n 0x0451: \"bo_cn\", # tibetan - prc\\n 0x041f: \"tr_tr\", # turkish\\n 0x0442: \"tk_tm\", # turkmen - cyrillic\\n 0x0480: \"ug_cn\", # uighur - arabic\\n 0x0422: \"uk_ua\", # ukrainian\\n 0x042e: \"wen_de\",# upper sorbian - germany\\n 0x0420: \"ur_pk\", # urdu\\n 0x0820: \"ur_in\", # urdu - india\\n 0x0443: \"uz_uz\", # uzbek - latin\\n 0x0843: \"uz_uz\", # uzbek - cyrillic\\n 0x042a: \"vi_vn\", # vietnamese\\n 0x0452: \"cy_gb\", # welsh\\n 0x0488: \"wo_sn\", # wolof - senegal\\n 0x0434: \"xh_za\", # xhosa - south africa\\n 0x0485: \"sah_ru\",# yakut - cyrillic\\n 0x0478: \"ii_cn\", # yi - prc\\n 0x046a: \"yo_ng\", # yoruba - nigeria\\n 0x0435: \"zu_za\", # zulu\\n }\\n def _print_locale():\\n \"\"\" test function.\\n \"\"\"\\n categories = {}\\n def _init_categories(categories=categories):\\n for k,v in globals().items():\\n if k[:3] == \\'lc_\\':\\n categories[k] = v\\n _init_categories()\\n del categories[\\'lc_all\\']\\n print(\\'locale defaults as determined by getdefaultlocale():\\')\\n print(\\'-\\'*72)\\n lang, enc = getdefaultlocale()\\n print(\\'language: \\', lang or \\'(undefined)\\')\\n print(\\'encoding: \\', enc or \\'(undefined)\\')\\n print()\\n print(\\'locale settings on startup:\\')\\n print(\\'-\\'*72)\\n for name,category in categories.items():\\n print(name, \\'...\\')\\n lang, enc = getlocale(category)\\n print(\\'   language: \\', lang or \\'(undefined)\\')\\n print(\\'   encoding: \\', enc or \\'(undefined)\\')\\n print()\\n print()\\n print(\\'locale settings after calling resetlocale():\\')\\n print(\\'-\\'*72)\\n resetlocale()\\n for name,category in categories.items():\\n print(name, \\'...\\')\\n lang, enc = getlocale(category)\\n print(\\'   language: \\', lang or \\'(undefined)\\')\\n print(\\'   encoding: \\', enc or \\'(undefined)\\')\\n print()\\n try:\\n setlocale(lc_all, \"\")\\n except:\\n print(\\'note:\\')\\n print(\\'setlocale(lc_all, \"\") does not support the default locale\\')\\n print(\\'given in the os environment variables.\\')\\n else:\\n print()\\n print(\\'locale settings after calling setlocale(lc_all, \"\"):\\')\\n print(\\'-\\'*72)\\n for name,category in categories.items():\\n print(name, \\'...\\')\\n lang, enc = getlocale(category)\\n print(\\'   language: \\', lang or \\'(undefined)\\')\\n print(\\'   encoding: \\', enc or \\'(undefined)\\')\\n print()\\n ###\\n try:\\n lc_messages\\n except nameerror:\\n pass\\n else:\\n __all__.append(\"lc_messages\")\\n if __name__==\\'__main__\\':\\n print(\\'locale aliasing:\\')\\n print()\\n _print_locale()\\n print()\\n print(\\'number formatting:\\')\\n print()\\n _test()\\n # module \\'ntpath\\' -- common operations on winnt/win95 pathnames\\n \"\"\"common pathname manipulations, windowsnt/95 version.\\n instead of importing this module directly, import os and refer to this\\n module as os.path.\\n \"\"\"\\n import os\\n import sys\\n import stat\\n import genericpath\\n from genericpath import *\\n __all__ = [\"normcase\",\"isabs\",\"join\",\"splitdrive\",\"split\",\"splitext\",\\n \"basename\",\"dirname\",\"commonprefix\",\"getsize\",\"getmtime\",\\n \"getatime\",\"getctime\", \"islink\",\"exists\",\"lexists\",\"isdir\",\"isfile\",\\n \"ismount\", \"expanduser\",\"expandvars\",\"normpath\",\"abspath\",\\n \"splitunc\",\"curdir\",\"pardir\",\"sep\",\"pathsep\",\"defpath\",\"altsep\",\\n \"extsep\",\"devnull\",\"realpath\",\"supports_unicode_filenames\",\"relpath\",\\n \"samefile\", \"sameopenfile\", \"samestat\", \"commonpath\"]\\n # strings representing various path-related bits and pieces\\n # these are primarily for export; internally, they are hardcoded.\\n curdir = \\'.\\'\\n pardir = \\'..\\'\\n extsep = \\'.\\'\\n sep = \\'\\\\\\\\\\'\\n pathsep = \\';\\'\\n altsep = \\'/\\'\\n defpath = \\'.;c:\\\\\\\\bin\\'\\n if \\'ce\\' in sys.builtin_module_names:\\n defpath = \\'\\\\\\\\windows\\'\\n devnull = \\'nul\\'\\n def _get_bothseps(path):\\n if isinstance(path, bytes):\\n return b\\'\\\\\\\\/\\'\\n else:\\n return \\'\\\\\\\\/\\'\\n # normalize the case of a pathname and map slashes to backslashes.\\n # other normalizations (such as optimizing \\'../\\' away) are not done\\n # (this is done by normpath).\\n def normcase(s):\\n \"\"\"normalize case of pathname.\\n makes all characters lowercase and all slashes into backslashes.\"\"\"\\n try:\\n if isinstance(s, bytes):\\n return s.replace(b\\'/\\', b\\'\\\\\\\\\\').lower()\\n else:\\n return s.replace(\\'/\\', \\'\\\\\\\\\\').lower()\\n except (typeerror, attributeerror):\\n if not isinstance(s, (bytes, str)):\\n raise typeerror(\"normcase() argument must be str or bytes, \"\\n \"not %r\" % s.__class__.__name__) from none\\n raise\\n # return whether a path is absolute.\\n # trivial in posix, harder on windows.\\n # for windows it is absolute if it starts with a slash or backslash (current\\n # volume), or if a pathname after the volume-letter-and-colon or unc-resource\\n # starts with a slash or backslash.\\n def isabs(s):\\n \"\"\"test whether a path is absolute\"\"\"\\n s = splitdrive(s)[1]\\n return len(s) > 0 and s[0] in _get_bothseps(s)\\n # join two (or more) paths.\\n def join(path, *paths):\\n if isinstance(path, bytes):\\n sep = b\\'\\\\\\\\\\'\\n seps = b\\'\\\\\\\\/\\'\\n colon = b\\':\\'\\n else:\\n sep = \\'\\\\\\\\\\'\\n seps = \\'\\\\\\\\/\\'\\n colon = \\':\\'\\n try:\\n if not paths:\\n path[:0] + sep  #23780: ensure compatible data type even if p is null.\\n result_drive, result_path = splitdrive(path)\\n for p in paths:\\n p_drive, p_path = splitdrive(p)\\n if p_path and p_path[0] in seps:\\n # second path is absolute\\n if p_drive or not result_drive:\\n result_drive = p_drive\\n result_path = p_path\\n continue\\n elif p_drive and p_drive != result_drive:\\n if p_drive.lower() != result_drive.lower():\\n # different drives => ignore the first path entirely\\n result_drive = p_drive\\n result_path = p_path\\n continue\\n # same drive in different case\\n result_drive = p_drive\\n # second path is relative to the first\\n if result_path and result_path[-1] not in seps:\\n result_path = result_path + sep\\n result_path = result_path + p_path\\n ## add separator between unc and non-absolute path\\n if (result_path and result_path[0] not in seps and\\n result_drive and result_drive[-1:] != colon):\\n return result_drive + sep + result_path\\n return result_drive + result_path\\n except (typeerror, attributeerror, byteswarning):\\n genericpath._check_arg_types(\\'join\\', path, *paths)\\n raise\\n # split a path in a drive specification (a drive letter followed by a\\n # colon) and the path specification.\\n # it is always true that drivespec + pathspec == p\\n def splitdrive(p):\\n \"\"\"split a pathname into drive/unc sharepoint and relative path specifiers.\\n returns a 2-tuple (drive_or_unc, path); either part may be empty.\\n if you assign\\n result = splitdrive(p)\\n it is always true that:\\n result[0] + result[1] == p\\n if the path contained a drive letter, drive_or_unc will contain everything\\n up to and including the colon.  e.g. splitdrive(\"c:/dir\") returns (\"c:\", \"/dir\")\\n if the path contained a unc path, the drive_or_unc will contain the host name\\n and share up to but not including the fourth directory separator character.\\n e.g. splitdrive(\"//host/computer/dir\") returns (\"//host/computer\", \"/dir\")\\n paths cannot contain both a drive letter and a unc path.\\n \"\"\"\\n if len(p) >= 2:\\n if isinstance(p, bytes):\\n sep = b\\'\\\\\\\\\\'\\n altsep = b\\'/\\'\\n colon = b\\':\\'\\n else:\\n sep = \\'\\\\\\\\\\'\\n altsep = \\'/\\'\\n colon = \\':\\'\\n normp = p.replace(altsep, sep)\\n if (normp[0:2] == sep*2) and (normp[2:3] != sep):\\n # is a unc path:\\n # vvvvvvvvvvvvvvvvvvvv drive letter or unc path\\n # \\\\\\\\machine\\\\mountpoint\\\\directory\\\\etc\\\\...\\n #           directory ^^^^^^^^^^^^^^^\\n index = normp.find(sep, 2)\\n if index == -1:\\n return p[:0], p\\n index2 = normp.find(sep, index + 1)\\n # a unc path can\\'t have two slashes in a row\\n # (after the initial two)\\n if index2 == index + 1:\\n return p[:0], p\\n if index2 == -1:\\n index2 = len(p)\\n return p[:index2], p[index2:]\\n if normp[1:2] == colon:\\n return p[:2], p[2:]\\n return p[:0], p\\n # parse unc paths\\n def splitunc(p):\\n \"\"\"deprecated since python 3.1.  please use splitdrive() instead;\\n it now handles unc paths.\\n split a pathname into unc mount point and relative path specifiers.\\n return a 2-tuple (unc, rest); either part may be empty.\\n if unc is not empty, it has the form \\'//host/mount\\' (or similar\\n using backslashes).  unc+rest is always the input path.\\n paths containing drive letters never have a unc part.\\n \"\"\"\\n import warnings\\n warnings.warn(\"ntpath.splitunc is deprecated, use ntpath.splitdrive instead\",\\n deprecationwarning, 2)\\n drive, path = splitdrive(p)\\n if len(drive) == 2:\\n # drive letter present\\n return p[:0], p\\n return drive, path\\n # split a path in head (everything up to the last \\'/\\') and tail (the\\n # rest).  after the trailing \\'/\\' is stripped, the invariant\\n # join(head, tail) == p holds.\\n # the resulting head won\\'t end in \\'/\\' unless it is the root.\\n def split(p):\\n \"\"\"split a pathname.\\n return tuple (head, tail) where tail is everything after the final slash.\\n either part may be empty.\"\"\"\\n seps = _get_bothseps(p)\\n d, p = splitdrive(p)\\n # set i to index beyond p\\'s last slash\\n i = len(p)\\n while i and p[i-1] not in seps:\\n i -= 1\\n head, tail = p[:i], p[i:]  # now tail has no slashes\\n # remove trailing slashes from head, unless it\\'s all slashes\\n head = head.rstrip(seps) or head\\n return d + head, tail\\n # split a path in root and extension.\\n # the extension is everything starting at the last dot in the last\\n # pathname component; the root is everything before that.\\n # it is always true that root + ext == p.\\n def splitext(p):\\n if isinstance(p, bytes):\\n return genericpath._splitext(p, b\\'\\\\\\\\\\', b\\'/\\', b\\'.\\')\\n else:\\n return genericpath._splitext(p, \\'\\\\\\\\\\', \\'/\\', \\'.\\')\\n splitext.__doc__ = genericpath._splitext.__doc__\\n # return the tail (basename) part of a path.\\n def basename(p):\\n \"\"\"returns the final component of a pathname\"\"\"\\n return split(p)[1]\\n # return the head (dirname) part of a path.\\n def dirname(p):\\n \"\"\"returns the directory component of a pathname\"\"\"\\n return split(p)[0]\\n # is a path a symbolic link?\\n # this will always return false on systems where os.lstat doesn\\'t exist.\\n def islink(path):\\n \"\"\"test whether a path is a symbolic link.\\n this will always return false for windows prior to 6.0.\\n \"\"\"\\n try:\\n st = os.lstat(path)\\n except (oserror, attributeerror):\\n return false\\n return stat.s_islnk(st.st_mode)\\n # being true for dangling symbolic links is also useful.\\n def lexists(path):\\n \"\"\"test whether a path exists.  returns true for broken symbolic links\"\"\"\\n try:\\n st = os.lstat(path)\\n except oserror:\\n return false\\n return true\\n # is a path a mount point?\\n # any drive letter root (eg c:\\\\)\\n # any share unc (eg \\\\\\\\server\\\\share)\\n # any volume mounted on a filesystem folder\\n #\\n # no one method detects all three situations. historically we\\'ve lexically\\n # detected drive letter roots and share uncs. the canonical approach to\\n # detecting mounted volumes (querying the reparse tag) fails for the most\\n # common case: drive letter roots. the alternative which uses getvolumepathname\\n # fails if the drive letter is the result of a subst.\\n try:\\n from nt import _getvolumepathname\\n except importerror:\\n _getvolumepathname = none\\n def ismount(path):\\n \"\"\"test whether a path is a mount point (a drive root, the root of a\\n share, or a mounted volume)\"\"\"\\n seps = _get_bothseps(path)\\n path = abspath(path)\\n root, rest = splitdrive(path)\\n if root and root[0] in seps:\\n return (not rest) or (rest in seps)\\n if rest in seps:\\n return true\\n if _getvolumepathname:\\n return path.rstrip(seps) == _getvolumepathname(path).rstrip(seps)\\n else:\\n return false\\n # expand paths beginning with \\'~\\' or \\'~user\\'.\\n # \\'~\\' means $home; \\'~user\\' means that user\\'s home directory.\\n # if the path doesn\\'t begin with \\'~\\', or if the user or $home is unknown,\\n # the path is returned unchanged (leaving error reporting to whatever\\n # function is called with the expanded path as argument).\\n # see also module \\'glob\\' for expansion of *, ? and [...] in pathnames.\\n # (a function should also be defined to do full *sh-style environment\\n # variable expansion.)\\n def expanduser(path):\\n \"\"\"expand ~ and ~user constructs.\\n if user or $home is unknown, do nothing.\"\"\"\\n if isinstance(path, bytes):\\n tilde = b\\'~\\'\\n else:\\n tilde = \\'~\\'\\n if not path.startswith(tilde):\\n return path\\n i, n = 1, len(path)\\n while i < n and path[i] not in _get_bothseps(path):\\n i += 1\\n if \\'home\\' in os.environ:\\n userhome = os.environ[\\'home\\']\\n elif \\'userprofile\\' in os.environ:\\n userhome = os.environ[\\'userprofile\\']\\n elif not \\'homepath\\' in os.environ:\\n return path\\n else:\\n try:\\n drive = os.environ[\\'homedrive\\']\\n except keyerror:\\n drive = \\'\\'\\n userhome = join(drive, os.environ[\\'homepath\\'])\\n if isinstance(path, bytes):\\n userhome = os.fsencode(userhome)\\n if i != 1: #~user\\n userhome = join(dirname(userhome), path[1:i])\\n return userhome + path[i:]\\n # expand paths containing shell variable substitutions.\\n # the following rules apply:\\n #       - no expansion within single quotes\\n #       - \\'$$\\' is translated into \\'$\\'\\n #       - \\'%%\\' is translated into \\'%\\' if \\'%%\\' are not seen in %var1%%var2%\\n #       - ${varname} is accepted.\\n #       - $varname is accepted.\\n #       - %varname% is accepted.\\n #       - varnames can be made out of letters, digits and the characters \\'_-\\'\\n #         (though is not verified in the ${varname} and %varname% cases)\\n # xxx with command.com you can use any characters in a variable name,\\n # xxx except \\'^|<>=\\'.\\n def expandvars(path):\\n \"\"\"expand shell variables of the forms $var, ${var} and %var%.\\n unknown variables are left unchanged.\"\"\"\\n if isinstance(path, bytes):\\n if b\\'$\\' not in path and b\\'%\\' not in path:\\n return path\\n import string\\n varchars = bytes(string.ascii_letters + string.digits + \\'_-\\', \\'ascii\\')\\n quote = b\\'\\\\\\'\\'\\n percent = b\\'%\\'\\n brace = b\\'{\\'\\n rbrace = b\\'}\\'\\n dollar = b\\'$\\'\\n environ = getattr(os, \\'environb\\', none)\\n else:\\n if \\'$\\' not in path and \\'%\\' not in path:\\n return path\\n import string\\n varchars = string.ascii_letters + string.digits + \\'_-\\'\\n quote = \\'\\\\\\'\\'\\n percent = \\'%\\'\\n brace = \\'{\\'\\n rbrace = \\'}\\'\\n dollar = \\'$\\'\\n environ = os.environ\\n res = path[:0]\\n index = 0\\n pathlen = len(path)\\n while index < pathlen:\\n c = path[index:index+1]\\n if c == quote:   # no expansion within single quotes\\n path = path[index + 1:]\\n pathlen = len(path)\\n try:\\n index = path.index(c)\\n res += c + path[:index + 1]\\n except valueerror:\\n res += c + path\\n index = pathlen - 1\\n elif c == percent:  # variable or \\'%\\'\\n if path[index + 1:index + 2] == percent:\\n res += c\\n index += 1\\n else:\\n path = path[index+1:]\\n pathlen = len(path)\\n try:\\n index = path.index(percent)\\n except valueerror:\\n res += percent + path\\n index = pathlen - 1\\n else:\\n var = path[:index]\\n try:\\n if environ is none:\\n value = os.fsencode(os.environ[os.fsdecode(var)])\\n else:\\n value = environ[var]\\n except keyerror:\\n value = percent + var + percent\\n res += value\\n elif c == dollar:  # variable or \\'$$\\'\\n if path[index + 1:index + 2] == dollar:\\n res += c\\n index += 1\\n elif path[index + 1:index + 2] == brace:\\n path = path[index+2:]\\n pathlen = len(path)\\n try:\\n index = path.index(rbrace)\\n except valueerror:\\n res += dollar + brace + path\\n index = pathlen - 1\\n else:\\n var = path[:index]\\n try:\\n if environ is none:\\n value = os.fsencode(os.environ[os.fsdecode(var)])\\n else:\\n value = environ[var]\\n except keyerror:\\n value = dollar + brace + var + rbrace\\n res += value\\n else:\\n var = path[:0]\\n index += 1\\n c = path[index:index + 1]\\n while c and c in varchars:\\n var += c\\n index += 1\\n c = path[index:index + 1]\\n try:\\n if environ is none:\\n value = os.fsencode(os.environ[os.fsdecode(var)])\\n else:\\n value = environ[var]\\n except keyerror:\\n value = dollar + var\\n res += value\\n if c:\\n index -= 1\\n else:\\n res += c\\n index += 1\\n return res\\n # normalize a path, e.g. a//b, a/./b and a/foo/../b all become a\\\\b.\\n # previously, this function also truncated pathnames to 8+3 format,\\n # but as this module is called \"ntpath\", that\\'s obviously wrong!\\n def normpath(path):\\n \"\"\"normalize path, eliminating double slashes, etc.\"\"\"\\n if isinstance(path, bytes):\\n sep = b\\'\\\\\\\\\\'\\n altsep = b\\'/\\'\\n curdir = b\\'.\\'\\n pardir = b\\'..\\'\\n special_prefixes = (b\\'\\\\\\\\\\\\\\\\.\\\\\\\\\\', b\\'\\\\\\\\\\\\\\\\?\\\\\\\\\\')\\n else:\\n sep = \\'\\\\\\\\\\'\\n altsep = \\'/\\'\\n curdir = \\'.\\'\\n pardir = \\'..\\'\\n special_prefixes = (\\'\\\\\\\\\\\\\\\\.\\\\\\\\\\', \\'\\\\\\\\\\\\\\\\?\\\\\\\\\\')\\n if path.startswith(special_prefixes):\\n # in the case of paths with these prefixes:\\n # \\\\\\\\.\\\\ -> device names\\n # \\\\\\\\?\\\\ -> literal paths\\n # do not do any normalization, but return the path unchanged\\n return path\\n path = path.replace(altsep, sep)\\n prefix, path = splitdrive(path)\\n # collapse initial backslashes\\n if path.startswith(sep):\\n prefix += sep\\n path = path.lstrip(sep)\\n comps = path.split(sep)\\n i = 0\\n while i < len(comps):\\n if not comps[i] or comps[i] == curdir:\\n del comps[i]\\n elif comps[i] == pardir:\\n if i > 0 and comps[i-1] != pardir:\\n del comps[i-1:i+1]\\n i -= 1\\n elif i == 0 and prefix.endswith(sep):\\n del comps[i]\\n else:\\n i += 1\\n else:\\n i += 1\\n # if the path is now empty, substitute \\'.\\'\\n if not prefix and not comps:\\n comps.append(curdir)\\n return prefix + sep.join(comps)\\n # return an absolute path.\\n try:\\n from nt import _getfullpathname\\n except importerror: # not running on windows - mock up something sensible\\n def abspath(path):\\n \"\"\"return the absolute version of a path.\"\"\"\\n if not isabs(path):\\n if isinstance(path, bytes):\\n cwd = os.getcwdb()\\n else:\\n cwd = os.getcwd()\\n path = join(cwd, path)\\n return normpath(path)\\n else:  # use native windows method on windows\\n def abspath(path):\\n \"\"\"return the absolute version of a path.\"\"\"\\n if path: # empty path must return current working directory.\\n try:\\n path = _getfullpathname(path)\\n except oserror:\\n pass # bad path - return unchanged.\\n elif isinstance(path, bytes):\\n path = os.getcwdb()\\n else:\\n path = os.getcwd()\\n return normpath(path)\\n # realpath is a no-op on systems without islink support\\n realpath = abspath\\n # win9x family and earlier have no unicode filename support.\\n supports_unicode_filenames = (hasattr(sys, \"getwindowsversion\") and\\n sys.getwindowsversion()[3] >= 2)\\n def relpath(path, start=none):\\n \"\"\"return a relative version of a path\"\"\"\\n if isinstance(path, bytes):\\n sep = b\\'\\\\\\\\\\'\\n curdir = b\\'.\\'\\n pardir = b\\'..\\'\\n else:\\n sep = \\'\\\\\\\\\\'\\n curdir = \\'.\\'\\n pardir = \\'..\\'\\n if start is none:\\n start = curdir\\n if not path:\\n raise valueerror(\"no path specified\")\\n try:\\n start_abs = abspath(normpath(start))\\n path_abs = abspath(normpath(path))\\n start_drive, start_rest = splitdrive(start_abs)\\n path_drive, path_rest = splitdrive(path_abs)\\n if normcase(start_drive) != normcase(path_drive):\\n raise valueerror(\"path is on mount %r, start on mount %r\" % (\\n path_drive, start_drive))\\n start_list = [x for x in start_rest.split(sep) if x]\\n path_list = [x for x in path_rest.split(sep) if x]\\n # work out how much of the filepath is shared by start and path.\\n i = 0\\n for e1, e2 in zip(start_list, path_list):\\n if normcase(e1) != normcase(e2):\\n break\\n i += 1\\n rel_list = [pardir] * (len(start_list)-i) + path_list[i:]\\n if not rel_list:\\n return curdir\\n return join(*rel_list)\\n except (typeerror, valueerror, attributeerror, byteswarning, deprecationwarning):\\n genericpath._check_arg_types(\\'relpath\\', path, start)\\n raise\\n # return the longest common sub-path of the sequence of paths given as input.\\n # the function is case-insensitive and \\'separator-insensitive\\', i.e. if the\\n # only difference between two paths is the use of \\'\\\\\\' versus \\'/\\' as separator,\\n # they are deemed to be equal.\\n #\\n # however, the returned path will have the standard \\'\\\\\\' separator (even if the\\n # given paths had the alternative \\'/\\' separator) and will have the case of the\\n # first path given in the sequence. additionally, any trailing separator is\\n # stripped from the returned path.\\n def commonpath(paths):\\n \"\"\"given a sequence of path names, returns the longest common sub-path.\"\"\"\\n if not paths:\\n raise valueerror(\\'commonpath() arg is an empty sequence\\')\\n if isinstance(paths[0], bytes):\\n sep = b\\'\\\\\\\\\\'\\n altsep = b\\'/\\'\\n curdir = b\\'.\\'\\n else:\\n sep = \\'\\\\\\\\\\'\\n altsep = \\'/\\'\\n curdir = \\'.\\'\\n try:\\n drivesplits = [splitdrive(p.replace(altsep, sep).lower()) for p in paths]\\n split_paths = [p.split(sep) for d, p in drivesplits]\\n try:\\n isabs, = set(p[:1] == sep for d, p in drivesplits)\\n except valueerror:\\n raise valueerror(\"can\\'t mix absolute and relative paths\") from none\\n # check that all drive letters or unc paths match. the check is made only\\n # now otherwise type errors for mixing strings and bytes would not be\\n # caught.\\n if len(set(d for d, p in drivesplits)) != 1:\\n raise valueerror(\"paths don\\'t have the same drive\")\\n drive, path = splitdrive(paths[0].replace(altsep, sep))\\n common = path.split(sep)\\n common = [c for c in common if c and c != curdir]\\n split_paths = [[c for c in s if c and c != curdir] for s in split_paths]\\n s1 = min(split_paths)\\n s2 = max(split_paths)\\n for i, c in enumerate(s1):\\n if c != s2[i]:\\n common = common[:i]\\n break\\n else:\\n common = common[:len(s1)]\\n prefix = drive + sep if isabs else drive\\n return prefix + sep.join(common)\\n except (typeerror, attributeerror):\\n genericpath._check_arg_types(\\'commonpath\\', *paths)\\n raise\\n # determine if two files are in fact the same file\\n try:\\n # getfinalpathnamebyhandle is available starting with windows 6.0.\\n # windows xp and non-windows os\\'es will mock _getfinalpathname.\\n if sys.getwindowsversion()[:2] >= (6, 0):\\n from nt import _getfinalpathname\\n else:\\n raise importerror\\n except (attributeerror, importerror):\\n # on windows xp and earlier, two files are the same if their absolute\\n # pathnames are the same.\\n # non-windows operating systems fake this method with an xp\\n # approximation.\\n def _getfinalpathname(f):\\n return normcase(abspath(f))\\n try:\\n # the genericpath.isdir implementation uses os.stat and checks the mode\\n # attribute to tell whether or not the path is a directory.\\n # this is overkill on windows - just pass the path to getfileattributes\\n # and check the attribute from there.\\n from nt import _isdir as isdir\\n except importerror:\\n # use genericpath.isdir as imported above.\\n pass\\n \"\"\"\\n operator interface\\n this module exports a set of functions corresponding to the intrinsic\\n operators of python.  for example, operator.add(x, y) is equivalent\\n to the expression x+y.  the function names are those used for special\\n methods; variants without leading and trailing \\'__\\' are also provided\\n for convenience.\\n this is the pure python implementation of the module.\\n \"\"\"\\n __all__ = [\\'abs\\', \\'add\\', \\'and_\\', \\'attrgetter\\', \\'concat\\', \\'contains\\', \\'countof\\',\\n \\'delitem\\', \\'eq\\', \\'floordiv\\', \\'ge\\', \\'getitem\\', \\'gt\\', \\'iadd\\', \\'iand\\',\\n \\'iconcat\\', \\'ifloordiv\\', \\'ilshift\\', \\'imatmul\\', \\'imod\\', \\'imul\\',\\n \\'index\\', \\'indexof\\', \\'inv\\', \\'invert\\', \\'ior\\', \\'ipow\\', \\'irshift\\',\\n \\'is_\\', \\'is_not\\', \\'isub\\', \\'itemgetter\\', \\'itruediv\\', \\'ixor\\', \\'le\\',\\n \\'length_hint\\', \\'lshift\\', \\'lt\\', \\'matmul\\', \\'methodcaller\\', \\'mod\\',\\n \\'mul\\', \\'ne\\', \\'neg\\', \\'not_\\', \\'or_\\', \\'pos\\', \\'pow\\', \\'rshift\\',\\n \\'setitem\\', \\'sub\\', \\'truediv\\', \\'truth\\', \\'xor\\']\\n from builtins import abs as _abs\\n # comparison operations *******************************************************#\\n def lt(a, b):\\n \"same as a < b.\"\\n return a < b\\n def le(a, b):\\n \"same as a <= b.\"\\n return a <= b\\n def eq(a, b):\\n \"same as a == b.\"\\n return a == b\\n def ne(a, b):\\n \"same as a != b.\"\\n return a != b\\n def ge(a, b):\\n \"same as a >= b.\"\\n return a >= b\\n def gt(a, b):\\n \"same as a > b.\"\\n return a > b\\n # logical operations **********************************************************#\\n def not_(a):\\n \"same as not a.\"\\n return not a\\n def truth(a):\\n \"return true if a is true, false otherwise.\"\\n return true if a else false\\n def is_(a, b):\\n \"same as a is b.\"\\n return a is b\\n def is_not(a, b):\\n \"same as a is not b.\"\\n return a is not b\\n # mathematical/bitwise operations *********************************************#\\n def abs(a):\\n \"same as abs(a).\"\\n return _abs(a)\\n def add(a, b):\\n \"same as a + b.\"\\n return a + b\\n def and_(a, b):\\n \"same as a & b.\"\\n return a & b\\n def floordiv(a, b):\\n \"same as a // b.\"\\n return a // b\\n def index(a):\\n \"same as a.__index__().\"\\n return a.__index__()\\n def inv(a):\\n \"same as ~a.\"\\n return ~a\\n invert = inv\\n def lshift(a, b):\\n \"same as a << b.\"\\n return a << b\\n def mod(a, b):\\n \"same as a % b.\"\\n return a % b\\n def mul(a, b):\\n \"same as a * b.\"\\n return a * b\\n def matmul(a, b):\\n \"same as a @ b.\"\\n return a @ b\\n def neg(a):\\n \"same as -a.\"\\n return -a\\n def or_(a, b):\\n \"same as a | b.\"\\n return a | b\\n def pos(a):\\n \"same as +a.\"\\n return +a\\n def pow(a, b):\\n \"same as a ** b.\"\\n return a ** b\\n def rshift(a, b):\\n \"same as a >> b.\"\\n return a >> b\\n def sub(a, b):\\n \"same as a - b.\"\\n return a - b\\n def truediv(a, b):\\n \"same as a / b.\"\\n return a / b\\n def xor(a, b):\\n \"same as a ^ b.\"\\n return a ^ b\\n # sequence operations *********************************************************#\\n def concat(a, b):\\n \"same as a + b, for a and b sequences.\"\\n if not hasattr(a, \\'__getitem__\\'):\\n msg = \"\\'%s\\' object can\\'t be concatenated\" % type(a).__name__\\n raise typeerror(msg)\\n return a + b\\n def contains(a, b):\\n \"same as b in a (note reversed operands).\"\\n return b in a\\n def countof(a, b):\\n \"return the number of times b occurs in a.\"\\n count = 0\\n for i in a:\\n if i == b:\\n count += 1\\n return count\\n def delitem(a, b):\\n \"same as del a[b].\"\\n del a[b]\\n def getitem(a, b):\\n \"same as a[b].\"\\n return a[b]\\n def indexof(a, b):\\n \"return the first index of b in a.\"\\n for i, j in enumerate(a):\\n if j == b:\\n return i\\n else:\\n raise valueerror(\\'sequence.index(x): x not in sequence\\')\\n def setitem(a, b, c):\\n \"same as a[b] = c.\"\\n a[b] = c\\n def length_hint(obj, default=0):\\n \"\"\"\\n return an estimate of the number of items in obj.\\n this is useful for presizing containers when building from an iterable.\\n if the object supports len(), the result will be exact. otherwise, it may\\n over- or under-estimate by an arbitrary amount. the result will be an\\n integer >= 0.\\n \"\"\"\\n if not isinstance(default, int):\\n msg = (\"\\'%s\\' object cannot be interpreted as an integer\" %\\n type(default).__name__)\\n raise typeerror(msg)\\n try:\\n return len(obj)\\n except typeerror:\\n pass\\n try:\\n hint = type(obj).__length_hint__\\n except attributeerror:\\n return default\\n try:\\n val = hint(obj)\\n except typeerror:\\n return default\\n if val is notimplemented:\\n return default\\n if not isinstance(val, int):\\n msg = (\\'__length_hint__ must be integer, not %s\\' %\\n type(val).__name__)\\n raise typeerror(msg)\\n if val < 0:\\n msg = \\'__length_hint__() should return >= 0\\'\\n raise valueerror(msg)\\n return val\\n # generalized lookup objects **************************************************#\\n class attrgetter:\\n \"\"\"\\n return a callable object that fetches the given attribute(s) from its operand.\\n after f = attrgetter(\\'name\\'), the call f(r) returns r.name.\\n after g = attrgetter(\\'name\\', \\'date\\'), the call g(r) returns (r.name, r.date).\\n after h = attrgetter(\\'name.first\\', \\'name.last\\'), the call h(r) returns\\n (r.name.first, r.name.last).\\n \"\"\"\\n __slots__ = (\\'_attrs\\', \\'_call\\')\\n def __init__(self, attr, *attrs):\\n if not attrs:\\n if not isinstance(attr, str):\\n raise typeerror(\\'attribute name must be a string\\')\\n self._attrs = (attr,)\\n names = attr.split(\\'.\\')\\n def func(obj):\\n for name in names:\\n obj = getattr(obj, name)\\n return obj\\n self._call = func\\n else:\\n self._attrs = (attr,) + attrs\\n getters = tuple(map(attrgetter, self._attrs))\\n def func(obj):\\n return tuple(getter(obj) for getter in getters)\\n self._call = func\\n def __call__(self, obj):\\n return self._call(obj)\\n def __repr__(self):\\n return \\'%s.%s(%s)\\' % (self.__class__.__module__,\\n self.__class__.__qualname__,\\n \\', \\'.join(map(repr, self._attrs)))\\n def __reduce__(self):\\n return self.__class__, self._attrs\\n class itemgetter:\\n \"\"\"\\n return a callable object that fetches the given item(s) from its operand.\\n after f = itemgetter(2), the call f(r) returns r[2].\\n after g = itemgetter(2, 5, 3), the call g(r) returns (r[2], r[5], r[3])\\n \"\"\"\\n __slots__ = (\\'_items\\', \\'_call\\')\\n def __init__(self, item, *items):\\n if not items:\\n self._items = (item,)\\n def func(obj):\\n return obj[item]\\n self._call = func\\n else:\\n self._items = items = (item,) + items\\n def func(obj):\\n return tuple(obj[i] for i in items)\\n self._call = func\\n def __call__(self, obj):\\n return self._call(obj)\\n def __repr__(self):\\n return \\'%s.%s(%s)\\' % (self.__class__.__module__,\\n self.__class__.__name__,\\n \\', \\'.join(map(repr, self._items)))\\n def __reduce__(self):\\n return self.__class__, self._items\\n class methodcaller:\\n \"\"\"\\n return a callable object that calls the given method on its operand.\\n after f = methodcaller(\\'name\\'), the call f(r) returns r.name().\\n after g = methodcaller(\\'name\\', \\'date\\', foo=1), the call g(r) returns\\n r.name(\\'date\\', foo=1).\\n \"\"\"\\n __slots__ = (\\'_name\\', \\'_args\\', \\'_kwargs\\')\\n def __init__(*args, **kwargs):\\n if len(args) < 2:\\n msg = \"methodcaller needs at least one argument, the method name\"\\n raise typeerror(msg)\\n self = args[0]\\n self._name = args[1]\\n if not isinstance(self._name, str):\\n raise typeerror(\\'method name must be a string\\')\\n self._args = args[2:]\\n self._kwargs = kwargs\\n def __call__(self, obj):\\n return getattr(obj, self._name)(*self._args, **self._kwargs)\\n def __repr__(self):\\n args = [repr(self._name)]\\n args.extend(map(repr, self._args))\\n args.extend(\\'%s=%r\\' % (k, v) for k, v in self._kwargs.items())\\n return \\'%s.%s(%s)\\' % (self.__class__.__module__,\\n self.__class__.__name__,\\n \\', \\'.join(args))\\n def __reduce__(self):\\n if not self._kwargs:\\n return self.__class__, (self._name,) + self._args\\n else:\\n from functools import partial\\n return partial(self.__class__, self._name, **self._kwargs), self._args\\n # in-place operations *********************************************************#\\n def iadd(a, b):\\n \"same as a += b.\"\\n a += b\\n return a\\n def iand(a, b):\\n \"same as a &= b.\"\\n a &= b\\n return a\\n def iconcat(a, b):\\n \"same as a += b, for a and b sequences.\"\\n if not hasattr(a, \\'__getitem__\\'):\\n msg = \"\\'%s\\' object can\\'t be concatenated\" % type(a).__name__\\n raise typeerror(msg)\\n a += b\\n return a\\n def ifloordiv(a, b):\\n \"same as a //= b.\"\\n a //= b\\n return a\\n def ilshift(a, b):\\n \"same as a <<= b.\"\\n a <<= b\\n return a\\n def imod(a, b):\\n \"same as a %= b.\"\\n a %= b\\n return a\\n def imul(a, b):\\n \"same as a *= b.\"\\n a *= b\\n return a\\n def imatmul(a, b):\\n \"same as a @= b.\"\\n a @= b\\n return a\\n def ior(a, b):\\n \"same as a |= b.\"\\n a |= b\\n return a\\n def ipow(a, b):\\n \"same as a **= b.\"\\n a **=b\\n return a\\n def irshift(a, b):\\n \"same as a >>= b.\"\\n a >>= b\\n return a\\n def isub(a, b):\\n \"same as a -= b.\"\\n a -= b\\n return a\\n def itruediv(a, b):\\n \"same as a /= b.\"\\n a /= b\\n return a\\n def ixor(a, b):\\n \"same as a ^= b.\"\\n a ^= b\\n return a\\n try:\\n from _operator import *\\n except importerror:\\n pass\\n else:\\n from _operator import __doc__\\n # all of these \"__func__ = func\" assignments have to happen after importing\\n # from _operator to make sure they\\'re set to the right function\\n __lt__ = lt\\n __le__ = le\\n __eq__ = eq\\n __ne__ = ne\\n __ge__ = ge\\n __gt__ = gt\\n __not__ = not_\\n __abs__ = abs\\n __add__ = add\\n __and__ = and_\\n __floordiv__ = floordiv\\n __index__ = index\\n __inv__ = inv\\n __invert__ = invert\\n __lshift__ = lshift\\n __mod__ = mod\\n __mul__ = mul\\n __matmul__ = matmul\\n __neg__ = neg\\n __or__ = or_\\n __pos__ = pos\\n __pow__ = pow\\n __rshift__ = rshift\\n __sub__ = sub\\n __truediv__ = truediv\\n __xor__ = xor\\n __concat__ = concat\\n __contains__ = contains\\n __delitem__ = delitem\\n __getitem__ = getitem\\n __setitem__ = setitem\\n __iadd__ = iadd\\n __iand__ = iand\\n __iconcat__ = iconcat\\n __ifloordiv__ = ifloordiv\\n __ilshift__ = ilshift\\n __imod__ = imod\\n __imul__ = imul\\n __imatmul__ = imatmul\\n __ior__ = ior\\n __ipow__ = ipow\\n __irshift__ = irshift\\n __isub__ = isub\\n __itruediv__ = itruediv\\n __ixor__ = ixor\\n r\"\"\"os routines for nt or posix depending on what system we\\'re on.\\n this exports:\\n - all functions from posix, nt or ce, e.g. unlink, stat, etc.\\n - os.path is either posixpath or ntpath\\n - os.name is either \\'posix\\', \\'nt\\' or \\'ce\\'.\\n - os.curdir is a string representing the current directory (\\'.\\' or \\':\\')\\n - os.pardir is a string representing the parent directory (\\'..\\' or \\'::\\')\\n - os.sep is the (or a most common) pathname separator (\\'/\\' or \\':\\' or \\'\\\\\\\\\\')\\n - os.extsep is the extension separator (always \\'.\\')\\n - os.altsep is the alternate pathname separator (none or \\'/\\')\\n - os.pathsep is the component separator used in $path etc\\n - os.linesep is the line separator in text files (\\'\\\\r\\' or \\'\\\\n\\' or \\'\\\\r\\\\n\\')\\n - os.defpath is the default search path for executables\\n - os.devnull is the file path of the null device (\\'/dev/null\\', etc.)\\n programs that import and use \\'os\\' stand a better chance of being\\n portable between different platforms.  of course, they must then\\n only use functions that are defined by all platforms (e.g., unlink\\n and opendir), and leave all pathname manipulation to os.path\\n (e.g., split and join).\\n \"\"\"\\n #\\'\\n import sys, errno\\n import stat as st\\n _names = sys.builtin_module_names\\n # note:  more names are added to __all__ later.\\n __all__ = [\"altsep\", \"curdir\", \"pardir\", \"sep\", \"pathsep\", \"linesep\",\\n \"defpath\", \"name\", \"path\", \"devnull\", \"seek_set\", \"seek_cur\",\\n \"seek_end\", \"fsencode\", \"fsdecode\", \"get_exec_path\", \"fdopen\",\\n \"popen\", \"extsep\"]\\n def _exists(name):\\n return name in globals()\\n def _get_exports_list(module):\\n try:\\n return list(module.__all__)\\n except attributeerror:\\n return [n for n in dir(module) if n[0] != \\'_\\']\\n # any new dependencies of the os module and/or changes in path separator\\n # requires updating importlib as well.\\n if \\'posix\\' in _names:\\n name = \\'posix\\'\\n linesep = \\'\\\\n\\'\\n from posix import *\\n try:\\n from posix import _exit\\n __all__.append(\\'_exit\\')\\n except importerror:\\n pass\\n import posixpath as path\\n try:\\n from posix import _have_functions\\n except importerror:\\n pass\\n import posix\\n __all__.extend(_get_exports_list(posix))\\n del posix\\n elif \\'nt\\' in _names:\\n name = \\'nt\\'\\n linesep = \\'\\\\r\\\\n\\'\\n from nt import *\\n try:\\n from nt import _exit\\n __all__.append(\\'_exit\\')\\n except importerror:\\n pass\\n import ntpath as path\\n import nt\\n __all__.extend(_get_exports_list(nt))\\n del nt\\n try:\\n from nt import _have_functions\\n except importerror:\\n pass\\n elif \\'ce\\' in _names:\\n name = \\'ce\\'\\n linesep = \\'\\\\r\\\\n\\'\\n from ce import *\\n try:\\n from ce import _exit\\n __all__.append(\\'_exit\\')\\n except importerror:\\n pass\\n # we can use the standard windows path.\\n import ntpath as path\\n import ce\\n __all__.extend(_get_exports_list(ce))\\n del ce\\n try:\\n from ce import _have_functions\\n except importerror:\\n pass\\n else:\\n raise importerror(\\'no os specific module found\\')\\n sys.modules[\\'os.path\\'] = path\\n from os.path import (curdir, pardir, sep, pathsep, defpath, extsep, altsep,\\n devnull)\\n del _names\\n if _exists(\"_have_functions\"):\\n _globals = globals()\\n def _add(str, fn):\\n if (fn in _globals) and (str in _have_functions):\\n _set.add(_globals[fn])\\n _set = set()\\n _add(\"have_faccessat\",  \"access\")\\n _add(\"have_fchmodat\",   \"chmod\")\\n _add(\"have_fchownat\",   \"chown\")\\n _add(\"have_fstatat\",    \"stat\")\\n _add(\"have_futimesat\",  \"utime\")\\n _add(\"have_linkat\",     \"link\")\\n _add(\"have_mkdirat\",    \"mkdir\")\\n _add(\"have_mkfifoat\",   \"mkfifo\")\\n _add(\"have_mknodat\",    \"mknod\")\\n _add(\"have_openat\",     \"open\")\\n _add(\"have_readlinkat\", \"readlink\")\\n _add(\"have_renameat\",   \"rename\")\\n _add(\"have_symlinkat\",  \"symlink\")\\n _add(\"have_unlinkat\",   \"unlink\")\\n _add(\"have_unlinkat\",   \"rmdir\")\\n _add(\"have_utimensat\",  \"utime\")\\n supports_dir_fd = _set\\n _set = set()\\n _add(\"have_faccessat\",  \"access\")\\n supports_effective_ids = _set\\n _set = set()\\n _add(\"have_fchdir\",     \"chdir\")\\n _add(\"have_fchmod\",     \"chmod\")\\n _add(\"have_fchown\",     \"chown\")\\n _add(\"have_fdopendir\",  \"listdir\")\\n _add(\"have_fexecve\",    \"execve\")\\n _set.add(stat) # fstat always works\\n _add(\"have_ftruncate\",  \"truncate\")\\n _add(\"have_futimens\",   \"utime\")\\n _add(\"have_futimes\",    \"utime\")\\n _add(\"have_fpathconf\",  \"pathconf\")\\n if _exists(\"statvfs\") and _exists(\"fstatvfs\"): # mac os x10.3\\n _add(\"have_fstatvfs\", \"statvfs\")\\n supports_fd = _set\\n _set = set()\\n _add(\"have_faccessat\",  \"access\")\\n # some platforms don\\'t support lchmod().  often the function exists\\n # anyway, as a stub that always returns enosup or perhaps eopnotsupp.\\n # (no, i don\\'t know why that\\'s a good design.)  ./configure will detect\\n # this and reject it--so have_lchmod still won\\'t be defined on such\\n # platforms.  this is very helpful.\\n #\\n # however, sometimes platforms without a working lchmod() *do* have\\n # fchmodat().  (examples: linux kernel 3.2 with glibc 2.15,\\n # openindiana 3.x.)  and fchmodat() has a flag that theoretically makes\\n # it behave like lchmod().  so in theory it would be a suitable\\n # replacement for lchmod().  but when lchmod() doesn\\'t work, fchmodat()\\'s\\n # flag doesn\\'t work *either*.  sadly ./configure isn\\'t sophisticated\\n # enough to detect this condition--it only determines whether or not\\n # fchmodat() minimally works.\\n #\\n # therefore we simply ignore fchmodat() when deciding whether or not\\n # os.chmod supports follow_symlinks.  just checking lchmod() is\\n # sufficient.  after all--if you have a working fchmodat(), your\\n # lchmod() almost certainly works too.\\n #\\n # _add(\"have_fchmodat\",   \"chmod\")\\n _add(\"have_fchownat\",   \"chown\")\\n _add(\"have_fstatat\",    \"stat\")\\n _add(\"have_lchflags\",   \"chflags\")\\n _add(\"have_lchmod\",     \"chmod\")\\n if _exists(\"lchown\"): # mac os x10.3\\n _add(\"have_lchown\", \"chown\")\\n _add(\"have_linkat\",     \"link\")\\n _add(\"have_lutimes\",    \"utime\")\\n _add(\"have_lstat\",      \"stat\")\\n _add(\"have_fstatat\",    \"stat\")\\n _add(\"have_utimensat\",  \"utime\")\\n _add(\"ms_windows\",      \"stat\")\\n supports_follow_symlinks = _set\\n del _set\\n del _have_functions\\n del _globals\\n del _add\\n # python uses fixed values for the seek_ constants; they are mapped\\n # to native constants if necessary in posixmodule.c\\n # other possible seek values are directly imported from posixmodule.c\\n seek_set = 0\\n seek_cur = 1\\n seek_end = 2\\n # super directory utilities.\\n # (inspired by eric raymond; the doc strings are mostly his)\\n def makedirs(name, mode=0o777, exist_ok=false):\\n \"\"\"makedirs(name [, mode=0o777][, exist_ok=false])\\n super-mkdir; create a leaf directory and all intermediate ones.  works like\\n mkdir, except that any intermediate path segment (not just the rightmost)\\n will be created if it does not exist. if the target directory already\\n exists, raise an oserror if exist_ok is false. otherwise no exception is\\n raised.  this is recursive.\\n \"\"\"\\n head, tail = path.split(name)\\n if not tail:\\n head, tail = path.split(head)\\n if head and tail and not path.exists(head):\\n try:\\n makedirs(head, mode, exist_ok)\\n except fileexistserror:\\n # defeats race condition when another thread created the path\\n pass\\n cdir = curdir\\n if isinstance(tail, bytes):\\n cdir = bytes(curdir, \\'ascii\\')\\n if tail == cdir:           # xxx/newdir/. exists if xxx/newdir exists\\n return\\n try:\\n mkdir(name, mode)\\n except oserror:\\n # cannot rely on checking for eexist, since the operating system\\n # could give priority to other errors like eacces or erofs\\n if not exist_ok or not path.isdir(name):\\n raise\\n def removedirs(name):\\n \"\"\"removedirs(name)\\n super-rmdir; remove a leaf directory and all empty intermediate\\n ones.  works like rmdir except that, if the leaf directory is\\n successfully removed, directories corresponding to rightmost path\\n segments will be pruned away until either the whole path is\\n consumed or an error occurs.  errors during this latter phase are\\n ignored -- they generally mean that a directory was not empty.\\n \"\"\"\\n rmdir(name)\\n head, tail = path.split(name)\\n if not tail:\\n head, tail = path.split(head)\\n while head and tail:\\n try:\\n rmdir(head)\\n except oserror:\\n break\\n head, tail = path.split(head)\\n def renames(old, new):\\n \"\"\"renames(old, new)\\n super-rename; create directories as necessary and delete any left\\n empty.  works like rename, except creation of any intermediate\\n directories needed to make the new pathname good is attempted\\n first.  after the rename, directories corresponding to rightmost\\n path segments of the old name will be pruned until either the\\n whole path is consumed or a nonempty directory is found.\\n note: this function can fail with the new directory structure made\\n if you lack permissions needed to unlink the leaf directory or\\n file.\\n \"\"\"\\n head, tail = path.split(new)\\n if head and tail and not path.exists(head):\\n makedirs(head)\\n rename(old, new)\\n head, tail = path.split(old)\\n if head and tail:\\n try:\\n removedirs(head)\\n except oserror:\\n pass\\n __all__.extend([\"makedirs\", \"removedirs\", \"renames\"])\\n def walk(top, topdown=true, onerror=none, followlinks=false):\\n \"\"\"directory tree generator.\\n for each directory in the directory tree rooted at top (including top\\n itself, but excluding \\'.\\' and \\'..\\'), yields a 3-tuple\\n dirpath, dirnames, filenames\\n dirpath is a string, the path to the directory.  dirnames is a list of\\n the names of the subdirectories in dirpath (excluding \\'.\\' and \\'..\\').\\n filenames is a list of the names of the non-directory files in dirpath.\\n note that the names in the lists are just names, with no path components.\\n to get a full path (which begins with top) to a file or directory in\\n dirpath, do os.path.join(dirpath, name).\\n if optional arg \\'topdown\\' is true or not specified, the triple for a\\n directory is generated before the triples for any of its subdirectories\\n (directories are generated top down).  if topdown is false, the triple\\n for a directory is generated after the triples for all of its\\n subdirectories (directories are generated bottom up).\\n when topdown is true, the caller can modify the dirnames list in-place\\n (e.g., via del or slice assignment), and walk will only recurse into the\\n subdirectories whose names remain in dirnames; this can be used to prune the\\n search, or to impose a specific order of visiting.  modifying dirnames when\\n topdown is false is ineffective, since the directories in dirnames have\\n already been generated by the time dirnames itself is generated. no matter\\n the value of topdown, the list of subdirectories is retrieved before the\\n tuples for the directory and its subdirectories are generated.\\n by default errors from the os.scandir() call are ignored.  if\\n optional arg \\'onerror\\' is specified, it should be a function; it\\n will be called with one argument, an oserror instance.  it can\\n report the error to continue with the walk, or raise the exception\\n to abort the walk.  note that the filename is available as the\\n filename attribute of the exception object.\\n by default, os.walk does not follow symbolic links to subdirectories on\\n systems that support them.  in order to get this functionality, set the\\n optional argument \\'followlinks\\' to true.\\n caution:  if you pass a relative pathname for top, don\\'t change the\\n current working directory between resumptions of walk.  walk never\\n changes the current directory, and assumes that the client doesn\\'t\\n either.\\n example:\\n import os\\n from os.path import join, getsize\\n for root, dirs, files in os.walk(\\'python/lib/email\\'):\\n print(root, \"consumes\", end=\"\")\\n print(sum([getsize(join(root, name)) for name in files]), end=\"\")\\n print(\"bytes in\", len(files), \"non-directory files\")\\n if \\'cvs\\' in dirs:\\n dirs.remove(\\'cvs\\')  # don\\'t visit cvs directories\\n \"\"\"\\n dirs = []\\n nondirs = []\\n # we may not have read permission for top, in which case we can\\'t\\n # get a list of the files the directory contains.  os.walk\\n # always suppressed the exception then, rather than blow up for a\\n # minor reason when (say) a thousand readable directories are still\\n # left to visit.  that logic is copied here.\\n try:\\n if name == \\'nt\\' and isinstance(top, bytes):\\n scandir_it = _dummy_scandir(top)\\n else:\\n # note that scandir is global in this module due\\n # to earlier import-*.\\n scandir_it = scandir(top)\\n entries = list(scandir_it)\\n except oserror as error:\\n if onerror is not none:\\n onerror(error)\\n return\\n for entry in entries:\\n try:\\n is_dir = entry.is_dir()\\n except oserror:\\n # if is_dir() raises an oserror, consider that the entry is not\\n # a directory, same behaviour than os.path.isdir().\\n is_dir = false\\n if is_dir:\\n dirs.append(entry.name)\\n else:\\n nondirs.append(entry.name)\\n if not topdown and is_dir:\\n # bottom-up: recurse into sub-directory, but exclude symlinks to\\n # directories if followlinks is false\\n if followlinks:\\n walk_into = true\\n else:\\n try:\\n is_symlink = entry.is_symlink()\\n except oserror:\\n # if is_symlink() raises an oserror, consider that the\\n # entry is not a symbolic link, same behaviour than\\n # os.path.islink().\\n is_symlink = false\\n walk_into = not is_symlink\\n if walk_into:\\n yield from walk(entry.path, topdown, onerror, followlinks)\\n # yield before recursion if going top down\\n if topdown:\\n yield top, dirs, nondirs\\n # recurse into sub-directories\\n islink, join = path.islink, path.join\\n for dirname in dirs:\\n new_path = join(top, dirname)\\n # issue #23605: os.path.islink() is used instead of caching\\n # entry.is_symlink() result during the loop on os.scandir() because\\n # the caller can replace the directory entry during the \"yield\"\\n # above.\\n if followlinks or not islink(new_path):\\n yield from walk(new_path, topdown, onerror, followlinks)\\n else:\\n # yield after recursion if going bottom up\\n yield top, dirs, nondirs\\n class _dummydirentry:\\n \"\"\"dummy implementation of direntry\\n only used internally by os.walk(bytes). since os.walk() doesn\\'t need the\\n follow_symlinks parameter: don\\'t implement it, always follow symbolic\\n links.\\n \"\"\"\\n def __init__(self, dir, name):\\n self.name = name\\n self.path = path.join(dir, name)\\n # mimick findfirstfile/findnextfile: we should get file attributes\\n # while iterating on a directory\\n self._stat = none\\n self._lstat = none\\n try:\\n self.stat(follow_symlinks=false)\\n except oserror:\\n pass\\n def stat(self, *, follow_symlinks=true):\\n if follow_symlinks:\\n if self._stat is none:\\n self._stat = stat(self.path)\\n return self._stat\\n else:\\n if self._lstat is none:\\n self._lstat = stat(self.path, follow_symlinks=false)\\n return self._lstat\\n def is_dir(self):\\n if self._lstat is not none and not self.is_symlink():\\n # use the cache lstat\\n stat = self.stat(follow_symlinks=false)\\n return st.s_isdir(stat.st_mode)\\n stat = self.stat()\\n return st.s_isdir(stat.st_mode)\\n def is_symlink(self):\\n stat = self.stat(follow_symlinks=false)\\n return st.s_islnk(stat.st_mode)\\n def _dummy_scandir(dir):\\n # listdir-based implementation for bytes patches on windows\\n for name in listdir(dir):\\n yield _dummydirentry(dir, name)\\n __all__.append(\"walk\")\\n if {open, stat} <= supports_dir_fd and {listdir, stat} <= supports_fd:\\n def fwalk(top=\".\", topdown=true, onerror=none, *, follow_symlinks=false, dir_fd=none):\\n \"\"\"directory tree generator.\\n this behaves exactly like walk(), except that it yields a 4-tuple\\n dirpath, dirnames, filenames, dirfd\\n `dirpath`, `dirnames` and `filenames` are identical to walk() output,\\n and `dirfd` is a file descriptor referring to the directory `dirpath`.\\n the advantage of fwalk() over walk() is that it\\'s safe against symlink\\n races (when follow_symlinks is false).\\n if dir_fd is not none, it should be a file descriptor open to a directory,\\n and top should be relative; top will then be relative to that directory.\\n (dir_fd is always supported for fwalk.)\\n caution:\\n since fwalk() yields file descriptors, those are only valid until the\\n next iteration step, so you should dup() them if you want to keep them\\n for a longer period.\\n example:\\n import os\\n for root, dirs, files, rootfd in os.fwalk(\\'python/lib/email\\'):\\n print(root, \"consumes\", end=\"\")\\n print(sum([os.stat(name, dir_fd=rootfd).st_size for name in files]),\\n end=\"\")\\n print(\"bytes in\", len(files), \"non-directory files\")\\n if \\'cvs\\' in dirs:\\n dirs.remove(\\'cvs\\')  # don\\'t visit cvs directories\\n \"\"\"\\n # note: to guard against symlink races, we use the standard\\n # lstat()/open()/fstat() trick.\\n orig_st = stat(top, follow_symlinks=false, dir_fd=dir_fd)\\n topfd = open(top, o_rdonly, dir_fd=dir_fd)\\n try:\\n if (follow_symlinks or (st.s_isdir(orig_st.st_mode) and\\n path.samestat(orig_st, stat(topfd)))):\\n yield from _fwalk(topfd, top, topdown, onerror, follow_symlinks)\\n finally:\\n close(topfd)\\n def _fwalk(topfd, toppath, topdown, onerror, follow_symlinks):\\n # note: this uses o(depth of the directory tree) file descriptors: if\\n # necessary, it can be adapted to only require o(1) fds, see issue\\n # #13734.\\n names = listdir(topfd)\\n dirs, nondirs = [], []\\n for name in names:\\n try:\\n # here, we don\\'t use at_symlink_nofollow to be consistent with\\n # walk() which reports symlinks to directories as directories.\\n # we do however check for symlinks before recursing into\\n # a subdirectory.\\n if st.s_isdir(stat(name, dir_fd=topfd).st_mode):\\n dirs.append(name)\\n else:\\n nondirs.append(name)\\n except filenotfounderror:\\n try:\\n # add dangling symlinks, ignore disappeared files\\n if st.s_islnk(stat(name, dir_fd=topfd, follow_symlinks=false)\\n .st_mode):\\n nondirs.append(name)\\n except filenotfounderror:\\n continue\\n if topdown:\\n yield toppath, dirs, nondirs, topfd\\n for name in dirs:\\n try:\\n orig_st = stat(name, dir_fd=topfd, follow_symlinks=follow_symlinks)\\n dirfd = open(name, o_rdonly, dir_fd=topfd)\\n except oserror as err:\\n if onerror is not none:\\n onerror(err)\\n continue\\n try:\\n if follow_symlinks or path.samestat(orig_st, stat(dirfd)):\\n dirpath = path.join(toppath, name)\\n yield from _fwalk(dirfd, dirpath, topdown, onerror, follow_symlinks)\\n finally:\\n close(dirfd)\\n if not topdown:\\n yield toppath, dirs, nondirs, topfd\\n __all__.append(\"fwalk\")\\n # make sure os.environ exists, at least\\n try:\\n environ\\n except nameerror:\\n environ = {}\\n def execl(file, *args):\\n \"\"\"execl(file, *args)\\n execute the executable file with argument list args, replacing the\\n current process. \"\"\"\\n execv(file, args)\\n def execle(file, *args):\\n \"\"\"execle(file, *args, env)\\n execute the executable file with argument list args and\\n environment env, replacing the current process. \"\"\"\\n env = args[-1]\\n execve(file, args[:-1], env)\\n def execlp(file, *args):\\n \"\"\"execlp(file, *args)\\n execute the executable file (which is searched for along $path)\\n with argument list args, replacing the current process. \"\"\"\\n execvp(file, args)\\n def execlpe(file, *args):\\n \"\"\"execlpe(file, *args, env)\\n execute the executable file (which is searched for along $path)\\n with argument list args and environment env, replacing the current\\n process. \"\"\"\\n env = args[-1]\\n execvpe(file, args[:-1], env)\\n def execvp(file, args):\\n \"\"\"execvp(file, args)\\n execute the executable file (which is searched for along $path)\\n with argument list args, replacing the current process.\\n args may be a list or tuple of strings. \"\"\"\\n _execvpe(file, args)\\n def execvpe(file, args, env):\\n \"\"\"execvpe(file, args, env)\\n execute the executable file (which is searched for along $path)\\n with argument list args and environment env , replacing the\\n current process.\\n args may be a list or tuple of strings. \"\"\"\\n _execvpe(file, args, env)\\n __all__.extend([\"execl\",\"execle\",\"execlp\",\"execlpe\",\"execvp\",\"execvpe\"])\\n def _execvpe(file, args, env=none):\\n if env is not none:\\n exec_func = execve\\n argrest = (args, env)\\n else:\\n exec_func = execv\\n argrest = (args,)\\n env = environ\\n head, tail = path.split(file)\\n if head:\\n exec_func(file, *argrest)\\n return\\n last_exc = saved_exc = none\\n saved_tb = none\\n path_list = get_exec_path(env)\\n if name != \\'nt\\':\\n file = fsencode(file)\\n path_list = map(fsencode, path_list)\\n for dir in path_list:\\n fullname = path.join(dir, file)\\n try:\\n exec_func(fullname, *argrest)\\n except oserror as e:\\n last_exc = e\\n tb = sys.exc_info()[2]\\n if (e.errno != errno.enoent and e.errno != errno.enotdir\\n and saved_exc is none):\\n saved_exc = e\\n saved_tb = tb\\n if saved_exc:\\n raise saved_exc.with_traceback(saved_tb)\\n raise last_exc.with_traceback(tb)\\n def get_exec_path(env=none):\\n \"\"\"returns the sequence of directories that will be searched for the\\n named executable (similar to a shell) when launching a process.\\n *env* must be an environment variable dict or none.  if *env* is none,\\n os.environ will be used.\\n \"\"\"\\n # use a local import instead of a global import to limit the number of\\n # modules loaded at startup: the os module is always loaded at startup by\\n # python. it may also avoid a bootstrap issue.\\n import warnings\\n if env is none:\\n env = environ\\n # {b\\'path\\': ...}.get(\\'path\\') and {\\'path\\': ...}.get(b\\'path\\') emit a\\n # byteswarning when using python -b or python -bb: ignore the warning\\n with warnings.catch_warnings():\\n warnings.simplefilter(\"ignore\", byteswarning)\\n try:\\n path_list = env.get(\\'path\\')\\n except typeerror:\\n path_list = none\\n if supports_bytes_environ:\\n try:\\n path_listb = env[b\\'path\\']\\n except (keyerror, typeerror):\\n pass\\n else:\\n if path_list is not none:\\n raise valueerror(\\n \"env cannot contain \\'path\\' and b\\'path\\' keys\")\\n path_list = path_listb\\n if path_list is not none and isinstance(path_list, bytes):\\n path_list = fsdecode(path_list)\\n if path_list is none:\\n path_list = defpath\\n return path_list.split(pathsep)\\n # change environ to automatically call putenv(), unsetenv if they exist.\\n from _collections_abc import mutablemapping\\n class _environ(mutablemapping):\\n def __init__(self, data, encodekey, decodekey, encodevalue, decodevalue, putenv, unsetenv):\\n self.encodekey = encodekey\\n self.decodekey = decodekey\\n self.encodevalue = encodevalue\\n self.decodevalue = decodevalue\\n self.putenv = putenv\\n self.unsetenv = unsetenv\\n self._data = data\\n def __getitem__(self, key):\\n try:\\n value = self._data[self.encodekey(key)]\\n except keyerror:\\n # raise keyerror with the original key value\\n raise keyerror(key) from none\\n return self.decodevalue(value)\\n def __setitem__(self, key, value):\\n key = self.encodekey(key)\\n value = self.encodevalue(value)\\n self.putenv(key, value)\\n self._data[key] = value\\n def __delitem__(self, key):\\n encodedkey = self.encodekey(key)\\n self.unsetenv(encodedkey)\\n try:\\n del self._data[encodedkey]\\n except keyerror:\\n # raise keyerror with the original key value\\n raise keyerror(key) from none\\n def __iter__(self):\\n for key in self._data:\\n yield self.decodekey(key)\\n def __len__(self):\\n return len(self._data)\\n def __repr__(self):\\n return \\'environ({{{}}})\\'.format(\\', \\'.join(\\n (\\'{!r}: {!r}\\'.format(self.decodekey(key), self.decodevalue(value))\\n for key, value in self._data.items())))\\n def copy(self):\\n return dict(self)\\n def setdefault(self, key, value):\\n if key not in self:\\n self[key] = value\\n return self[key]\\n try:\\n _putenv = putenv\\n except nameerror:\\n _putenv = lambda key, value: none\\n else:\\n if \"putenv\" not in __all__:\\n __all__.append(\"putenv\")\\n try:\\n _unsetenv = unsetenv\\n except nameerror:\\n _unsetenv = lambda key: _putenv(key, \"\")\\n else:\\n if \"unsetenv\" not in __all__:\\n __all__.append(\"unsetenv\")\\n def _createenviron():\\n if name == \\'nt\\':\\n # where env var names must be uppercase\\n def check_str(value):\\n if not isinstance(value, str):\\n raise typeerror(\"str expected, not %s\" % type(value).__name__)\\n return value\\n encode = check_str\\n decode = str\\n def encodekey(key):\\n return encode(key).upper()\\n data = {}\\n for key, value in environ.items():\\n data[encodekey(key)] = value\\n else:\\n # where env var names can be mixed case\\n encoding = sys.getfilesystemencoding()\\n def encode(value):\\n if not isinstance(value, str):\\n raise typeerror(\"str expected, not %s\" % type(value).__name__)\\n return value.encode(encoding, \\'surrogateescape\\')\\n def decode(value):\\n return value.decode(encoding, \\'surrogateescape\\')\\n encodekey = encode\\n data = environ\\n return _environ(data,\\n encodekey, decode,\\n encode, decode,\\n _putenv, _unsetenv)\\n # unicode environ\\n environ = _createenviron()\\n del _createenviron\\n def getenv(key, default=none):\\n \"\"\"get an environment variable, return none if it doesn\\'t exist.\\n the optional second argument can specify an alternate default.\\n key, default and the result are str.\"\"\"\\n return environ.get(key, default)\\n supports_bytes_environ = (name != \\'nt\\')\\n __all__.extend((\"getenv\", \"supports_bytes_environ\"))\\n if supports_bytes_environ:\\n def _check_bytes(value):\\n if not isinstance(value, bytes):\\n raise typeerror(\"bytes expected, not %s\" % type(value).__name__)\\n return value\\n # bytes environ\\n environb = _environ(environ._data,\\n _check_bytes, bytes,\\n _check_bytes, bytes,\\n _putenv, _unsetenv)\\n del _check_bytes\\n def getenvb(key, default=none):\\n \"\"\"get an environment variable, return none if it doesn\\'t exist.\\n the optional second argument can specify an alternate default.\\n key, default and the result are bytes.\"\"\"\\n return environb.get(key, default)\\n __all__.extend((\"environb\", \"getenvb\"))\\n def _fscodec():\\n encoding = sys.getfilesystemencoding()\\n if encoding == \\'mbcs\\':\\n errors = \\'strict\\'\\n else:\\n errors = \\'surrogateescape\\'\\n def fsencode(filename):\\n \"\"\"\\n encode filename to the filesystem encoding with \\'surrogateescape\\' error\\n handler, return bytes unchanged. on windows, use \\'strict\\' error handler if\\n the file system encoding is \\'mbcs\\' (which is the default encoding).\\n \"\"\"\\n if isinstance(filename, bytes):\\n return filename\\n elif isinstance(filename, str):\\n return filename.encode(encoding, errors)\\n else:\\n raise typeerror(\"expect bytes or str, not %s\" % type(filename).__name__)\\n def fsdecode(filename):\\n \"\"\"\\n decode filename from the filesystem encoding with \\'surrogateescape\\' error\\n handler, return str unchanged. on windows, use \\'strict\\' error handler if\\n the file system encoding is \\'mbcs\\' (which is the default encoding).\\n \"\"\"\\n if isinstance(filename, str):\\n return filename\\n elif isinstance(filename, bytes):\\n return filename.decode(encoding, errors)\\n else:\\n raise typeerror(\"expect bytes or str, not %s\" % type(filename).__name__)\\n return fsencode, fsdecode\\n fsencode, fsdecode = _fscodec()\\n del _fscodec\\n # supply spawn*() (probably only for unix)\\n if _exists(\"fork\") and not _exists(\"spawnv\") and _exists(\"execv\"):\\n p_wait = 0\\n p_nowait = p_nowaito = 1\\n __all__.extend([\"p_wait\", \"p_nowait\", \"p_nowaito\"])\\n # xxx should we support p_detach?  i suppose it could fork()**2\\n # and close the std i/o streams.  also, p_overlay is the same\\n # as execv*()?\\n def _spawnvef(mode, file, args, env, func):\\n # internal helper; func is the exec*() function to use\\n pid = fork()\\n if not pid:\\n # child\\n try:\\n if env is none:\\n func(file, args)\\n else:\\n func(file, args, env)\\n except:\\n _exit(127)\\n else:\\n # parent\\n if mode == p_nowait:\\n return pid # caller is responsible for waiting!\\n while 1:\\n wpid, sts = waitpid(pid, 0)\\n if wifstopped(sts):\\n continue\\n elif wifsignaled(sts):\\n return -wtermsig(sts)\\n elif wifexited(sts):\\n return wexitstatus(sts)\\n else:\\n raise oserror(\"not stopped, signaled or exited???\")\\n def spawnv(mode, file, args):\\n \"\"\"spawnv(mode, file, args) -> integer\\n execute file with arguments from args in a subprocess.\\n if mode == p_nowait return the pid of the process.\\n if mode == p_wait return the process\\'s exit code if it exits normally;\\n otherwise return -sig, where sig is the signal that killed it. \"\"\"\\n return _spawnvef(mode, file, args, none, execv)\\n def spawnve(mode, file, args, env):\\n \"\"\"spawnve(mode, file, args, env) -> integer\\n execute file with arguments from args in a subprocess with the\\n specified environment.\\n if mode == p_nowait return the pid of the process.\\n if mode == p_wait return the process\\'s exit code if it exits normally;\\n otherwise return -sig, where sig is the signal that killed it. \"\"\"\\n return _spawnvef(mode, file, args, env, execve)\\n # note: spawnvp[e] is\\'t currently supported on windows\\n def spawnvp(mode, file, args):\\n \"\"\"spawnvp(mode, file, args) -> integer\\n execute file (which is looked for along $path) with arguments from\\n args in a subprocess.\\n if mode == p_nowait return the pid of the process.\\n if mode == p_wait return the process\\'s exit code if it exits normally;\\n otherwise return -sig, where sig is the signal that killed it. \"\"\"\\n return _spawnvef(mode, file, args, none, execvp)\\n def spawnvpe(mode, file, args, env):\\n \"\"\"spawnvpe(mode, file, args, env) -> integer\\n execute file (which is looked for along $path) with arguments from\\n args in a subprocess with the supplied environment.\\n if mode == p_nowait return the pid of the process.\\n if mode == p_wait return the process\\'s exit code if it exits normally;\\n otherwise return -sig, where sig is the signal that killed it. \"\"\"\\n return _spawnvef(mode, file, args, env, execvpe)\\n __all__.extend([\"spawnv\", \"spawnve\", \"spawnvp\", \"spawnvpe\"])\\n if _exists(\"spawnv\"):\\n # these aren\\'t supplied by the basic windows code\\n # but can be easily implemented in python\\n def spawnl(mode, file, *args):\\n \"\"\"spawnl(mode, file, *args) -> integer\\n execute file with arguments from args in a subprocess.\\n if mode == p_nowait return the pid of the process.\\n if mode == p_wait return the process\\'s exit code if it exits normally;\\n otherwise return -sig, where sig is the signal that killed it. \"\"\"\\n return spawnv(mode, file, args)\\n def spawnle(mode, file, *args):\\n \"\"\"spawnle(mode, file, *args, env) -> integer\\n execute file with arguments from args in a subprocess with the\\n supplied environment.\\n if mode == p_nowait return the pid of the process.\\n if mode == p_wait return the process\\'s exit code if it exits normally;\\n otherwise return -sig, where sig is the signal that killed it. \"\"\"\\n env = args[-1]\\n return spawnve(mode, file, args[:-1], env)\\n __all__.extend([\"spawnl\", \"spawnle\"])\\n if _exists(\"spawnvp\"):\\n # at the moment, windows doesn\\'t implement spawnvp[e],\\n # so it won\\'t have spawnlp[e] either.\\n def spawnlp(mode, file, *args):\\n \"\"\"spawnlp(mode, file, *args) -> integer\\n execute file (which is looked for along $path) with arguments from\\n args in a subprocess with the supplied environment.\\n if mode == p_nowait return the pid of the process.\\n if mode == p_wait return the process\\'s exit code if it exits normally;\\n otherwise return -sig, where sig is the signal that killed it. \"\"\"\\n return spawnvp(mode, file, args)\\n def spawnlpe(mode, file, *args):\\n \"\"\"spawnlpe(mode, file, *args, env) -> integer\\n execute file (which is looked for along $path) with arguments from\\n args in a subprocess with the supplied environment.\\n if mode == p_nowait return the pid of the process.\\n if mode == p_wait return the process\\'s exit code if it exits normally;\\n otherwise return -sig, where sig is the signal that killed it. \"\"\"\\n env = args[-1]\\n return spawnvpe(mode, file, args[:-1], env)\\n __all__.extend([\"spawnlp\", \"spawnlpe\"])\\n # supply os.popen()\\n def popen(cmd, mode=\"r\", buffering=-1):\\n if not isinstance(cmd, str):\\n raise typeerror(\"invalid cmd type (%s, expected string)\" % type(cmd))\\n if mode not in (\"r\", \"w\"):\\n raise valueerror(\"invalid mode %r\" % mode)\\n if buffering == 0 or buffering is none:\\n raise valueerror(\"popen() does not support unbuffered streams\")\\n import subprocess, io\\n if mode == \"r\":\\n proc = subprocess.popen(cmd,\\n shell=true,\\n stdout=subprocess.pipe,\\n bufsize=buffering)\\n return _wrap_close(io.textiowrapper(proc.stdout), proc)\\n else:\\n proc = subprocess.popen(cmd,\\n shell=true,\\n stdin=subprocess.pipe,\\n bufsize=buffering)\\n return _wrap_close(io.textiowrapper(proc.stdin), proc)\\n # helper for popen() -- a proxy for a file whose close waits for the process\\n class _wrap_close:\\n def __init__(self, stream, proc):\\n self._stream = stream\\n self._proc = proc\\n def close(self):\\n self._stream.close()\\n returncode = self._proc.wait()\\n if returncode == 0:\\n return none\\n if name == \\'nt\\':\\n return returncode\\n else:\\n return returncode << 8  # shift left to match old behavior\\n def __enter__(self):\\n return self\\n def __exit__(self, *args):\\n self.close()\\n def __getattr__(self, name):\\n return getattr(self._stream, name)\\n def __iter__(self):\\n return iter(self._stream)\\n # supply os.fdopen()\\n def fdopen(fd, *args, **kwargs):\\n if not isinstance(fd, int):\\n raise typeerror(\"invalid fd type (%s, expected integer)\" % type(fd))\\n import io\\n return io.open(fd, *args, **kwargs)\\n \"\"\"common operations on posix pathnames.\\n instead of importing this module directly, import os and refer to\\n this module as os.path.  the \"os.path\" name is an alias for this\\n module on posix systems; on other systems (e.g. mac, windows),\\n os.path provides the same operations in a manner specific to that\\n platform, and is an alias to another module (e.g. macpath, ntpath).\\n some of this can actually be useful on non-posix systems too, e.g.\\n for manipulation of the pathname component of urls.\\n \"\"\"\\n import os\\n import sys\\n import stat\\n import genericpath\\n from genericpath import *\\n __all__ = [\"normcase\",\"isabs\",\"join\",\"splitdrive\",\"split\",\"splitext\",\\n \"basename\",\"dirname\",\"commonprefix\",\"getsize\",\"getmtime\",\\n \"getatime\",\"getctime\",\"islink\",\"exists\",\"lexists\",\"isdir\",\"isfile\",\\n \"ismount\", \"expanduser\",\"expandvars\",\"normpath\",\"abspath\",\\n \"samefile\",\"sameopenfile\",\"samestat\",\\n \"curdir\",\"pardir\",\"sep\",\"pathsep\",\"defpath\",\"altsep\",\"extsep\",\\n \"devnull\",\"realpath\",\"supports_unicode_filenames\",\"relpath\",\\n \"commonpath\"]\\n # strings representing various path-related bits and pieces.\\n # these are primarily for export; internally, they are hardcoded.\\n curdir = \\'.\\'\\n pardir = \\'..\\'\\n extsep = \\'.\\'\\n sep = \\'/\\'\\n pathsep = \\':\\'\\n defpath = \\':/bin:/usr/bin\\'\\n altsep = none\\n devnull = \\'/dev/null\\'\\n def _get_sep(path):\\n if isinstance(path, bytes):\\n return b\\'/\\'\\n else:\\n return \\'/\\'\\n # normalize the case of a pathname.  trivial in posix, string.lower on mac.\\n # on ms-dos this may also turn slashes into backslashes; however, other\\n # normalizations (such as optimizing \\'../\\' away) are not allowed\\n # (another function should be defined to do that).\\n def normcase(s):\\n \"\"\"normalize case of pathname.  has no effect under posix\"\"\"\\n if not isinstance(s, (bytes, str)):\\n raise typeerror(\"normcase() argument must be str or bytes, \"\\n \"not \\'{}\\'\".format(s.__class__.__name__))\\n return s\\n # return whether a path is absolute.\\n # trivial in posix, harder on the mac or ms-dos.\\n def isabs(s):\\n \"\"\"test whether a path is absolute\"\"\"\\n sep = _get_sep(s)\\n return s.startswith(sep)\\n # join pathnames.\\n # ignore the previous parts if a part is absolute.\\n # insert a \\'/\\' unless the first part is empty or already ends in \\'/\\'.\\n def join(a, *p):\\n \"\"\"join two or more pathname components, inserting \\'/\\' as needed.\\n if any component is an absolute path, all previous path components\\n will be discarded.  an empty last part will result in a path that\\n ends with a separator.\"\"\"\\n sep = _get_sep(a)\\n path = a\\n try:\\n if not p:\\n path[:0] + sep  #23780: ensure compatible data type even if p is null.\\n for b in p:\\n if b.startswith(sep):\\n path = b\\n elif not path or path.endswith(sep):\\n path += b\\n else:\\n path += sep + b\\n except (typeerror, attributeerror, byteswarning):\\n genericpath._check_arg_types(\\'join\\', a, *p)\\n raise\\n return path\\n # split a path in head (everything up to the last \\'/\\') and tail (the\\n # rest).  if the path ends in \\'/\\', tail will be empty.  if there is no\\n # \\'/\\' in the path, head  will be empty.\\n # trailing \\'/\\'es are stripped from head unless it is the root.\\n def split(p):\\n \"\"\"split a pathname.  returns tuple \"(head, tail)\" where \"tail\" is\\n everything after the final slash.  either part may be empty.\"\"\"\\n sep = _get_sep(p)\\n i = p.rfind(sep) + 1\\n head, tail = p[:i], p[i:]\\n if head and head != sep*len(head):\\n head = head.rstrip(sep)\\n return head, tail\\n # split a path in root and extension.\\n # the extension is everything starting at the last dot in the last\\n # pathname component; the root is everything before that.\\n # it is always true that root + ext == p.\\n def splitext(p):\\n if isinstance(p, bytes):\\n sep = b\\'/\\'\\n extsep = b\\'.\\'\\n else:\\n sep = \\'/\\'\\n extsep = \\'.\\'\\n return genericpath._splitext(p, sep, none, extsep)\\n splitext.__doc__ = genericpath._splitext.__doc__\\n # split a pathname into a drive specification and the rest of the\\n # path.  useful on dos/windows/nt; on unix, the drive is always empty.\\n def splitdrive(p):\\n \"\"\"split a pathname into drive and path. on posix, drive is always\\n empty.\"\"\"\\n return p[:0], p\\n # return the tail (basename) part of a path, same as split(path)[1].\\n def basename(p):\\n \"\"\"returns the final component of a pathname\"\"\"\\n sep = _get_sep(p)\\n i = p.rfind(sep) + 1\\n return p[i:]\\n # return the head (dirname) part of a path, same as split(path)[0].\\n def dirname(p):\\n \"\"\"returns the directory component of a pathname\"\"\"\\n sep = _get_sep(p)\\n i = p.rfind(sep) + 1\\n head = p[:i]\\n if head and head != sep*len(head):\\n head = head.rstrip(sep)\\n return head\\n # is a path a symbolic link?\\n # this will always return false on systems where os.lstat doesn\\'t exist.\\n def islink(path):\\n \"\"\"test whether a path is a symbolic link\"\"\"\\n try:\\n st = os.lstat(path)\\n except (oserror, attributeerror):\\n return false\\n return stat.s_islnk(st.st_mode)\\n # being true for dangling symbolic links is also useful.\\n def lexists(path):\\n \"\"\"test whether a path exists.  returns true for broken symbolic links\"\"\"\\n try:\\n os.lstat(path)\\n except oserror:\\n return false\\n return true\\n # is a path a mount point?\\n # (does this work for all unixes?  is it even guaranteed to work by posix?)\\n def ismount(path):\\n \"\"\"test whether a path is a mount point\"\"\"\\n try:\\n s1 = os.lstat(path)\\n except oserror:\\n # it doesn\\'t exist -- so not a mount point. :-)\\n return false\\n else:\\n # a symlink can never be a mount point\\n if stat.s_islnk(s1.st_mode):\\n return false\\n if isinstance(path, bytes):\\n parent = join(path, b\\'..\\')\\n else:\\n parent = join(path, \\'..\\')\\n try:\\n s2 = os.lstat(parent)\\n except oserror:\\n return false\\n dev1 = s1.st_dev\\n dev2 = s2.st_dev\\n if dev1 != dev2:\\n return true     # path/.. on a different device as path\\n ino1 = s1.st_ino\\n ino2 = s2.st_ino\\n if ino1 == ino2:\\n return true     # path/.. is the same i-node as path\\n return false\\n # expand paths beginning with \\'~\\' or \\'~user\\'.\\n # \\'~\\' means $home; \\'~user\\' means that user\\'s home directory.\\n # if the path doesn\\'t begin with \\'~\\', or if the user or $home is unknown,\\n # the path is returned unchanged (leaving error reporting to whatever\\n # function is called with the expanded path as argument).\\n # see also module \\'glob\\' for expansion of *, ? and [...] in pathnames.\\n # (a function should also be defined to do full *sh-style environment\\n # variable expansion.)\\n def expanduser(path):\\n \"\"\"expand ~ and ~user constructions.  if user or $home is unknown,\\n do nothing.\"\"\"\\n if isinstance(path, bytes):\\n tilde = b\\'~\\'\\n else:\\n tilde = \\'~\\'\\n if not path.startswith(tilde):\\n return path\\n sep = _get_sep(path)\\n i = path.find(sep, 1)\\n if i < 0:\\n i = len(path)\\n if i == 1:\\n if \\'home\\' not in os.environ:\\n import pwd\\n userhome = pwd.getpwuid(os.getuid()).pw_dir\\n else:\\n userhome = os.environ[\\'home\\']\\n else:\\n import pwd\\n name = path[1:i]\\n if isinstance(name, bytes):\\n name = str(name, \\'ascii\\')\\n try:\\n pwent = pwd.getpwnam(name)\\n except keyerror:\\n return path\\n userhome = pwent.pw_dir\\n if isinstance(path, bytes):\\n userhome = os.fsencode(userhome)\\n root = b\\'/\\'\\n else:\\n root = \\'/\\'\\n userhome = userhome.rstrip(root)\\n return (userhome + path[i:]) or root\\n # expand paths containing shell variable substitutions.\\n # this expands the forms $variable and ${variable} only.\\n # non-existent variables are left unchanged.\\n _varprog = none\\n _varprogb = none\\n def expandvars(path):\\n \"\"\"expand shell variables of form $var and ${var}.  unknown variables\\n are left unchanged.\"\"\"\\n global _varprog, _varprogb\\n if isinstance(path, bytes):\\n if b\\'$\\' not in path:\\n return path\\n if not _varprogb:\\n import re\\n _varprogb = re.compile(br\\'\\\\$(\\\\w+|\\\\{[^}]*\\\\})\\', re.ascii)\\n search = _varprogb.search\\n start = b\\'{\\'\\n end = b\\'}\\'\\n environ = getattr(os, \\'environb\\', none)\\n else:\\n if \\'$\\' not in path:\\n return path\\n if not _varprog:\\n import re\\n _varprog = re.compile(r\\'\\\\$(\\\\w+|\\\\{[^}]*\\\\})\\', re.ascii)\\n search = _varprog.search\\n start = \\'{\\'\\n end = \\'}\\'\\n environ = os.environ\\n i = 0\\n while true:\\n m = search(path, i)\\n if not m:\\n break\\n i, j = m.span(0)\\n name = m.group(1)\\n if name.startswith(start) and name.endswith(end):\\n name = name[1:-1]\\n try:\\n if environ is none:\\n value = os.fsencode(os.environ[os.fsdecode(name)])\\n else:\\n value = environ[name]\\n except keyerror:\\n i = j\\n else:\\n tail = path[j:]\\n path = path[:i] + value\\n i = len(path)\\n path += tail\\n return path\\n # normalize a path, e.g. a//b, a/./b and a/foo/../b all become a/b.\\n # it should be understood that this may change the meaning of the path\\n # if it contains symbolic links!\\n def normpath(path):\\n \"\"\"normalize path, eliminating double slashes, etc.\"\"\"\\n if isinstance(path, bytes):\\n sep = b\\'/\\'\\n empty = b\\'\\'\\n dot = b\\'.\\'\\n dotdot = b\\'..\\'\\n else:\\n sep = \\'/\\'\\n empty = \\'\\'\\n dot = \\'.\\'\\n dotdot = \\'..\\'\\n if path == empty:\\n return dot\\n initial_slashes = path.startswith(sep)\\n # posix allows one or two initial slashes, but treats three or more\\n # as single slash.\\n if (initial_slashes and\\n path.startswith(sep*2) and not path.startswith(sep*3)):\\n initial_slashes = 2\\n comps = path.split(sep)\\n new_comps = []\\n for comp in comps:\\n if comp in (empty, dot):\\n continue\\n if (comp != dotdot or (not initial_slashes and not new_comps) or\\n (new_comps and new_comps[-1] == dotdot)):\\n new_comps.append(comp)\\n elif new_comps:\\n new_comps.pop()\\n comps = new_comps\\n path = sep.join(comps)\\n if initial_slashes:\\n path = sep*initial_slashes + path\\n return path or dot\\n def abspath(path):\\n \"\"\"return an absolute path.\"\"\"\\n if not isabs(path):\\n if isinstance(path, bytes):\\n cwd = os.getcwdb()\\n else:\\n cwd = os.getcwd()\\n path = join(cwd, path)\\n return normpath(path)\\n # return a canonical path (i.e. the absolute location of a file on the\\n # filesystem).\\n def realpath(filename):\\n \"\"\"return the canonical path of the specified filename, eliminating any\\n symbolic links encountered in the path.\"\"\"\\n path, ok = _joinrealpath(filename[:0], filename, {})\\n return abspath(path)\\n # join two paths, normalizing and eliminating any symbolic links\\n # encountered in the second path.\\n def _joinrealpath(path, rest, seen):\\n if isinstance(path, bytes):\\n sep = b\\'/\\'\\n curdir = b\\'.\\'\\n pardir = b\\'..\\'\\n else:\\n sep = \\'/\\'\\n curdir = \\'.\\'\\n pardir = \\'..\\'\\n if isabs(rest):\\n rest = rest[1:]\\n path = sep\\n while rest:\\n name, _, rest = rest.partition(sep)\\n if not name or name == curdir:\\n # current dir\\n continue\\n if name == pardir:\\n # parent dir\\n if path:\\n path, name = split(path)\\n if name == pardir:\\n path = join(path, pardir, pardir)\\n else:\\n path = pardir\\n continue\\n newpath = join(path, name)\\n if not islink(newpath):\\n path = newpath\\n continue\\n # resolve the symbolic link\\n if newpath in seen:\\n # already seen this path\\n path = seen[newpath]\\n if path is not none:\\n # use cached value\\n continue\\n # the symlink is not resolved, so we must have a symlink loop.\\n # return already resolved part + rest of the path unchanged.\\n return join(newpath, rest), false\\n seen[newpath] = none # not resolved symlink\\n path, ok = _joinrealpath(path, os.readlink(newpath), seen)\\n if not ok:\\n return join(path, rest), false\\n seen[newpath] = path # resolved symlink\\n return path, true\\n supports_unicode_filenames = (sys.platform == \\'darwin\\')\\n def relpath(path, start=none):\\n \"\"\"return a relative version of a path\"\"\"\\n if not path:\\n raise valueerror(\"no path specified\")\\n if isinstance(path, bytes):\\n curdir = b\\'.\\'\\n sep = b\\'/\\'\\n pardir = b\\'..\\'\\n else:\\n curdir = \\'.\\'\\n sep = \\'/\\'\\n pardir = \\'..\\'\\n if start is none:\\n start = curdir\\n try:\\n start_list = [x for x in abspath(start).split(sep) if x]\\n path_list = [x for x in abspath(path).split(sep) if x]\\n # work out how much of the filepath is shared by start and path.\\n i = len(commonprefix([start_list, path_list]))\\n rel_list = [pardir] * (len(start_list)-i) + path_list[i:]\\n if not rel_list:\\n return curdir\\n return join(*rel_list)\\n except (typeerror, attributeerror, byteswarning, deprecationwarning):\\n genericpath._check_arg_types(\\'relpath\\', path, start)\\n raise\\n # return the longest common sub-path of the sequence of paths given as input.\\n # the paths are not normalized before comparing them (this is the\\n # responsibility of the caller). any trailing separator is stripped from the\\n # returned path.\\n def commonpath(paths):\\n \"\"\"given a sequence of path names, returns the longest common sub-path.\"\"\"\\n if not paths:\\n raise valueerror(\\'commonpath() arg is an empty sequence\\')\\n if isinstance(paths[0], bytes):\\n sep = b\\'/\\'\\n curdir = b\\'.\\'\\n else:\\n sep = \\'/\\'\\n curdir = \\'.\\'\\n try:\\n split_paths = [path.split(sep) for path in paths]\\n try:\\n isabs, = set(p[:1] == sep for p in paths)\\n except valueerror:\\n raise valueerror(\"can\\'t mix absolute and relative paths\") from none\\n split_paths = [[c for c in s if c and c != curdir] for s in split_paths]\\n s1 = min(split_paths)\\n s2 = max(split_paths)\\n common = s1\\n for i, c in enumerate(s1):\\n if c != s2[i]:\\n common = s1[:i]\\n break\\n prefix = sep if isabs else sep[:0]\\n return prefix + sep.join(common)\\n except (typeerror, attributeerror):\\n genericpath._check_arg_types(\\'commonpath\\', *paths)\\n raise\\n \"\"\"random variable generators.\\n integers\\n --------\\n uniform within range\\n sequences\\n ---------\\n pick random element\\n pick random sample\\n generate random permutation\\n distributions on the real line:\\n ------------------------------\\n uniform\\n triangular\\n normal (gaussian)\\n lognormal\\n negative exponential\\n gamma\\n beta\\n pareto\\n weibull\\n distributions on the circle (angles 0 to 2pi)\\n ---------------------------------------------\\n circular uniform\\n von mises\\n general notes on the underlying mersenne twister core generator:\\n * the period is 2**19937-1.\\n * it is one of the most extensively tested generators in existence.\\n * the random() method is implemented in c, executes in a single python step,\\n and is, therefore, threadsafe.\\n \"\"\"\\n from warnings import warn as _warn\\n from types import methodtype as _methodtype, builtinmethodtype as _builtinmethodtype\\n from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil\\n from math import sqrt as _sqrt, acos as _acos, cos as _cos, sin as _sin\\n from os import urandom as _urandom\\n from _collections_abc import set as _set, sequence as _sequence\\n from hashlib import sha512 as _sha512\\n __all__ = [\"random\",\"seed\",\"random\",\"uniform\",\"randint\",\"choice\",\"sample\",\\n \"randrange\",\"shuffle\",\"normalvariate\",\"lognormvariate\",\\n \"expovariate\",\"vonmisesvariate\",\"gammavariate\",\"triangular\",\\n \"gauss\",\"betavariate\",\"paretovariate\",\"weibullvariate\",\\n \"getstate\",\"setstate\", \"getrandbits\",\\n \"systemrandom\"]\\n nv_magicconst = 4 * _exp(-0.5)/_sqrt(2.0)\\n twopi = 2.0*_pi\\n log4 = _log(4.0)\\n sg_magicconst = 1.0 + _log(4.5)\\n bpf = 53        # number of bits in a float\\n recip_bpf = 2**-bpf\\n # translated by guido van rossum from c source provided by\\n # adrian baddeley.  adapted by raymond hettinger for use with\\n # the mersenne twister  and os.urandom() core generators.\\n import _random\\n class random(_random.random):\\n \"\"\"random number generator base class used by bound module functions.\\n used to instantiate instances of random to get generators that don\\'t\\n share state.\\n class random can also be subclassed if you want to use a different basic\\n generator of your own devising: in that case, override the following\\n methods:  random(), seed(), getstate(), and setstate().\\n optionally, implement a getrandbits() method so that randrange()\\n can cover arbitrarily large ranges.\\n \"\"\"\\n version = 3     # used by getstate/setstate\\n def __init__(self, x=none):\\n \"\"\"initialize an instance.\\n optional argument x controls seeding, as for random.seed().\\n \"\"\"\\n self.seed(x)\\n self.gauss_next = none\\n def seed(self, a=none, version=2):\\n \"\"\"initialize internal state from hashable object.\\n none or no argument seeds from current time or from an operating\\n system specific randomness source if available.\\n for version 2 (the default), all of the bits are used if *a* is a str,\\n bytes, or bytearray.  for version 1, the hash() of *a* is used instead.\\n if *a* is an int, all bits are used.\\n \"\"\"\\n if a is none:\\n try:\\n # seed with enough bytes to span the 19937 bit\\n # state space for the mersenne twister\\n a = int.from_bytes(_urandom(2500), \\'big\\')\\n except notimplementederror:\\n import time\\n a = int(time.time() * 256) # use fractional seconds\\n if version == 2:\\n if isinstance(a, (str, bytes, bytearray)):\\n if isinstance(a, str):\\n a = a.encode()\\n a += _sha512(a).digest()\\n a = int.from_bytes(a, \\'big\\')\\n super().seed(a)\\n self.gauss_next = none\\n def getstate(self):\\n \"\"\"return internal state; can be passed to setstate() later.\"\"\"\\n return self.version, super().getstate(), self.gauss_next\\n def setstate(self, state):\\n \"\"\"restore internal state from object returned by getstate().\"\"\"\\n version = state[0]\\n if version == 3:\\n version, internalstate, self.gauss_next = state\\n super().setstate(internalstate)\\n elif version == 2:\\n version, internalstate, self.gauss_next = state\\n # in version 2, the state was saved as signed ints, which causes\\n #   inconsistencies between 32/64-bit systems. the state is\\n #   really unsigned 32-bit ints, so we convert negative ints from\\n #   version 2 to positive longs for version 3.\\n try:\\n internalstate = tuple(x % (2**32) for x in internalstate)\\n except valueerror as e:\\n raise typeerror from e\\n super().setstate(internalstate)\\n else:\\n raise valueerror(\"state with version %s passed to \"\\n \"random.setstate() of version %s\" %\\n (version, self.version))\\n ## ---- methods below this point do not need to be overridden when\\n ## ---- subclassing for the purpose of using a different core generator.\\n ## -------------------- pickle support  -------------------\\n # issue 17489: since __reduce__ was defined to fix #759889 this is no\\n # longer called; we leave it here because it has been here since random was\\n # rewritten back in 2001 and why risk breaking something.\\n def __getstate__(self): # for pickle\\n return self.getstate()\\n def __setstate__(self, state):  # for pickle\\n self.setstate(state)\\n def __reduce__(self):\\n return self.__class__, (), self.getstate()\\n ## -------------------- integer methods  -------------------\\n def randrange(self, start, stop=none, step=1, _int=int):\\n \"\"\"choose a random item from range(start, stop[, step]).\\n this fixes the problem with randint() which includes the\\n endpoint; in python this is usually not what you want.\\n \"\"\"\\n # this code is a bit messy to make it fast for the\\n # common case while still doing adequate error checking.\\n istart = _int(start)\\n if istart != start:\\n raise valueerror(\"non-integer arg 1 for randrange()\")\\n if stop is none:\\n if istart > 0:\\n return self._randbelow(istart)\\n raise valueerror(\"empty range for randrange()\")\\n # stop argument supplied.\\n istop = _int(stop)\\n if istop != stop:\\n raise valueerror(\"non-integer stop for randrange()\")\\n width = istop - istart\\n if step == 1 and width > 0:\\n return istart + self._randbelow(width)\\n if step == 1:\\n raise valueerror(\"empty range for randrange() (%d,%d, %d)\" % (istart, istop, width))\\n # non-unit step argument supplied.\\n istep = _int(step)\\n if istep != step:\\n raise valueerror(\"non-integer step for randrange()\")\\n if istep > 0:\\n n = (width + istep - 1) // istep\\n elif istep < 0:\\n n = (width + istep + 1) // istep\\n else:\\n raise valueerror(\"zero step for randrange()\")\\n if n <= 0:\\n raise valueerror(\"empty range for randrange()\")\\n return istart + istep*self._randbelow(n)\\n def randint(self, a, b):\\n \"\"\"return random integer in range [a, b], including both end points.\\n \"\"\"\\n return self.randrange(a, b+1)\\n def _randbelow(self, n, int=int, maxsize=1<<bpf, type=type,\\n method=_methodtype, builtinmethod=_builtinmethodtype):\\n \"return a random int in the range [0,n).  raises valueerror if n==0.\"\\n random = self.random\\n getrandbits = self.getrandbits\\n # only call self.getrandbits if the original random() builtin method\\n # has not been overridden or if a new getrandbits() was supplied.\\n if type(random) is builtinmethod or type(getrandbits) is method:\\n k = n.bit_length()  # don\\'t use (n-1) here because n can be 1\\n r = getrandbits(k)          # 0 <= r < 2**k\\n while r >= n:\\n r = getrandbits(k)\\n return r\\n # there\\'s an overridden random() method but no new getrandbits() method,\\n # so we can only use random() from here.\\n if n >= maxsize:\\n _warn(\"underlying random() generator does not supply \\\\n\"\\n \"enough bits to choose from a population range this large.\\\\n\"\\n \"to remove the range limitation, add a getrandbits() method.\")\\n return int(random() * n)\\n rem = maxsize % n\\n limit = (maxsize - rem) / maxsize   # int(limit * maxsize) % n == 0\\n r = random()\\n while r >= limit:\\n r = random()\\n return int(r*maxsize) % n\\n ## -------------------- sequence methods  -------------------\\n def choice(self, seq):\\n \"\"\"choose a random element from a non-empty sequence.\"\"\"\\n try:\\n i = self._randbelow(len(seq))\\n except valueerror:\\n raise indexerror(\\'cannot choose from an empty sequence\\')\\n return seq[i]\\n def shuffle(self, x, random=none):\\n \"\"\"shuffle list x in place, and return none.\\n optional argument random is a 0-argument function returning a\\n random float in [0.0, 1.0); if it is the default none, the\\n standard random.random will be used.\\n \"\"\"\\n if random is none:\\n randbelow = self._randbelow\\n for i in reversed(range(1, len(x))):\\n # pick an element in x[:i+1] with which to exchange x[i]\\n j = randbelow(i+1)\\n x[i], x[j] = x[j], x[i]\\n else:\\n _int = int\\n for i in reversed(range(1, len(x))):\\n # pick an element in x[:i+1] with which to exchange x[i]\\n j = _int(random() * (i+1))\\n x[i], x[j] = x[j], x[i]\\n def sample(self, population, k):\\n \"\"\"chooses k unique random elements from a population sequence or set.\\n returns a new list containing elements from the population while\\n leaving the original population unchanged.  the resulting list is\\n in selection order so that all sub-slices will also be valid random\\n samples.  this allows raffle winners (the sample) to be partitioned\\n into grand prize and second place winners (the subslices).\\n members of the population need not be hashable or unique.  if the\\n population contains repeats, then each occurrence is a possible\\n selection in the sample.\\n to choose a sample in a range of integers, use range as an argument.\\n this is especially fast and space efficient for sampling from a\\n large population:   sample(range(10000000), 60)\\n \"\"\"\\n # sampling without replacement entails tracking either potential\\n # selections (the pool) in a list or previous selections in a set.\\n # when the number of selections is small compared to the\\n # population, then tracking selections is efficient, requiring\\n # only a small set and an occasional reselection.  for\\n # a larger number of selections, the pool tracking method is\\n # preferred since the list takes less space than the\\n # set and it doesn\\'t suffer from frequent reselections.\\n if isinstance(population, _set):\\n population = tuple(population)\\n if not isinstance(population, _sequence):\\n raise typeerror(\"population must be a sequence or set.  for dicts, use list(d).\")\\n randbelow = self._randbelow\\n n = len(population)\\n if not 0 <= k <= n:\\n raise valueerror(\"sample larger than population\")\\n result = [none] * k\\n setsize = 21        # size of a small set minus size of an empty list\\n if k > 5:\\n setsize += 4 ** _ceil(_log(k * 3, 4)) # table size for big sets\\n if n <= setsize:\\n # an n-length list is smaller than a k-length set\\n pool = list(population)\\n for i in range(k):         # invariant:  non-selected at [0,n-i)\\n j = randbelow(n-i)\\n result[i] = pool[j]\\n pool[j] = pool[n-i-1]   # move non-selected item into vacancy\\n else:\\n selected = set()\\n selected_add = selected.add\\n for i in range(k):\\n j = randbelow(n)\\n while j in selected:\\n j = randbelow(n)\\n selected_add(j)\\n result[i] = population[j]\\n return result\\n ## -------------------- real-valued distributions  -------------------\\n ## -------------------- uniform distribution -------------------\\n def uniform(self, a, b):\\n \"get a random number in the range [a, b) or [a, b] depending on rounding.\"\\n return a + (b-a) * self.random()\\n ## -------------------- triangular --------------------\\n def triangular(self, low=0.0, high=1.0, mode=none):\\n \"\"\"triangular distribution.\\n continuous distribution bounded by given lower and upper limits,\\n and having a given mode value in-between.\\n http://en.wikipedia.org/wiki/triangular_distribution\\n \"\"\"\\n u = self.random()\\n try:\\n c = 0.5 if mode is none else (mode - low) / (high - low)\\n except zerodivisionerror:\\n return low\\n if u > c:\\n u = 1.0 - u\\n c = 1.0 - c\\n low, high = high, low\\n return low + (high - low) * (u * c) ** 0.5\\n ## -------------------- normal distribution --------------------\\n def normalvariate(self, mu, sigma):\\n \"\"\"normal distribution.\\n mu is the mean, and sigma is the standard deviation.\\n \"\"\"\\n # mu = mean, sigma = standard deviation\\n # uses kinderman and monahan method. reference: kinderman,\\n # a.j. and monahan, j.f., \"computer generation of random\\n # variables using the ratio of uniform deviates\", acm trans\\n # math software, 3, (1977), pp257-260.\\n random = self.random\\n while 1:\\n u1 = random()\\n u2 = 1.0 - random()\\n z = nv_magicconst*(u1-0.5)/u2\\n zz = z*z/4.0\\n if zz <= -_log(u2):\\n break\\n return mu + z*sigma\\n ## -------------------- lognormal distribution --------------------\\n def lognormvariate(self, mu, sigma):\\n \"\"\"log normal distribution.\\n if you take the natural logarithm of this distribution, you\\'ll get a\\n normal distribution with mean mu and standard deviation sigma.\\n mu can have any value, and sigma must be greater than zero.\\n \"\"\"\\n return _exp(self.normalvariate(mu, sigma))\\n ## -------------------- exponential distribution --------------------\\n def expovariate(self, lambd):\\n \"\"\"exponential distribution.\\n lambd is 1.0 divided by the desired mean.  it should be\\n nonzero.  (the parameter would be called \"lambda\", but that is\\n a reserved word in python.)  returned values range from 0 to\\n positive infinity if lambd is positive, and from negative\\n infinity to 0 if lambd is negative.\\n \"\"\"\\n # lambd: rate lambd = 1/mean\\n # (\\'lambda\\' is a python reserved word)\\n # we use 1-random() instead of random() to preclude the\\n # possibility of taking the log of zero.\\n return -_log(1.0 - self.random())/lambd\\n ## -------------------- von mises distribution --------------------\\n def vonmisesvariate(self, mu, kappa):\\n \"\"\"circular data distribution.\\n mu is the mean angle, expressed in radians between 0 and 2*pi, and\\n kappa is the concentration parameter, which must be greater than or\\n equal to zero.  if kappa is equal to zero, this distribution reduces\\n to a uniform random angle over the range 0 to 2*pi.\\n \"\"\"\\n # mu:    mean angle (in radians between 0 and 2*pi)\\n # kappa: concentration parameter kappa (>= 0)\\n # if kappa = 0 generate uniform random angle\\n # based upon an algorithm published in: fisher, n.i.,\\n # \"statistical analysis of circular data\", cambridge\\n # university press, 1993.\\n # thanks to magnus kessler for a correction to the\\n # implementation of step 4.\\n random = self.random\\n if kappa <= 1e-6:\\n return twopi * random()\\n s = 0.5 / kappa\\n r = s + _sqrt(1.0 + s * s)\\n while 1:\\n u1 = random()\\n z = _cos(_pi * u1)\\n d = z / (r + z)\\n u2 = random()\\n if u2 < 1.0 - d * d or u2 <= (1.0 - d) * _exp(d):\\n break\\n q = 1.0 / r\\n f = (q + z) / (1.0 + q * z)\\n u3 = random()\\n if u3 > 0.5:\\n theta = (mu + _acos(f)) % twopi\\n else:\\n theta = (mu - _acos(f)) % twopi\\n return theta\\n ## -------------------- gamma distribution --------------------\\n def gammavariate(self, alpha, beta):\\n \"\"\"gamma distribution.  not the gamma function!\\n conditions on the parameters are alpha > 0 and beta > 0.\\n the probability distribution function is:\\n x ** (alpha - 1) * math.exp(-x / beta)\\n pdf(x) =  --------------------------------------\\n math.gamma(alpha) * beta ** alpha\\n \"\"\"\\n # alpha > 0, beta > 0, mean is alpha*beta, variance is alpha*beta**2\\n # warning: a few older sources define the gamma distribution in terms\\n # of alpha > -1.0\\n if alpha <= 0.0 or beta <= 0.0:\\n raise valueerror(\\'gammavariate: alpha and beta must be > 0.0\\')\\n random = self.random\\n if alpha > 1.0:\\n # uses r.c.h. cheng, \"the generation of gamma\\n # variables with non-integral shape parameters\",\\n # applied statistics, (1977), 26, no. 1, p71-74\\n ainv = _sqrt(2.0 * alpha - 1.0)\\n bbb = alpha - log4\\n ccc = alpha + ainv\\n while 1:\\n u1 = random()\\n if not 1e-7 < u1 < .9999999:\\n continue\\n u2 = 1.0 - random()\\n v = _log(u1/(1.0-u1))/ainv\\n x = alpha*_exp(v)\\n z = u1*u1*u2\\n r = bbb+ccc*v-x\\n if r + sg_magicconst - 4.5*z >= 0.0 or r >= _log(z):\\n return x * beta\\n elif alpha == 1.0:\\n # expovariate(1)\\n u = random()\\n while u <= 1e-7:\\n u = random()\\n return -_log(u) * beta\\n else:   # alpha is between 0 and 1 (exclusive)\\n # uses algorithm gs of statistical computing - kennedy & gentle\\n while 1:\\n u = random()\\n b = (_e + alpha)/_e\\n p = b*u\\n if p <= 1.0:\\n x = p ** (1.0/alpha)\\n else:\\n x = -_log((b-p)/alpha)\\n u1 = random()\\n if p > 1.0:\\n if u1 <= x ** (alpha - 1.0):\\n break\\n elif u1 <= _exp(-x):\\n break\\n return x * beta\\n ## -------------------- gauss (faster alternative) --------------------\\n def gauss(self, mu, sigma):\\n \"\"\"gaussian distribution.\\n mu is the mean, and sigma is the standard deviation.  this is\\n slightly faster than the normalvariate() function.\\n not thread-safe without a lock around calls.\\n \"\"\"\\n # when x and y are two variables from [0, 1), uniformly\\n # distributed, then\\n #\\n #    cos(2*pi*x)*sqrt(-2*log(1-y))\\n #    sin(2*pi*x)*sqrt(-2*log(1-y))\\n #\\n # are two *independent* variables with normal distribution\\n # (mu = 0, sigma = 1).\\n # (lambert meertens)\\n # (corrected version; bug discovered by mike miller, fixed by lm)\\n # multithreading note: when two threads call this function\\n # simultaneously, it is possible that they will receive the\\n # same return value.  the window is very small though.  to\\n # avoid this, you have to use a lock around all calls.  (i\\n # didn\\'t want to slow this down in the serial case by using a\\n # lock here.)\\n random = self.random\\n z = self.gauss_next\\n self.gauss_next = none\\n if z is none:\\n x2pi = random() * twopi\\n g2rad = _sqrt(-2.0 * _log(1.0 - random()))\\n z = _cos(x2pi) * g2rad\\n self.gauss_next = _sin(x2pi) * g2rad\\n return mu + z*sigma\\n ## -------------------- beta --------------------\\n ## see\\n ## http://mail.python.org/pipermail/python-bugs-list/2001-january/003752.html\\n ## for ivan frohne\\'s insightful analysis of why the original implementation:\\n ##\\n ##    def betavariate(self, alpha, beta):\\n ##        # discrete event simulation in c, pp 87-88.\\n ##\\n ##        y = self.expovariate(alpha)\\n ##        z = self.expovariate(1.0/beta)\\n ##        return z/(y+z)\\n ##\\n ## was dead wrong, and how it probably got that way.\\n def betavariate(self, alpha, beta):\\n \"\"\"beta distribution.\\n conditions on the parameters are alpha > 0 and beta > 0.\\n returned values range between 0 and 1.\\n \"\"\"\\n # this version due to janne sinkkonen, and matches all the std\\n # texts (e.g., knuth vol 2 ed 3 pg 134 \"the beta distribution\").\\n y = self.gammavariate(alpha, 1.)\\n if y == 0:\\n return 0.0\\n else:\\n return y / (y + self.gammavariate(beta, 1.))\\n ## -------------------- pareto --------------------\\n def paretovariate(self, alpha):\\n \"\"\"pareto distribution.  alpha is the shape parameter.\"\"\"\\n # jain, pg. 495\\n u = 1.0 - self.random()\\n return 1.0 / u ** (1.0/alpha)\\n ## -------------------- weibull --------------------\\n def weibullvariate(self, alpha, beta):\\n \"\"\"weibull distribution.\\n alpha is the scale parameter and beta is the shape parameter.\\n \"\"\"\\n # jain, pg. 499; bug fix courtesy bill arms\\n u = 1.0 - self.random()\\n return alpha * (-_log(u)) ** (1.0/beta)\\n ## --------------- operating system random source  ------------------\\n class systemrandom(random):\\n \"\"\"alternate random number generator using sources provided\\n by the operating system (such as /dev/urandom on unix or\\n cryptgenrandom on windows).\\n not available on all systems (see os.urandom() for details).\\n \"\"\"\\n def random(self):\\n \"\"\"get the next random number in the range [0.0, 1.0).\"\"\"\\n return (int.from_bytes(_urandom(7), \\'big\\') >> 3) * recip_bpf\\n def getrandbits(self, k):\\n \"\"\"getrandbits(k) -> x.  generates an int with k random bits.\"\"\"\\n if k <= 0:\\n raise valueerror(\\'number of bits must be greater than zero\\')\\n if k != int(k):\\n raise typeerror(\\'number of bits should be an integer\\')\\n numbytes = (k + 7) // 8                       # bits / 8 and rounded up\\n x = int.from_bytes(_urandom(numbytes), \\'big\\')\\n return x >> (numbytes * 8 - k)                # trim excess bits\\n def seed(self, *args, **kwds):\\n \"stub method.  not used for a system random number generator.\"\\n return none\\n def _notimplemented(self, *args, **kwds):\\n \"method should not be called for a system random number generator.\"\\n raise notimplementederror(\\'system entropy source does not have state.\\')\\n getstate = setstate = _notimplemented\\n ## -------------------- test program --------------------\\n def _test_generator(n, func, args):\\n import time\\n print(n, \\'times\\', func.__name__)\\n total = 0.0\\n sqsum = 0.0\\n smallest = 1e10\\n largest = -1e10\\n t0 = time.time()\\n for i in range(n):\\n x = func(*args)\\n total += x\\n sqsum = sqsum + x*x\\n smallest = min(x, smallest)\\n largest = max(x, largest)\\n t1 = time.time()\\n print(round(t1-t0, 3), \\'sec,\\', end=\\' \\')\\n avg = total/n\\n stddev = _sqrt(sqsum/n - avg*avg)\\n print(\\'avg %g, stddev %g, min %g, max %g\\\\n\\' % \\\\\\n (avg, stddev, smallest, largest))\\n def _test(n=2000):\\n _test_generator(n, random, ())\\n _test_generator(n, normalvariate, (0.0, 1.0))\\n _test_generator(n, lognormvariate, (0.0, 1.0))\\n _test_generator(n, vonmisesvariate, (0.0, 1.0))\\n _test_generator(n, gammavariate, (0.01, 1.0))\\n _test_generator(n, gammavariate, (0.1, 1.0))\\n _test_generator(n, gammavariate, (0.1, 2.0))\\n _test_generator(n, gammavariate, (0.5, 1.0))\\n _test_generator(n, gammavariate, (0.9, 1.0))\\n _test_generator(n, gammavariate, (1.0, 1.0))\\n _test_generator(n, gammavariate, (2.0, 1.0))\\n _test_generator(n, gammavariate, (20.0, 1.0))\\n _test_generator(n, gammavariate, (200.0, 1.0))\\n _test_generator(n, gauss, (0.0, 1.0))\\n _test_generator(n, betavariate, (3.0, 3.0))\\n _test_generator(n, triangular, (0.0, 1.0, 1.0/3.0))\\n # create one instance, seeded from current time, and export its methods\\n # as module-level functions.  the functions share state across all uses\\n #(both in the user\\'s code and in the python libraries), but that\\'s fine\\n # for most programs and is easier for the casual user than making them\\n # instantiate their own random() instance.\\n _inst = random()\\n seed = _inst.seed\\n random = _inst.random\\n uniform = _inst.uniform\\n triangular = _inst.triangular\\n randint = _inst.randint\\n choice = _inst.choice\\n randrange = _inst.randrange\\n sample = _inst.sample\\n shuffle = _inst.shuffle\\n normalvariate = _inst.normalvariate\\n lognormvariate = _inst.lognormvariate\\n expovariate = _inst.expovariate\\n vonmisesvariate = _inst.vonmisesvariate\\n gammavariate = _inst.gammavariate\\n gauss = _inst.gauss\\n betavariate = _inst.betavariate\\n paretovariate = _inst.paretovariate\\n weibullvariate = _inst.weibullvariate\\n getstate = _inst.getstate\\n setstate = _inst.setstate\\n getrandbits = _inst.getrandbits\\n if __name__ == \\'__main__\\':\\n _test()\\n #\\n # secret labs\\' regular expression engine\\n #\\n # re-compatible interface for the sre matching engine\\n #\\n # copyright (c) 1998-2001 by secret labs ab.  all rights reserved.\\n #\\n # this version of the sre library can be redistributed under cnri\\'s\\n # python 1.6 license.  for any other use, please contact secret labs\\n # ab (info@pythonware.com).\\n #\\n # portions of this engine have been developed in cooperation with\\n # cnri.  hewlett-packard provided funding for 1.6 integration and\\n # other compatibility work.\\n #\\n r\"\"\"support for regular expressions (re).\\n this module provides regular expression matching operations similar to\\n those found in perl.  it supports both 8-bit and unicode strings; both\\n the pattern and the strings being processed can contain null bytes and\\n characters outside the us ascii range.\\n regular expressions can contain both special and ordinary characters.\\n most ordinary characters, like \"a\", \"a\", or \"0\", are the simplest\\n regular expressions; they simply match themselves.  you can\\n concatenate ordinary characters, so last matches the string \\'last\\'.\\n the special characters are:\\n \".\"      matches any character except a newline.\\n \"^\"      matches the start of the string.\\n \"$\"      matches the end of the string or just before the newline at\\n the end of the string.\\n \"*\"      matches 0 or more (greedy) repetitions of the preceding re.\\n greedy means that it will match as many repetitions as possible.\\n \"+\"      matches 1 or more (greedy) repetitions of the preceding re.\\n \"?\"      matches 0 or 1 (greedy) of the preceding re.\\n *?,+?,?? non-greedy versions of the previous three special characters.\\n {m,n}    matches from m to n repetitions of the preceding re.\\n {m,n}?   non-greedy version of the above.\\n \"\\\\\\\\\"     either escapes special characters or signals a special sequence.\\n []       indicates a set of characters.\\n a \"^\" as the first character indicates a complementing set.\\n \"|\"      a|b, creates an re that will match either a or b.\\n (...)    matches the re inside the parentheses.\\n the contents can be retrieved or matched later in the string.\\n (?ailmsux) set the a, i, l, m, s, u, or x flag for the re (see below).\\n (?:...)  non-grouping version of regular parentheses.\\n (?p<name>...) the substring matched by the group is accessible by name.\\n (?p=name)     matches the text matched earlier by the group named name.\\n (?#...)  a comment; ignored.\\n (?=...)  matches if ... matches next, but doesn\\'t consume the string.\\n (?!...)  matches if ... doesn\\'t match next.\\n (?<=...) matches if preceded by ... (must be fixed length).\\n (?<!...) matches if not preceded by ... (must be fixed length).\\n (?(id/name)yes|no) matches yes pattern if the group with id/name matched,\\n the (optional) no pattern otherwise.\\n the special sequences consist of \"\\\\\\\\\" and a character from the list\\n below.  if the ordinary character is not on the list, then the\\n resulting re will match the second character.\\n \\\\number  matches the contents of the group of the same number.\\n \\\\a       matches only at the start of the string.\\n \\\\z       matches only at the end of the string.\\n \\\\b       matches the empty string, but only at the start or end of a word.\\n \\\\b       matches the empty string, but not at the start or end of a word.\\n \\\\d       matches any decimal digit; equivalent to the set [0-9] in\\n bytes patterns or string patterns with the ascii flag.\\n in string patterns without the ascii flag, it will match the whole\\n range of unicode digits.\\n \\\\d       matches any non-digit character; equivalent to [^\\\\d].\\n \\\\s       matches any whitespace character; equivalent to [ \\\\t\\\\n\\\\r\\\\f\\\\v] in\\n bytes patterns or string patterns with the ascii flag.\\n in string patterns without the ascii flag, it will match the whole\\n range of unicode whitespace characters.\\n \\\\s       matches any non-whitespace character; equivalent to [^\\\\s].\\n \\\\w       matches any alphanumeric character; equivalent to [a-za-z0-9_]\\n in bytes patterns or string patterns with the ascii flag.\\n in string patterns without the ascii flag, it will match the\\n range of unicode alphanumeric characters (letters plus digits\\n plus underscore).\\n with locale, it will match the set [0-9_] plus characters defined\\n as letters for the current locale.\\n \\\\w       matches the complement of \\\\w.\\n \\\\\\\\       matches a literal backslash.\\n this module exports the following functions:\\n match     match a regular expression pattern to the beginning of a string.\\n fullmatch match a regular expression pattern to all of a string.\\n search    search a string for the presence of a pattern.\\n sub       substitute occurrences of a pattern found in a string.\\n subn      same as sub, but also return the number of substitutions made.\\n split     split a string by the occurrences of a pattern.\\n findall   find all occurrences of a pattern in a string.\\n finditer  return an iterator yielding a match object for each match.\\n compile   compile a pattern into a regexobject.\\n purge     clear the regular expression cache.\\n escape    backslash all non-alphanumerics in a string.\\n some of the functions in this module takes flags as optional parameters:\\n a  ascii       for string patterns, make \\\\w, \\\\w, \\\\b, \\\\b, \\\\d, \\\\d\\n match the corresponding ascii character categories\\n (rather than the whole unicode categories, which is the\\n default).\\n for bytes patterns, this flag is the only available\\n behaviour and needn\\'t be specified.\\n i  ignorecase  perform case-insensitive matching.\\n l  locale      make \\\\w, \\\\w, \\\\b, \\\\b, dependent on the current locale.\\n m  multiline   \"^\" matches the beginning of lines (after a newline)\\n as well as the string.\\n \"$\" matches the end of lines (before a newline) as well\\n as the end of the string.\\n s  dotall      \".\" matches any character at all, including the newline.\\n x  verbose     ignore whitespace and comments for nicer looking re\\'s.\\n u  unicode     for compatibility only. ignored for string patterns (it\\n is the default), and forbidden for bytes patterns.\\n this module also defines an exception \\'error\\'.\\n \"\"\"\\n import sys\\n import sre_compile\\n import sre_parse\\n try:\\n import _locale\\n except importerror:\\n _locale = none\\n # public symbols\\n __all__ = [\\n \"match\", \"fullmatch\", \"search\", \"sub\", \"subn\", \"split\",\\n \"findall\", \"finditer\", \"compile\", \"purge\", \"template\", \"escape\",\\n \"error\", \"a\", \"i\", \"l\", \"m\", \"s\", \"x\", \"u\",\\n \"ascii\", \"ignorecase\", \"locale\", \"multiline\", \"dotall\", \"verbose\",\\n \"unicode\",\\n ]\\n __version__ = \"2.2.1\"\\n # flags\\n a = ascii = sre_compile.sre_flag_ascii # assume ascii \"locale\"\\n i = ignorecase = sre_compile.sre_flag_ignorecase # ignore case\\n l = locale = sre_compile.sre_flag_locale # assume current 8-bit locale\\n u = unicode = sre_compile.sre_flag_unicode # assume unicode \"locale\"\\n m = multiline = sre_compile.sre_flag_multiline # make anchors look for newline\\n s = dotall = sre_compile.sre_flag_dotall # make dot match newline\\n x = verbose = sre_compile.sre_flag_verbose # ignore whitespace and comments\\n # sre extensions (experimental, don\\'t rely on these)\\n t = template = sre_compile.sre_flag_template # disable backtracking\\n debug = sre_compile.sre_flag_debug # dump pattern after compilation\\n # sre exception\\n error = sre_compile.error\\n # --------------------------------------------------------------------\\n # public interface\\n def match(pattern, string, flags=0):\\n \"\"\"try to apply the pattern at the start of the string, returning\\n a match object, or none if no match was found.\"\"\"\\n return _compile(pattern, flags).match(string)\\n def fullmatch(pattern, string, flags=0):\\n \"\"\"try to apply the pattern to all of the string, returning\\n a match object, or none if no match was found.\"\"\"\\n return _compile(pattern, flags).fullmatch(string)\\n def search(pattern, string, flags=0):\\n \"\"\"scan through string looking for a match to the pattern, returning\\n a match object, or none if no match was found.\"\"\"\\n return _compile(pattern, flags).search(string)\\n def sub(pattern, repl, string, count=0, flags=0):\\n \"\"\"return the string obtained by replacing the leftmost\\n non-overlapping occurrences of the pattern in string by the\\n replacement repl.  repl can be either a string or a callable;\\n if a string, backslash escapes in it are processed.  if it is\\n a callable, it\\'s passed the match object and must return\\n a replacement string to be used.\"\"\"\\n return _compile(pattern, flags).sub(repl, string, count)\\n def subn(pattern, repl, string, count=0, flags=0):\\n \"\"\"return a 2-tuple containing (new_string, number).\\n new_string is the string obtained by replacing the leftmost\\n non-overlapping occurrences of the pattern in the source\\n string by the replacement repl.  number is the number of\\n substitutions that were made. repl can be either a string or a\\n callable; if a string, backslash escapes in it are processed.\\n if it is a callable, it\\'s passed the match object and must\\n return a replacement string to be used.\"\"\"\\n return _compile(pattern, flags).subn(repl, string, count)\\n def split(pattern, string, maxsplit=0, flags=0):\\n \"\"\"split the source string by the occurrences of the pattern,\\n returning a list containing the resulting substrings.  if\\n capturing parentheses are used in pattern, then the text of all\\n groups in the pattern are also returned as part of the resulting\\n list.  if maxsplit is nonzero, at most maxsplit splits occur,\\n and the remainder of the string is returned as the final element\\n of the list.\"\"\"\\n return _compile(pattern, flags).split(string, maxsplit)\\n def findall(pattern, string, flags=0):\\n \"\"\"return a list of all non-overlapping matches in the string.\\n if one or more capturing groups are present in the pattern, return\\n a list of groups; this will be a list of tuples if the pattern\\n has more than one group.\\n empty matches are included in the result.\"\"\"\\n return _compile(pattern, flags).findall(string)\\n def finditer(pattern, string, flags=0):\\n \"\"\"return an iterator over all non-overlapping matches in the\\n string.  for each match, the iterator returns a match object.\\n empty matches are included in the result.\"\"\"\\n return _compile(pattern, flags).finditer(string)\\n def compile(pattern, flags=0):\\n \"compile a regular expression pattern, returning a pattern object.\"\\n return _compile(pattern, flags)\\n def purge():\\n \"clear the regular expression caches\"\\n _cache.clear()\\n _cache_repl.clear()\\n def template(pattern, flags=0):\\n \"compile a template pattern, returning a pattern object\"\\n return _compile(pattern, flags|t)\\n _alphanum_str = frozenset(\\n \"_abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz01234567890\")\\n _alphanum_bytes = frozenset(\\n b\"_abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz01234567890\")\\n def escape(pattern):\\n \"\"\"\\n escape all the characters in pattern except ascii letters, numbers and \\'_\\'.\\n \"\"\"\\n if isinstance(pattern, str):\\n alphanum = _alphanum_str\\n s = list(pattern)\\n for i, c in enumerate(pattern):\\n if c not in alphanum:\\n if c == \"\\\\000\":\\n s[i] = \"\\\\\\\\000\"\\n else:\\n s[i] = \"\\\\\\\\\" + c\\n return \"\".join(s)\\n else:\\n alphanum = _alphanum_bytes\\n s = []\\n esc = ord(b\"\\\\\\\\\")\\n for c in pattern:\\n if c in alphanum:\\n s.append(c)\\n else:\\n if c == 0:\\n s.extend(b\"\\\\\\\\000\")\\n else:\\n s.append(esc)\\n s.append(c)\\n return bytes(s)\\n # --------------------------------------------------------------------\\n # internals\\n _cache = {}\\n _cache_repl = {}\\n _pattern_type = type(sre_compile.compile(\"\", 0))\\n _maxcache = 512\\n def _compile(pattern, flags):\\n # internal: compile pattern\\n try:\\n p, loc = _cache[type(pattern), pattern, flags]\\n if loc is none or loc == _locale.setlocale(_locale.lc_ctype):\\n return p\\n except keyerror:\\n pass\\n if isinstance(pattern, _pattern_type):\\n if flags:\\n raise valueerror(\\n \"cannot process flags argument with a compiled pattern\")\\n return pattern\\n if not sre_compile.isstring(pattern):\\n raise typeerror(\"first argument must be string or compiled pattern\")\\n p = sre_compile.compile(pattern, flags)\\n if not (flags & debug):\\n if len(_cache) >= _maxcache:\\n _cache.clear()\\n if p.flags & locale:\\n if not _locale:\\n return p\\n loc = _locale.setlocale(_locale.lc_ctype)\\n else:\\n loc = none\\n _cache[type(pattern), pattern, flags] = p, loc\\n return p\\n def _compile_repl(repl, pattern):\\n # internal: compile replacement pattern\\n try:\\n return _cache_repl[repl, pattern]\\n except keyerror:\\n pass\\n p = sre_parse.parse_template(repl, pattern)\\n if len(_cache_repl) >= _maxcache:\\n _cache_repl.clear()\\n _cache_repl[repl, pattern] = p\\n return p\\n def _expand(pattern, match, template):\\n # internal: match.expand implementation hook\\n template = sre_parse.parse_template(template, pattern)\\n return sre_parse.expand_template(template, match)\\n def _subx(pattern, template):\\n # internal: pattern.sub/subn implementation helper\\n template = _compile_repl(template, pattern)\\n if not template[0] and len(template[1]) == 1:\\n # literal replacement\\n return template[1][0]\\n def filter(match, template=template):\\n return sre_parse.expand_template(template, match)\\n return filter\\n # register myself for pickling\\n import copyreg\\n def _pickle(p):\\n return _compile, (p.pattern, p.flags)\\n copyreg.pickle(_pattern_type, _pickle, _compile)\\n # --------------------------------------------------------------------\\n # experimental stuff (see python-dev discussions for details)\\n class scanner:\\n def __init__(self, lexicon, flags=0):\\n from sre_constants import branch, subpattern\\n self.lexicon = lexicon\\n # combine phrases into a compound pattern\\n p = []\\n s = sre_parse.pattern()\\n s.flags = flags\\n for phrase, action in lexicon:\\n gid = s.opengroup()\\n p.append(sre_parse.subpattern(s, [\\n (subpattern, (gid, sre_parse.parse(phrase, flags))),\\n ]))\\n s.closegroup(gid, p[-1])\\n p = sre_parse.subpattern(s, [(branch, (none, p))])\\n self.scanner = sre_compile.compile(p)\\n def scan(self, string):\\n result = []\\n append = result.append\\n match = self.scanner.scanner(string).match\\n i = 0\\n while true:\\n m = match()\\n if not m:\\n break\\n j = m.end()\\n if i == j:\\n break\\n action = self.lexicon[m.lastindex-1][1]\\n if callable(action):\\n self.match = m\\n action = action(self, m.group())\\n if action is not none:\\n append(action)\\n i = j\\n return result, string[i:]\\n \"\"\"redo the builtin repr() (representation) but with limits on most sizes.\"\"\"\\n __all__ = [\"repr\", \"repr\", \"recursive_repr\"]\\n import builtins\\n from itertools import islice\\n try:\\n from _thread import get_ident\\n except importerror:\\n from _dummy_thread import get_ident\\n def recursive_repr(fillvalue=\\'...\\'):\\n \\'decorator to make a repr function return fillvalue for a recursive call\\'\\n def decorating_function(user_function):\\n repr_running = set()\\n def wrapper(self):\\n key = id(self), get_ident()\\n if key in repr_running:\\n return fillvalue\\n repr_running.add(key)\\n try:\\n result = user_function(self)\\n finally:\\n repr_running.discard(key)\\n return result\\n # can\\'t use functools.wraps() here because of bootstrap issues\\n wrapper.__module__ = getattr(user_function, \\'__module__\\')\\n wrapper.__doc__ = getattr(user_function, \\'__doc__\\')\\n wrapper.__name__ = getattr(user_function, \\'__name__\\')\\n wrapper.__qualname__ = getattr(user_function, \\'__qualname__\\')\\n wrapper.__annotations__ = getattr(user_function, \\'__annotations__\\', {})\\n return wrapper\\n return decorating_function\\n class repr:\\n def __init__(self):\\n self.maxlevel = 6\\n self.maxtuple = 6\\n self.maxlist = 6\\n self.maxarray = 5\\n self.maxdict = 4\\n self.maxset = 6\\n self.maxfrozenset = 6\\n self.maxdeque = 6\\n self.maxstring = 30\\n self.maxlong = 40\\n self.maxother = 30\\n def repr(self, x):\\n return self.repr1(x, self.maxlevel)\\n def repr1(self, x, level):\\n typename = type(x).__name__\\n if \\' \\' in typename:\\n parts = typename.split()\\n typename = \\'_\\'.join(parts)\\n if hasattr(self, \\'repr_\\' + typename):\\n return getattr(self, \\'repr_\\' + typename)(x, level)\\n else:\\n return self.repr_instance(x, level)\\n def _repr_iterable(self, x, level, left, right, maxiter, trail=\\'\\'):\\n n = len(x)\\n if level <= 0 and n:\\n s = \\'...\\'\\n else:\\n newlevel = level - 1\\n repr1 = self.repr1\\n pieces = [repr1(elem, newlevel) for elem in islice(x, maxiter)]\\n if n > maxiter:  pieces.append(\\'...\\')\\n s = \\', \\'.join(pieces)\\n if n == 1 and trail:  right = trail + right\\n return \\'%s%s%s\\' % (left, s, right)\\n def repr_tuple(self, x, level):\\n return self._repr_iterable(x, level, \\'(\\', \\')\\', self.maxtuple, \\',\\')\\n def repr_list(self, x, level):\\n return self._repr_iterable(x, level, \\'[\\', \\']\\', self.maxlist)\\n def repr_array(self, x, level):\\n if not x:\\n return \"array(\\'%s\\')\" % x.typecode\\n header = \"array(\\'%s\\', [\" % x.typecode\\n return self._repr_iterable(x, level, header, \\'])\\', self.maxarray)\\n def repr_set(self, x, level):\\n if not x:\\n return \\'set()\\'\\n x = _possibly_sorted(x)\\n return self._repr_iterable(x, level, \\'{\\', \\'}\\', self.maxset)\\n def repr_frozenset(self, x, level):\\n if not x:\\n return \\'frozenset()\\'\\n x = _possibly_sorted(x)\\n return self._repr_iterable(x, level, \\'frozenset({\\', \\'})\\',\\n self.maxfrozenset)\\n def repr_deque(self, x, level):\\n return self._repr_iterable(x, level, \\'deque([\\', \\'])\\', self.maxdeque)\\n def repr_dict(self, x, level):\\n n = len(x)\\n if n == 0: return \\'{}\\'\\n if level <= 0: return \\'{...}\\'\\n newlevel = level - 1\\n repr1 = self.repr1\\n pieces = []\\n for key in islice(_possibly_sorted(x), self.maxdict):\\n keyrepr = repr1(key, newlevel)\\n valrepr = repr1(x[key], newlevel)\\n pieces.append(\\'%s: %s\\' % (keyrepr, valrepr))\\n if n > self.maxdict: pieces.append(\\'...\\')\\n s = \\', \\'.join(pieces)\\n return \\'{%s}\\' % (s,)\\n def repr_str(self, x, level):\\n s = builtins.repr(x[:self.maxstring])\\n if len(s) > self.maxstring:\\n i = max(0, (self.maxstring-3)//2)\\n j = max(0, self.maxstring-3-i)\\n s = builtins.repr(x[:i] + x[len(x)-j:])\\n s = s[:i] + \\'...\\' + s[len(s)-j:]\\n return s\\n def repr_int(self, x, level):\\n s = builtins.repr(x) # xxx hope this isn\\'t too slow...\\n if len(s) > self.maxlong:\\n i = max(0, (self.maxlong-3)//2)\\n j = max(0, self.maxlong-3-i)\\n s = s[:i] + \\'...\\' + s[len(s)-j:]\\n return s\\n def repr_instance(self, x, level):\\n try:\\n s = builtins.repr(x)\\n # bugs in x.__repr__() can cause arbitrary\\n # exceptions -- then make up something\\n except exception:\\n return \\'<%s instance at %#x>\\' % (x.__class__.__name__, id(x))\\n if len(s) > self.maxother:\\n i = max(0, (self.maxother-3)//2)\\n j = max(0, self.maxother-3-i)\\n s = s[:i] + \\'...\\' + s[len(s)-j:]\\n return s\\n def _possibly_sorted(x):\\n # since not all sequences of items can be sorted and comparison\\n # functions may raise arbitrary exceptions, return an unsorted\\n # sequence in that case.\\n try:\\n return sorted(x)\\n except exception:\\n return list(x)\\n arepr = repr()\\n repr = arepr.repr\\n \"\"\"word completion for gnu readline.\\n the completer completes keywords, built-ins and globals in a selectable\\n namespace (which defaults to __main__); when completing name.name..., it\\n evaluates (!) the expression up to the last dot and completes its attributes.\\n it\\'s very cool to do \"import sys\" type \"sys.\", hit the completion key (twice),\\n and see the list of names defined by the sys module!\\n tip: to use the tab key as the completion key, call\\n readline.parse_and_bind(\"tab: complete\")\\n notes:\\n - exceptions raised by the completer function are *ignored* (and generally cause\\n the completion to fail).  this is a feature -- since readline sets the tty\\n device in raw (or cbreak) mode, printing a traceback wouldn\\'t work well\\n without some complicated hoopla to save, reset and restore the tty state.\\n - the evaluation of the name.name... form may cause arbitrary application\\n defined code to be executed if an object with a __getattr__ hook is found.\\n since it is the responsibility of the application (or the user) to enable this\\n feature, i consider this an acceptable risk.  more complicated expressions\\n (e.g. function calls or indexing operations) are *not* evaluated.\\n - when the original stdin is not a tty device, gnu readline is never\\n used, and this module (and the readline module) are silently inactive.\\n \"\"\"\\n import atexit\\n import builtins\\n import __main__\\n __all__ = [\"completer\"]\\n class completer:\\n def __init__(self, namespace = none):\\n \"\"\"create a new completer for the command line.\\n completer([namespace]) -> completer instance.\\n if unspecified, the default namespace where completions are performed\\n is __main__ (technically, __main__.__dict__). namespaces should be\\n given as dictionaries.\\n completer instances should be used as the completion mechanism of\\n readline via the set_completer() call:\\n readline.set_completer(completer(my_namespace).complete)\\n \"\"\"\\n if namespace and not isinstance(namespace, dict):\\n raise typeerror(\\'namespace must be a dictionary\\')\\n # don\\'t bind to namespace quite yet, but flag whether the user wants a\\n # specific namespace or to use __main__.__dict__. this will allow us\\n # to bind to __main__.__dict__ at completion time, not now.\\n if namespace is none:\\n self.use_main_ns = 1\\n else:\\n self.use_main_ns = 0\\n self.namespace = namespace\\n def complete(self, text, state):\\n \"\"\"return the next possible completion for \\'text\\'.\\n this is called successively with state == 0, 1, 2, ... until it\\n returns none.  the completion should begin with \\'text\\'.\\n \"\"\"\\n if self.use_main_ns:\\n self.namespace = __main__.__dict__\\n if not text.strip():\\n if state == 0:\\n if _readline_available:\\n readline.insert_text(\\'\\\\t\\')\\n readline.redisplay()\\n return \\'\\'\\n else:\\n return \\'\\\\t\\'\\n else:\\n return none\\n if state == 0:\\n if \".\" in text:\\n self.matches = self.attr_matches(text)\\n else:\\n self.matches = self.global_matches(text)\\n try:\\n return self.matches[state]\\n except indexerror:\\n return none\\n def _callable_postfix(self, val, word):\\n if callable(val):\\n word = word + \"(\"\\n return word\\n def global_matches(self, text):\\n \"\"\"compute matches when text is a simple name.\\n return a list of all keywords, built-in functions and names currently\\n defined in self.namespace that match.\\n \"\"\"\\n import keyword\\n matches = []\\n seen = {\"__builtins__\"}\\n n = len(text)\\n for word in keyword.kwlist:\\n if word[:n] == text:\\n seen.add(word)\\n matches.append(word)\\n for nspace in [self.namespace, builtins.__dict__]:\\n for word, val in nspace.items():\\n if word[:n] == text and word not in seen:\\n seen.add(word)\\n matches.append(self._callable_postfix(val, word))\\n return matches\\n def attr_matches(self, text):\\n \"\"\"compute matches when text contains a dot.\\n assuming the text is of the form name.name....[name], and is\\n evaluable in self.namespace, it will be evaluated and its attributes\\n (as revealed by dir()) are used as possible completions.  (for class\\n instances, class members are also considered.)\\n warning: this can still invoke arbitrary c code, if an object\\n with a __getattr__ hook is evaluated.\\n \"\"\"\\n import re\\n m = re.match(r\"(\\\\w+(\\\\.\\\\w+)*)\\\\.(\\\\w*)\", text)\\n if not m:\\n return []\\n expr, attr = m.group(1, 3)\\n try:\\n thisobject = eval(expr, self.namespace)\\n except exception:\\n return []\\n # get the content of the object, except __builtins__\\n words = set(dir(thisobject))\\n words.discard(\"__builtins__\")\\n if hasattr(thisobject, \\'__class__\\'):\\n words.add(\\'__class__\\')\\n words.update(get_class_members(thisobject.__class__))\\n matches = []\\n n = len(attr)\\n for word in words:\\n if word[:n] == attr:\\n try:\\n val = getattr(thisobject, word)\\n except exception:\\n continue  # exclude properties that are not set\\n word = self._callable_postfix(val, \"%s.%s\" % (expr, word))\\n matches.append(word)\\n matches.sort()\\n return matches\\n def get_class_members(klass):\\n ret = dir(klass)\\n if hasattr(klass,\\'__bases__\\'):\\n for base in klass.__bases__:\\n ret = ret + get_class_members(base)\\n return ret\\n try:\\n import readline\\n except importerror:\\n _readline_available = false\\n else:\\n readline.set_completer(completer().complete)\\n # release references early at shutdown (the readline module\\'s\\n # contents are quasi-immortal, and the completer function holds a\\n # reference to globals).\\n atexit.register(lambda: readline.set_completer(none))\\n _readline_available = true\\n \"\"\"utility functions for copying and archiving files and directory trees.\\n xxx the functions here don\\'t copy the resource fork or other metadata on mac.\\n \"\"\"\\n import os\\n import sys\\n import stat\\n import fnmatch\\n import collections\\n import errno\\n import tarfile\\n try:\\n import bz2\\n del bz2\\n _bz2_supported = true\\n except importerror:\\n _bz2_supported = false\\n try:\\n import lzma\\n del lzma\\n _lzma_supported = true\\n except importerror:\\n _lzma_supported = false\\n try:\\n from pwd import getpwnam\\n except importerror:\\n getpwnam = none\\n try:\\n from grp import getgrnam\\n except importerror:\\n getgrnam = none\\n __all__ = [\"copyfileobj\", \"copyfile\", \"copymode\", \"copystat\", \"copy\", \"copy2\",\\n \"copytree\", \"move\", \"rmtree\", \"error\", \"specialfileerror\",\\n \"execerror\", \"make_archive\", \"get_archive_formats\",\\n \"register_archive_format\", \"unregister_archive_format\",\\n \"get_unpack_formats\", \"register_unpack_format\",\\n \"unregister_unpack_format\", \"unpack_archive\",\\n \"ignore_patterns\", \"chown\", \"which\", \"get_terminal_size\",\\n \"samefileerror\"]\\n # disk_usage is added later, if available on the platform\\n class error(oserror):\\n pass\\n class samefileerror(error):\\n \"\"\"raised when source and destination are the same file.\"\"\"\\n class specialfileerror(oserror):\\n \"\"\"raised when trying to do a kind of operation (e.g. copying) which is\\n not supported on a special file (e.g. a named pipe)\"\"\"\\n class execerror(oserror):\\n \"\"\"raised when a command could not be executed\"\"\"\\n class readerror(oserror):\\n \"\"\"raised when an archive cannot be read\"\"\"\\n class registryerror(exception):\\n \"\"\"raised when a registry operation with the archiving\\n and unpacking registeries fails\"\"\"\\n def copyfileobj(fsrc, fdst, length=16*1024):\\n \"\"\"copy data from file-like object fsrc to file-like object fdst\"\"\"\\n while 1:\\n buf = fsrc.read(length)\\n if not buf:\\n break\\n fdst.write(buf)\\n def _samefile(src, dst):\\n # macintosh, unix.\\n if hasattr(os.path, \\'samefile\\'):\\n try:\\n return os.path.samefile(src, dst)\\n except oserror:\\n return false\\n # all other platforms: check for same pathname.\\n return (os.path.normcase(os.path.abspath(src)) ==\\n os.path.normcase(os.path.abspath(dst)))\\n def copyfile(src, dst, *, follow_symlinks=true):\\n \"\"\"copy data from src to dst.\\n if follow_symlinks is not set and src is a symbolic link, a new\\n symlink will be created instead of copying the file it points to.\\n \"\"\"\\n if _samefile(src, dst):\\n raise samefileerror(\"{!r} and {!r} are the same file\".format(src, dst))\\n for fn in [src, dst]:\\n try:\\n st = os.stat(fn)\\n except oserror:\\n # file most likely does not exist\\n pass\\n else:\\n # xxx what about other special files? (sockets, devices...)\\n if stat.s_isfifo(st.st_mode):\\n raise specialfileerror(\"`%s` is a named pipe\" % fn)\\n if not follow_symlinks and os.path.islink(src):\\n os.symlink(os.readlink(src), dst)\\n else:\\n with open(src, \\'rb\\') as fsrc:\\n with open(dst, \\'wb\\') as fdst:\\n copyfileobj(fsrc, fdst)\\n return dst\\n def copymode(src, dst, *, follow_symlinks=true):\\n \"\"\"copy mode bits from src to dst.\\n if follow_symlinks is not set, symlinks aren\\'t followed if and only\\n if both `src` and `dst` are symlinks.  if `lchmod` isn\\'t available\\n (e.g. linux) this method does nothing.\\n \"\"\"\\n if not follow_symlinks and os.path.islink(src) and os.path.islink(dst):\\n if hasattr(os, \\'lchmod\\'):\\n stat_func, chmod_func = os.lstat, os.lchmod\\n else:\\n return\\n elif hasattr(os, \\'chmod\\'):\\n stat_func, chmod_func = os.stat, os.chmod\\n else:\\n return\\n st = stat_func(src)\\n chmod_func(dst, stat.s_imode(st.st_mode))\\n if hasattr(os, \\'listxattr\\'):\\n def _copyxattr(src, dst, *, follow_symlinks=true):\\n \"\"\"copy extended filesystem attributes from `src` to `dst`.\\n overwrite existing attributes.\\n if `follow_symlinks` is false, symlinks won\\'t be followed.\\n \"\"\"\\n try:\\n names = os.listxattr(src, follow_symlinks=follow_symlinks)\\n except oserror as e:\\n if e.errno not in (errno.enotsup, errno.enodata):\\n raise\\n return\\n for name in names:\\n try:\\n value = os.getxattr(src, name, follow_symlinks=follow_symlinks)\\n os.setxattr(dst, name, value, follow_symlinks=follow_symlinks)\\n except oserror as e:\\n if e.errno not in (errno.eperm, errno.enotsup, errno.enodata):\\n raise\\n else:\\n def _copyxattr(*args, **kwargs):\\n pass\\n def copystat(src, dst, *, follow_symlinks=true):\\n \"\"\"copy all stat info (mode bits, atime, mtime, flags) from src to dst.\\n if the optional flag `follow_symlinks` is not set, symlinks aren\\'t followed if and\\n only if both `src` and `dst` are symlinks.\\n \"\"\"\\n def _nop(*args, ns=none, follow_symlinks=none):\\n pass\\n # follow symlinks (aka don\\'t not follow symlinks)\\n follow = follow_symlinks or not (os.path.islink(src) and os.path.islink(dst))\\n if follow:\\n # use the real function if it exists\\n def lookup(name):\\n return getattr(os, name, _nop)\\n else:\\n # use the real function only if it exists\\n # *and* it supports follow_symlinks\\n def lookup(name):\\n fn = getattr(os, name, _nop)\\n if fn in os.supports_follow_symlinks:\\n return fn\\n return _nop\\n st = lookup(\"stat\")(src, follow_symlinks=follow)\\n mode = stat.s_imode(st.st_mode)\\n lookup(\"utime\")(dst, ns=(st.st_atime_ns, st.st_mtime_ns),\\n follow_symlinks=follow)\\n try:\\n lookup(\"chmod\")(dst, mode, follow_symlinks=follow)\\n except notimplementederror:\\n # if we got a notimplementederror, it\\'s because\\n #   * follow_symlinks=false,\\n #   * lchown() is unavailable, and\\n #   * either\\n #       * fchownat() is unavailable or\\n #       * fchownat() doesn\\'t implement at_symlink_nofollow.\\n #         (it returned enosup.)\\n # therefore we\\'re out of options--we simply cannot chown the\\n # symlink.  give up, suppress the error.\\n # (which is what shutil always did in this circumstance.)\\n pass\\n if hasattr(st, \\'st_flags\\'):\\n try:\\n lookup(\"chflags\")(dst, st.st_flags, follow_symlinks=follow)\\n except oserror as why:\\n for err in \\'eopnotsupp\\', \\'enotsup\\':\\n if hasattr(errno, err) and why.errno == getattr(errno, err):\\n break\\n else:\\n raise\\n _copyxattr(src, dst, follow_symlinks=follow)\\n def copy(src, dst, *, follow_symlinks=true):\\n \"\"\"copy data and mode bits (\"cp src dst\"). return the file\\'s destination.\\n the destination may be a directory.\\n if follow_symlinks is false, symlinks won\\'t be followed. this\\n resembles gnu\\'s \"cp -p src dst\".\\n if source and destination are the same file, a samefileerror will be\\n raised.\\n \"\"\"\\n if os.path.isdir(dst):\\n dst = os.path.join(dst, os.path.basename(src))\\n copyfile(src, dst, follow_symlinks=follow_symlinks)\\n copymode(src, dst, follow_symlinks=follow_symlinks)\\n return dst\\n def copy2(src, dst, *, follow_symlinks=true):\\n \"\"\"copy data and all stat info (\"cp -p src dst\"). return the file\\'s\\n destination.\"\\n the destination may be a directory.\\n if follow_symlinks is false, symlinks won\\'t be followed. this\\n resembles gnu\\'s \"cp -p src dst\".\\n \"\"\"\\n if os.path.isdir(dst):\\n dst = os.path.join(dst, os.path.basename(src))\\n copyfile(src, dst, follow_symlinks=follow_symlinks)\\n copystat(src, dst, follow_symlinks=follow_symlinks)\\n return dst\\n def ignore_patterns(*patterns):\\n \"\"\"function that can be used as copytree() ignore parameter.\\n patterns is a sequence of glob-style patterns\\n that are used to exclude files\"\"\"\\n def _ignore_patterns(path, names):\\n ignored_names = []\\n for pattern in patterns:\\n ignored_names.extend(fnmatch.filter(names, pattern))\\n return set(ignored_names)\\n return _ignore_patterns\\n def copytree(src, dst, symlinks=false, ignore=none, copy_function=copy2,\\n ignore_dangling_symlinks=false):\\n \"\"\"recursively copy a directory tree.\\n the destination directory must not already exist.\\n if exception(s) occur, an error is raised with a list of reasons.\\n if the optional symlinks flag is true, symbolic links in the\\n source tree result in symbolic links in the destination tree; if\\n it is false, the contents of the files pointed to by symbolic\\n links are copied. if the file pointed by the symlink doesn\\'t\\n exist, an exception will be added in the list of errors raised in\\n an error exception at the end of the copy process.\\n you can set the optional ignore_dangling_symlinks flag to true if you\\n want to silence this exception. notice that this has no effect on\\n platforms that don\\'t support os.symlink.\\n the optional ignore argument is a callable. if given, it\\n is called with the `src` parameter, which is the directory\\n being visited by copytree(), and `names` which is the list of\\n `src` contents, as returned by os.listdir():\\n callable(src, names) -> ignored_names\\n since copytree() is called recursively, the callable will be\\n called once for each directory that is copied. it returns a\\n list of names relative to the `src` directory that should\\n not be copied.\\n the optional copy_function argument is a callable that will be used\\n to copy each file. it will be called with the source path and the\\n destination path as arguments. by default, copy2() is used, but any\\n function that supports the same signature (like copy()) can be used.\\n \"\"\"\\n names = os.listdir(src)\\n if ignore is not none:\\n ignored_names = ignore(src, names)\\n else:\\n ignored_names = set()\\n os.makedirs(dst)\\n errors = []\\n for name in names:\\n if name in ignored_names:\\n continue\\n srcname = os.path.join(src, name)\\n dstname = os.path.join(dst, name)\\n try:\\n if os.path.islink(srcname):\\n linkto = os.readlink(srcname)\\n if symlinks:\\n # we can\\'t just leave it to `copy_function` because legacy\\n # code with a custom `copy_function` may rely on copytree\\n # doing the right thing.\\n os.symlink(linkto, dstname)\\n copystat(srcname, dstname, follow_symlinks=not symlinks)\\n else:\\n # ignore dangling symlink if the flag is on\\n if not os.path.exists(linkto) and ignore_dangling_symlinks:\\n continue\\n # otherwise let the copy occurs. copy2 will raise an error\\n if os.path.isdir(srcname):\\n copytree(srcname, dstname, symlinks, ignore,\\n copy_function)\\n else:\\n copy_function(srcname, dstname)\\n elif os.path.isdir(srcname):\\n copytree(srcname, dstname, symlinks, ignore, copy_function)\\n else:\\n # will raise a specialfileerror for unsupported file types\\n copy_function(srcname, dstname)\\n # catch the error from the recursive copytree so that we can\\n # continue with other files\\n except error as err:\\n errors.extend(err.args[0])\\n except oserror as why:\\n errors.append((srcname, dstname, str(why)))\\n try:\\n copystat(src, dst)\\n except oserror as why:\\n # copying file access times may fail on windows\\n if getattr(why, \\'winerror\\', none) is none:\\n errors.append((src, dst, str(why)))\\n if errors:\\n raise error(errors)\\n return dst\\n # version vulnerable to race conditions\\n def _rmtree_unsafe(path, onerror):\\n try:\\n if os.path.islink(path):\\n # symlinks to directories are forbidden, see bug #1669\\n raise oserror(\"cannot call rmtree on a symbolic link\")\\n except oserror:\\n onerror(os.path.islink, path, sys.exc_info())\\n # can\\'t continue even if onerror hook returns\\n return\\n names = []\\n try:\\n names = os.listdir(path)\\n except oserror:\\n onerror(os.listdir, path, sys.exc_info())\\n for name in names:\\n fullname = os.path.join(path, name)\\n try:\\n mode = os.lstat(fullname).st_mode\\n except oserror:\\n mode = 0\\n if stat.s_isdir(mode):\\n _rmtree_unsafe(fullname, onerror)\\n else:\\n try:\\n os.unlink(fullname)\\n except oserror:\\n onerror(os.unlink, fullname, sys.exc_info())\\n try:\\n os.rmdir(path)\\n except oserror:\\n onerror(os.rmdir, path, sys.exc_info())\\n # version using fd-based apis to protect against races\\n def _rmtree_safe_fd(topfd, path, onerror):\\n names = []\\n try:\\n names = os.listdir(topfd)\\n except oserror as err:\\n err.filename = path\\n onerror(os.listdir, path, sys.exc_info())\\n for name in names:\\n fullname = os.path.join(path, name)\\n try:\\n orig_st = os.stat(name, dir_fd=topfd, follow_symlinks=false)\\n mode = orig_st.st_mode\\n except oserror:\\n mode = 0\\n if stat.s_isdir(mode):\\n try:\\n dirfd = os.open(name, os.o_rdonly, dir_fd=topfd)\\n except oserror:\\n onerror(os.open, fullname, sys.exc_info())\\n else:\\n try:\\n if os.path.samestat(orig_st, os.fstat(dirfd)):\\n _rmtree_safe_fd(dirfd, fullname, onerror)\\n try:\\n os.rmdir(name, dir_fd=topfd)\\n except oserror:\\n onerror(os.rmdir, fullname, sys.exc_info())\\n else:\\n try:\\n # this can only happen if someone replaces\\n # a directory with a symlink after the call to\\n # stat.s_isdir above.\\n raise oserror(\"cannot call rmtree on a symbolic \"\\n \"link\")\\n except oserror:\\n onerror(os.path.islink, fullname, sys.exc_info())\\n finally:\\n os.close(dirfd)\\n else:\\n try:\\n os.unlink(name, dir_fd=topfd)\\n except oserror:\\n onerror(os.unlink, fullname, sys.exc_info())\\n _use_fd_functions = ({os.open, os.stat, os.unlink, os.rmdir} <=\\n os.supports_dir_fd and\\n os.listdir in os.supports_fd and\\n os.stat in os.supports_follow_symlinks)\\n def rmtree(path, ignore_errors=false, onerror=none):\\n \"\"\"recursively delete a directory tree.\\n if ignore_errors is set, errors are ignored; otherwise, if onerror\\n is set, it is called to handle the error with arguments (func,\\n path, exc_info) where func is platform and implementation dependent;\\n path is the argument to that function that caused it to fail; and\\n exc_info is a tuple returned by sys.exc_info().  if ignore_errors\\n is false and onerror is none, an exception is raised.\\n \"\"\"\\n if ignore_errors:\\n def onerror(*args):\\n pass\\n elif onerror is none:\\n def onerror(*args):\\n raise\\n if _use_fd_functions:\\n # while the unsafe rmtree works fine on bytes, the fd based does not.\\n if isinstance(path, bytes):\\n path = os.fsdecode(path)\\n # note: to guard against symlink races, we use the standard\\n # lstat()/open()/fstat() trick.\\n try:\\n orig_st = os.lstat(path)\\n except exception:\\n onerror(os.lstat, path, sys.exc_info())\\n return\\n try:\\n fd = os.open(path, os.o_rdonly)\\n except exception:\\n onerror(os.lstat, path, sys.exc_info())\\n return\\n try:\\n if os.path.samestat(orig_st, os.fstat(fd)):\\n _rmtree_safe_fd(fd, path, onerror)\\n try:\\n os.rmdir(path)\\n except oserror:\\n onerror(os.rmdir, path, sys.exc_info())\\n else:\\n try:\\n # symlinks to directories are forbidden, see bug #1669\\n raise oserror(\"cannot call rmtree on a symbolic link\")\\n except oserror:\\n onerror(os.path.islink, path, sys.exc_info())\\n finally:\\n os.close(fd)\\n else:\\n return _rmtree_unsafe(path, onerror)\\n # allow introspection of whether or not the hardening against symlink\\n # attacks is supported on the current platform\\n rmtree.avoids_symlink_attacks = _use_fd_functions\\n def _basename(path):\\n # a basename() variant which first strips the trailing slash, if present.\\n # thus we always get the last component of the path, even for directories.\\n sep = os.path.sep + (os.path.altsep or \\'\\')\\n return os.path.basename(path.rstrip(sep))\\n def move(src, dst, copy_function=copy2):\\n \"\"\"recursively move a file or directory to another location. this is\\n similar to the unix \"mv\" command. return the file or directory\\'s\\n destination.\\n if the destination is a directory or a symlink to a directory, the source\\n is moved inside the directory. the destination path must not already\\n exist.\\n if the destination already exists but is not a directory, it may be\\n overwritten depending on os.rename() semantics.\\n if the destination is on our current filesystem, then rename() is used.\\n otherwise, src is copied to the destination and then removed. symlinks are\\n recreated under the new name if os.rename() fails because of cross\\n filesystem renames.\\n the optional `copy_function` argument is a callable that will be used\\n to copy the source or it will be delegated to `copytree`.\\n by default, copy2() is used, but any function that supports the same\\n signature (like copy()) can be used.\\n a lot more could be done here...  a look at a mv.c shows a lot of\\n the issues this implementation glosses over.\\n \"\"\"\\n real_dst = dst\\n if os.path.isdir(dst):\\n if _samefile(src, dst):\\n # we might be on a case insensitive filesystem,\\n # perform the rename anyway.\\n os.rename(src, dst)\\n return\\n real_dst = os.path.join(dst, _basename(src))\\n if os.path.exists(real_dst):\\n raise error(\"destination path \\'%s\\' already exists\" % real_dst)\\n try:\\n os.rename(src, real_dst)\\n except oserror:\\n if os.path.islink(src):\\n linkto = os.readlink(src)\\n os.symlink(linkto, real_dst)\\n os.unlink(src)\\n elif os.path.isdir(src):\\n if _destinsrc(src, dst):\\n raise error(\"cannot move a directory \\'%s\\' into itself\"\\n \" \\'%s\\'.\" % (src, dst))\\n copytree(src, real_dst, copy_function=copy_function,\\n symlinks=true)\\n rmtree(src)\\n else:\\n copy_function(src, real_dst)\\n os.unlink(src)\\n return real_dst\\n def _destinsrc(src, dst):\\n src = os.path.abspath(src)\\n dst = os.path.abspath(dst)\\n if not src.endswith(os.path.sep):\\n src += os.path.sep\\n if not dst.endswith(os.path.sep):\\n dst += os.path.sep\\n return dst.startswith(src)\\n def _get_gid(name):\\n \"\"\"returns a gid, given a group name.\"\"\"\\n if getgrnam is none or name is none:\\n return none\\n try:\\n result = getgrnam(name)\\n except keyerror:\\n result = none\\n if result is not none:\\n return result[2]\\n return none\\n def _get_uid(name):\\n \"\"\"returns an uid, given a user name.\"\"\"\\n if getpwnam is none or name is none:\\n return none\\n try:\\n result = getpwnam(name)\\n except keyerror:\\n result = none\\n if result is not none:\\n return result[2]\\n return none\\n def _make_tarball(base_name, base_dir, compress=\"gzip\", verbose=0, dry_run=0,\\n owner=none, group=none, logger=none):\\n \"\"\"create a (possibly compressed) tar file from all the files under\\n \\'base_dir\\'.\\n \\'compress\\' must be \"gzip\" (the default), \"bzip2\", \"xz\", or none.\\n \\'owner\\' and \\'group\\' can be used to define an owner and a group for the\\n archive that is being built. if not provided, the current owner and group\\n will be used.\\n the output tar file will be named \\'base_name\\' +  \".tar\", possibly plus\\n the appropriate compression extension (\".gz\", \".bz2\", or \".xz\").\\n returns the output filename.\\n \"\"\"\\n tar_compression = {\\'gzip\\': \\'gz\\', none: \\'\\'}\\n compress_ext = {\\'gzip\\': \\'.gz\\'}\\n if _bz2_supported:\\n tar_compression[\\'bzip2\\'] = \\'bz2\\'\\n compress_ext[\\'bzip2\\'] = \\'.bz2\\'\\n if _lzma_supported:\\n tar_compression[\\'xz\\'] = \\'xz\\'\\n compress_ext[\\'xz\\'] = \\'.xz\\'\\n # flags for compression program, each element of list will be an argument\\n if compress is not none and compress not in compress_ext:\\n raise valueerror(\"bad value for \\'compress\\', or compression format not \"\\n \"supported : {0}\".format(compress))\\n archive_name = base_name + \\'.tar\\' + compress_ext.get(compress, \\'\\')\\n archive_dir = os.path.dirname(archive_name)\\n if archive_dir and not os.path.exists(archive_dir):\\n if logger is not none:\\n logger.info(\"creating %s\", archive_dir)\\n if not dry_run:\\n os.makedirs(archive_dir)\\n # creating the tarball\\n if logger is not none:\\n logger.info(\\'creating tar archive\\')\\n uid = _get_uid(owner)\\n gid = _get_gid(group)\\n def _set_uid_gid(tarinfo):\\n if gid is not none:\\n tarinfo.gid = gid\\n tarinfo.gname = group\\n if uid is not none:\\n tarinfo.uid = uid\\n tarinfo.uname = owner\\n return tarinfo\\n if not dry_run:\\n tar = tarfile.open(archive_name, \\'w|%s\\' % tar_compression[compress])\\n try:\\n tar.add(base_dir, filter=_set_uid_gid)\\n finally:\\n tar.close()\\n return archive_name\\n def _make_zipfile(base_name, base_dir, verbose=0, dry_run=0, logger=none):\\n \"\"\"create a zip file from all the files under \\'base_dir\\'.\\n the output zip file will be named \\'base_name\\' + \".zip\".  uses either the\\n \"zipfile\" python module (if available) or the infozip \"zip\" utility\\n (if installed and found on the default search path).  if neither tool is\\n available, raises execerror.  returns the name of the output zip\\n file.\\n \"\"\"\\n import zipfile\\n zip_filename = base_name + \".zip\"\\n archive_dir = os.path.dirname(base_name)\\n if archive_dir and not os.path.exists(archive_dir):\\n if logger is not none:\\n logger.info(\"creating %s\", archive_dir)\\n if not dry_run:\\n os.makedirs(archive_dir)\\n if logger is not none:\\n logger.info(\"creating \\'%s\\' and adding \\'%s\\' to it\",\\n zip_filename, base_dir)\\n if not dry_run:\\n with zipfile.zipfile(zip_filename, \"w\",\\n compression=zipfile.zip_deflated) as zf:\\n path = os.path.normpath(base_dir)\\n zf.write(path, path)\\n if logger is not none:\\n logger.info(\"adding \\'%s\\'\", path)\\n for dirpath, dirnames, filenames in os.walk(base_dir):\\n for name in sorted(dirnames):\\n path = os.path.normpath(os.path.join(dirpath, name))\\n zf.write(path, path)\\n if logger is not none:\\n logger.info(\"adding \\'%s\\'\", path)\\n for name in filenames:\\n path = os.path.normpath(os.path.join(dirpath, name))\\n if os.path.isfile(path):\\n zf.write(path, path)\\n if logger is not none:\\n logger.info(\"adding \\'%s\\'\", path)\\n return zip_filename\\n _archive_formats = {\\n \\'gztar\\': (_make_tarball, [(\\'compress\\', \\'gzip\\')], \"gzip\\'ed tar-file\"),\\n \\'tar\\':   (_make_tarball, [(\\'compress\\', none)], \"uncompressed tar file\"),\\n \\'zip\\':   (_make_zipfile, [], \"zip file\")\\n }\\n if _bz2_supported:\\n _archive_formats[\\'bztar\\'] = (_make_tarball, [(\\'compress\\', \\'bzip2\\')],\\n \"bzip2\\'ed tar-file\")\\n if _lzma_supported:\\n _archive_formats[\\'xztar\\'] = (_make_tarball, [(\\'compress\\', \\'xz\\')],\\n \"xz\\'ed tar-file\")\\n def get_archive_formats():\\n \"\"\"returns a list of supported formats for archiving and unarchiving.\\n each element of the returned sequence is a tuple (name, description)\\n \"\"\"\\n formats = [(name, registry[2]) for name, registry in\\n _archive_formats.items()]\\n formats.sort()\\n return formats\\n def register_archive_format(name, function, extra_args=none, description=\\'\\'):\\n \"\"\"registers an archive format.\\n name is the name of the format. function is the callable that will be\\n used to create archives. if provided, extra_args is a sequence of\\n (name, value) tuples that will be passed as arguments to the callable.\\n description can be provided to describe the format, and will be returned\\n by the get_archive_formats() function.\\n \"\"\"\\n if extra_args is none:\\n extra_args = []\\n if not callable(function):\\n raise typeerror(\\'the %s object is not callable\\' % function)\\n if not isinstance(extra_args, (tuple, list)):\\n raise typeerror(\\'extra_args needs to be a sequence\\')\\n for element in extra_args:\\n if not isinstance(element, (tuple, list)) or len(element) !=2:\\n raise typeerror(\\'extra_args elements are : (arg_name, value)\\')\\n _archive_formats[name] = (function, extra_args, description)\\n def unregister_archive_format(name):\\n del _archive_formats[name]\\n def make_archive(base_name, format, root_dir=none, base_dir=none, verbose=0,\\n dry_run=0, owner=none, group=none, logger=none):\\n \"\"\"create an archive file (eg. zip or tar).\\n \\'base_name\\' is the name of the file to create, minus any format-specific\\n extension; \\'format\\' is the archive format: one of \"zip\", \"tar\", \"bztar\"\\n or \"gztar\".\\n \\'root_dir\\' is a directory that will be the root directory of the\\n archive; ie. we typically chdir into \\'root_dir\\' before creating the\\n archive.  \\'base_dir\\' is the directory where we start archiving from;\\n ie. \\'base_dir\\' will be the common prefix of all files and\\n directories in the archive.  \\'root_dir\\' and \\'base_dir\\' both default\\n to the current directory.  returns the name of the archive file.\\n \\'owner\\' and \\'group\\' are used when creating a tar archive. by default,\\n uses the current owner and group.\\n \"\"\"\\n save_cwd = os.getcwd()\\n if root_dir is not none:\\n if logger is not none:\\n logger.debug(\"changing into \\'%s\\'\", root_dir)\\n base_name = os.path.abspath(base_name)\\n if not dry_run:\\n os.chdir(root_dir)\\n if base_dir is none:\\n base_dir = os.curdir\\n kwargs = {\\'dry_run\\': dry_run, \\'logger\\': logger}\\n try:\\n format_info = _archive_formats[format]\\n except keyerror:\\n raise valueerror(\"unknown archive format \\'%s\\'\" % format)\\n func = format_info[0]\\n for arg, val in format_info[1]:\\n kwargs[arg] = val\\n if format != \\'zip\\':\\n kwargs[\\'owner\\'] = owner\\n kwargs[\\'group\\'] = group\\n try:\\n filename = func(base_name, base_dir, **kwargs)\\n finally:\\n if root_dir is not none:\\n if logger is not none:\\n logger.debug(\"changing back to \\'%s\\'\", save_cwd)\\n os.chdir(save_cwd)\\n return filename\\n def get_unpack_formats():\\n \"\"\"returns a list of supported formats for unpacking.\\n each element of the returned sequence is a tuple\\n (name, extensions, description)\\n \"\"\"\\n formats = [(name, info[0], info[3]) for name, info in\\n _unpack_formats.items()]\\n formats.sort()\\n return formats\\n def _check_unpack_options(extensions, function, extra_args):\\n \"\"\"checks what gets registered as an unpacker.\"\"\"\\n # first make sure no other unpacker is registered for this extension\\n existing_extensions = {}\\n for name, info in _unpack_formats.items():\\n for ext in info[0]:\\n existing_extensions[ext] = name\\n for extension in extensions:\\n if extension in existing_extensions:\\n msg = \\'%s is already registered for \"%s\"\\'\\n raise registryerror(msg % (extension,\\n existing_extensions[extension]))\\n if not callable(function):\\n raise typeerror(\\'the registered function must be a callable\\')\\n def register_unpack_format(name, extensions, function, extra_args=none,\\n description=\\'\\'):\\n \"\"\"registers an unpack format.\\n `name` is the name of the format. `extensions` is a list of extensions\\n corresponding to the format.\\n `function` is the callable that will be\\n used to unpack archives. the callable will receive archives to unpack.\\n if it\\'s unable to handle an archive, it needs to raise a readerror\\n exception.\\n if provided, `extra_args` is a sequence of\\n (name, value) tuples that will be passed as arguments to the callable.\\n description can be provided to describe the format, and will be returned\\n by the get_unpack_formats() function.\\n \"\"\"\\n if extra_args is none:\\n extra_args = []\\n _check_unpack_options(extensions, function, extra_args)\\n _unpack_formats[name] = extensions, function, extra_args, description\\n def unregister_unpack_format(name):\\n \"\"\"removes the pack format from the registery.\"\"\"\\n del _unpack_formats[name]\\n def _ensure_directory(path):\\n \"\"\"ensure that the parent directory of `path` exists\"\"\"\\n dirname = os.path.dirname(path)\\n if not os.path.isdir(dirname):\\n os.makedirs(dirname)\\n def _unpack_zipfile(filename, extract_dir):\\n \"\"\"unpack zip `filename` to `extract_dir`\\n \"\"\"\\n try:\\n import zipfile\\n except importerror:\\n raise readerror(\\'zlib not supported, cannot unpack this archive.\\')\\n if not zipfile.is_zipfile(filename):\\n raise readerror(\"%s is not a zip file\" % filename)\\n zip = zipfile.zipfile(filename)\\n try:\\n for info in zip.infolist():\\n name = info.filename\\n # don\\'t extract absolute paths or ones with .. in them\\n if name.startswith(\\'/\\') or \\'..\\' in name:\\n continue\\n target = os.path.join(extract_dir, *name.split(\\'/\\'))\\n if not target:\\n continue\\n _ensure_directory(target)\\n if not name.endswith(\\'/\\'):\\n # file\\n data = zip.read(info.filename)\\n f = open(target, \\'wb\\')\\n try:\\n f.write(data)\\n finally:\\n f.close()\\n del data\\n finally:\\n zip.close()\\n def _unpack_tarfile(filename, extract_dir):\\n \"\"\"unpack tar/tar.gz/tar.bz2/tar.xz `filename` to `extract_dir`\\n \"\"\"\\n try:\\n tarobj = tarfile.open(filename)\\n except tarfile.tarerror:\\n raise readerror(\\n \"%s is not a compressed or uncompressed tar file\" % filename)\\n try:\\n tarobj.extractall(extract_dir)\\n finally:\\n tarobj.close()\\n _unpack_formats = {\\n \\'gztar\\': ([\\'.tar.gz\\', \\'.tgz\\'], _unpack_tarfile, [], \"gzip\\'ed tar-file\"),\\n \\'tar\\':   ([\\'.tar\\'], _unpack_tarfile, [], \"uncompressed tar file\"),\\n \\'zip\\':   ([\\'.zip\\'], _unpack_zipfile, [], \"zip file\")\\n }\\n if _bz2_supported:\\n _unpack_formats[\\'bztar\\'] = ([\\'.tar.bz2\\', \\'.tbz2\\'], _unpack_tarfile, [],\\n \"bzip2\\'ed tar-file\")\\n if _lzma_supported:\\n _unpack_formats[\\'xztar\\'] = ([\\'.tar.xz\\', \\'.txz\\'], _unpack_tarfile, [],\\n \"xz\\'ed tar-file\")\\n def _find_unpack_format(filename):\\n for name, info in _unpack_formats.items():\\n for extension in info[0]:\\n if filename.endswith(extension):\\n return name\\n return none\\n def unpack_archive(filename, extract_dir=none, format=none):\\n \"\"\"unpack an archive.\\n `filename` is the name of the archive.\\n `extract_dir` is the name of the target directory, where the archive\\n is unpacked. if not provided, the current working directory is used.\\n `format` is the archive format: one of \"zip\", \"tar\", or \"gztar\". or any\\n other registered format. if not provided, unpack_archive will use the\\n filename extension and see if an unpacker was registered for that\\n extension.\\n in case none is found, a valueerror is raised.\\n \"\"\"\\n if extract_dir is none:\\n extract_dir = os.getcwd()\\n if format is not none:\\n try:\\n format_info = _unpack_formats[format]\\n except keyerror:\\n raise valueerror(\"unknown unpack format \\'{0}\\'\".format(format))\\n func = format_info[1]\\n func(filename, extract_dir, **dict(format_info[2]))\\n else:\\n # we need to look at the registered unpackers supported extensions\\n format = _find_unpack_format(filename)\\n if format is none:\\n raise readerror(\"unknown archive format \\'{0}\\'\".format(filename))\\n func = _unpack_formats[format][1]\\n kwargs = dict(_unpack_formats[format][2])\\n func(filename, extract_dir, **kwargs)\\n if hasattr(os, \\'statvfs\\'):\\n __all__.append(\\'disk_usage\\')\\n _ntuple_diskusage = collections.namedtuple(\\'usage\\', \\'total used free\\')\\n def disk_usage(path):\\n \"\"\"return disk usage statistics about the given path.\\n returned value is a named tuple with attributes \\'total\\', \\'used\\' and\\n \\'free\\', which are the amount of total, used and free space, in bytes.\\n \"\"\"\\n st = os.statvfs(path)\\n free = st.f_bavail * st.f_frsize\\n total = st.f_blocks * st.f_frsize\\n used = (st.f_blocks - st.f_bfree) * st.f_frsize\\n return _ntuple_diskusage(total, used, free)\\n elif os.name == \\'nt\\':\\n import nt\\n __all__.append(\\'disk_usage\\')\\n _ntuple_diskusage = collections.namedtuple(\\'usage\\', \\'total used free\\')\\n def disk_usage(path):\\n \"\"\"return disk usage statistics about the given path.\\n returned values is a named tuple with attributes \\'total\\', \\'used\\' and\\n \\'free\\', which are the amount of total, used and free space, in bytes.\\n \"\"\"\\n total, free = nt._getdiskusage(path)\\n used = total - free\\n return _ntuple_diskusage(total, used, free)\\n def chown(path, user=none, group=none):\\n \"\"\"change owner user and group of the given path.\\n user and group can be the uid/gid or the user/group names, and in that case,\\n they are converted to their respective uid/gid.\\n \"\"\"\\n if user is none and group is none:\\n raise valueerror(\"user and/or group must be set\")\\n _user = user\\n _group = group\\n # -1 means don\\'t change it\\n if user is none:\\n _user = -1\\n # user can either be an int (the uid) or a string (the system username)\\n elif isinstance(user, str):\\n _user = _get_uid(user)\\n if _user is none:\\n raise lookuperror(\"no such user: {!r}\".format(user))\\n if group is none:\\n _group = -1\\n elif not isinstance(group, int):\\n _group = _get_gid(group)\\n if _group is none:\\n raise lookuperror(\"no such group: {!r}\".format(group))\\n os.chown(path, _user, _group)\\n def get_terminal_size(fallback=(80, 24)):\\n \"\"\"get the size of the terminal window.\\n for each of the two dimensions, the environment variable, columns\\n and lines respectively, is checked. if the variable is defined and\\n the value is a positive integer, it is used.\\n when columns or lines is not defined, which is the common case,\\n the terminal connected to sys.__stdout__ is queried\\n by invoking os.get_terminal_size.\\n if the terminal size cannot be successfully queried, either because\\n the system doesn\\'t support querying, or because we are not\\n connected to a terminal, the value given in fallback parameter\\n is used. fallback defaults to (80, 24) which is the default\\n size used by many terminal emulators.\\n the value returned is a named tuple of type os.terminal_size.\\n \"\"\"\\n # columns, lines are the working values\\n try:\\n columns = int(os.environ[\\'columns\\'])\\n except (keyerror, valueerror):\\n columns = 0\\n try:\\n lines = int(os.environ[\\'lines\\'])\\n except (keyerror, valueerror):\\n lines = 0\\n # only query if necessary\\n if columns <= 0 or lines <= 0:\\n try:\\n size = os.get_terminal_size(sys.__stdout__.fileno())\\n except (attributeerror, valueerror, oserror):\\n # stdout is none, closed, detached, or not a terminal, or\\n # os.get_terminal_size() is unsupported\\n size = os.terminal_size(fallback)\\n if columns <= 0:\\n columns = size.columns\\n if lines <= 0:\\n lines = size.lines\\n return os.terminal_size((columns, lines))\\n def which(cmd, mode=os.f_ok | os.x_ok, path=none):\\n \"\"\"given a command, mode, and a path string, return the path which\\n conforms to the given mode on the path, or none if there is no such\\n file.\\n `mode` defaults to os.f_ok | os.x_ok. `path` defaults to the result\\n of os.environ.get(\"path\"), or can be overridden with a custom search\\n path.\\n \"\"\"\\n # check that a given file can be accessed with the correct mode.\\n # additionally check that `file` is not a directory, as on windows\\n # directories pass the os.access check.\\n def _access_check(fn, mode):\\n return (os.path.exists(fn) and os.access(fn, mode)\\n and not os.path.isdir(fn))\\n # if we\\'re given a path with a directory part, look it up directly rather\\n # than referring to path directories. this includes checking relative to the\\n # current directory, e.g. ./script\\n if os.path.dirname(cmd):\\n if _access_check(cmd, mode):\\n return cmd\\n return none\\n if path is none:\\n path = os.environ.get(\"path\", os.defpath)\\n if not path:\\n return none\\n path = path.split(os.pathsep)\\n if sys.platform == \"win32\":\\n # the current directory takes precedence on windows.\\n if not os.curdir in path:\\n path.insert(0, os.curdir)\\n # pathext is necessary to check on windows.\\n pathext = os.environ.get(\"pathext\", \"\").split(os.pathsep)\\n # see if the given file matches any of the expected path extensions.\\n # this will allow us to short circuit when given \"python.exe\".\\n # if it does match, only test that one, otherwise we have to try\\n # others.\\n if any(cmd.lower().endswith(ext.lower()) for ext in pathext):\\n files = [cmd]\\n else:\\n files = [cmd + ext for ext in pathext]\\n else:\\n # on other platforms you don\\'t have things like pathext to tell you\\n # what file suffixes are executable, so just pass on cmd as-is.\\n files = [cmd]\\n seen = set()\\n for dir in path:\\n normdir = os.path.normcase(dir)\\n if not normdir in seen:\\n seen.add(normdir)\\n for thefile in files:\\n name = os.path.join(dir, thefile)\\n if _access_check(name, mode):\\n return name\\n return none\\n \"\"\"append module search paths for third-party packages to sys.path.\\n ****************************************************************\\n * this module is automatically imported during initialization. *\\n ****************************************************************\\n in earlier versions of python (up to 1.5a3), scripts or modules that\\n needed to use site-specific modules would place ``import site\\'\\'\\n somewhere near the top of their code.  because of the automatic\\n import, this is no longer necessary (but code that does it still\\n works).\\n this will append site-specific paths to the module search path.  on\\n unix, it starts with sys.prefix and sys.exec_prefix (if different) and\\n appends lib/python<version>/site-packages as well as lib/site-python.\\n it also supports the debian convention of\\n lib/python<version>/dist-packages.  on other platforms (mainly mac and\\n windows), it uses just sys.prefix (and sys.exec_prefix, if different,\\n but this is unlikely).  the resulting directories, if they exist, are\\n appended to sys.path, and also inspected for path configuration files.\\n for debian, this sys.path is augmented with directories in /usr/local.\\n local addons go into /usr/local/lib/python<version>/site-packages\\n (resp. /usr/local/lib/site-python), debian addons install into\\n /usr/{lib,share}/python<version>/dist-packages.\\n a path configuration file is a file whose name has the form\\n <package>.pth; its contents are additional directories (one per line)\\n to be added to sys.path.  non-existing directories (or\\n non-directories) are never added to sys.path; no directory is added to\\n sys.path more than once.  blank lines and lines beginning with\\n \\'#\\' are skipped. lines starting with \\'import\\' are executed.\\n for example, suppose sys.prefix and sys.exec_prefix are set to\\n /usr/local and there is a directory /usr/local/lib/python2.x/site-packages\\n with three subdirectories, foo, bar and spam, and two path\\n configuration files, foo.pth and bar.pth.  assume foo.pth contains the\\n following:\\n # foo package configuration\\n foo\\n bar\\n bletch\\n and bar.pth contains:\\n # bar package configuration\\n bar\\n then the following directories are added to sys.path, in this order:\\n /usr/local/lib/python2.x/site-packages/bar\\n /usr/local/lib/python2.x/site-packages/foo\\n note that bletch is omitted because it doesn\\'t exist; bar precedes foo\\n because bar.pth comes alphabetically before foo.pth; and spam is\\n omitted because it is not mentioned in either path configuration file.\\n after these path manipulations, an attempt is made to import a module\\n named sitecustomize, which can perform arbitrary additional\\n site-specific customizations.  if this import fails with an\\n importerror exception, it is silently ignored.\\n \"\"\"\\n import sys\\n import os\\n try:\\n import __builtin__ as builtins\\n except importerror:\\n import builtins\\n try:\\n set\\n except nameerror:\\n from sets import set as set\\n # prefixes for site-packages; add additional prefixes like /usr/local here\\n prefixes = [sys.prefix, sys.exec_prefix]\\n # enable per user site-packages directory\\n # set it to false to disable the feature or true to force the feature\\n enable_user_site = none\\n # for distutils.commands.install\\n user_site = none\\n user_base = none\\n _is_64bit = (getattr(sys, \\'maxsize\\', none) or getattr(sys, \\'maxint\\')) > 2**32\\n _is_pypy = hasattr(sys, \\'pypy_version_info\\')\\n _is_jython = sys.platform[:4] == \\'java\\'\\n if _is_jython:\\n moduletype = type(os)\\n def makepath(*paths):\\n dir = os.path.join(*paths)\\n if _is_jython and (dir == \\'__classpath__\\' or\\n dir.startswith(\\'__pyclasspath__\\')):\\n return dir, dir\\n dir = os.path.abspath(dir)\\n return dir, os.path.normcase(dir)\\n def abs__file__():\\n \"\"\"set all module\\' __file__ attribute to an absolute path\"\"\"\\n for m in sys.modules.values():\\n if ((_is_jython and not isinstance(m, moduletype)) or\\n hasattr(m, \\'__loader__\\')):\\n # only modules need the abspath in jython. and don\\'t mess\\n # with a pep 302-supplied __file__\\n continue\\n f = getattr(m, \\'__file__\\', none)\\n if f is none:\\n continue\\n m.__file__ = os.path.abspath(f)\\n def removeduppaths():\\n \"\"\" remove duplicate entries from sys.path along with making them\\n absolute\"\"\"\\n # this ensures that the initial path provided by the interpreter contains\\n # only absolute pathnames, even if we\\'re running from the build directory.\\n l = []\\n known_paths = set()\\n for dir in sys.path:\\n # filter out duplicate paths (on case-insensitive file systems also\\n # if they only differ in case); turn relative paths into absolute\\n # paths.\\n dir, dircase = makepath(dir)\\n if not dircase in known_paths:\\n l.append(dir)\\n known_paths.add(dircase)\\n sys.path[:] = l\\n return known_paths\\n # xxx this should not be part of site.py, since it is needed even when\\n # using the -s option for python.  see http://www.python.org/sf/586680\\n def addbuilddir():\\n \"\"\"append ./build/lib.<platform> in case we\\'re running in the build dir\\n (especially for guido :-)\"\"\"\\n from distutils.util import get_platform\\n s = \"build/lib.%s-%.3s\" % (get_platform(), sys.version)\\n if hasattr(sys, \\'gettotalrefcount\\'):\\n s += \\'-pydebug\\'\\n s = os.path.join(os.path.dirname(sys.path[-1]), s)\\n sys.path.append(s)\\n def _init_pathinfo():\\n \"\"\"return a set containing all existing directory entries from sys.path\"\"\"\\n d = set()\\n for dir in sys.path:\\n try:\\n if os.path.isdir(dir):\\n dir, dircase = makepath(dir)\\n d.add(dircase)\\n except typeerror:\\n continue\\n return d\\n def addpackage(sitedir, name, known_paths):\\n \"\"\"add a new path to known_paths by combining sitedir and \\'name\\' or execute\\n sitedir if it starts with \\'import\\'\"\"\"\\n if known_paths is none:\\n _init_pathinfo()\\n reset = 1\\n else:\\n reset = 0\\n fullname = os.path.join(sitedir, name)\\n try:\\n f = open(fullname, \"ru\")\\n except ioerror:\\n return\\n try:\\n for line in f:\\n if line.startswith(\"#\"):\\n continue\\n if line.startswith(\"import\"):\\n exec(line)\\n continue\\n line = line.rstrip()\\n dir, dircase = makepath(sitedir, line)\\n if not dircase in known_paths and os.path.exists(dir):\\n sys.path.append(dir)\\n known_paths.add(dircase)\\n finally:\\n f.close()\\n if reset:\\n known_paths = none\\n return known_paths\\n def addsitedir(sitedir, known_paths=none):\\n \"\"\"add \\'sitedir\\' argument to sys.path if missing and handle .pth files in\\n \\'sitedir\\'\"\"\"\\n if known_paths is none:\\n known_paths = _init_pathinfo()\\n reset = 1\\n else:\\n reset = 0\\n sitedir, sitedircase = makepath(sitedir)\\n if not sitedircase in known_paths:\\n sys.path.append(sitedir)        # add path component\\n try:\\n names = os.listdir(sitedir)\\n except os.error:\\n return\\n names.sort()\\n for name in names:\\n if name.endswith(os.extsep + \"pth\"):\\n addpackage(sitedir, name, known_paths)\\n if reset:\\n known_paths = none\\n return known_paths\\n def addsitepackages(known_paths, sys_prefix=sys.prefix, exec_prefix=sys.exec_prefix):\\n \"\"\"add site-packages (and possibly site-python) to sys.path\"\"\"\\n prefixes = [os.path.join(sys_prefix, \"local\"), sys_prefix]\\n if exec_prefix != sys_prefix:\\n prefixes.append(os.path.join(exec_prefix, \"local\"))\\n for prefix in prefixes:\\n if prefix:\\n if sys.platform in (\\'os2emx\\', \\'riscos\\') or _is_jython:\\n sitedirs = [os.path.join(prefix, \"lib\", \"site-packages\")]\\n elif _is_pypy:\\n sitedirs = [os.path.join(prefix, \\'site-packages\\')]\\n elif sys.platform == \\'darwin\\' and prefix == sys_prefix:\\n if prefix.startswith(\"/system/library/frameworks/\"): # apple\\'s python\\n sitedirs = [os.path.join(\"/library/python\", sys.version[:3], \"site-packages\"),\\n os.path.join(prefix, \"extras\", \"lib\", \"python\")]\\n else: # any other python distros on osx work this way\\n sitedirs = [os.path.join(prefix, \"lib\",\\n \"python\" + sys.version[:3], \"site-packages\")]\\n elif os.sep == \\'/\\':\\n sitedirs = [os.path.join(prefix,\\n \"lib\",\\n \"python\" + sys.version[:3],\\n \"site-packages\"),\\n os.path.join(prefix, \"lib\", \"site-python\"),\\n os.path.join(prefix, \"python\" + sys.version[:3], \"lib-dynload\")]\\n lib64_dir = os.path.join(prefix, \"lib64\", \"python\" + sys.version[:3], \"site-packages\")\\n if (os.path.exists(lib64_dir) and\\n os.path.realpath(lib64_dir) not in [os.path.realpath(p) for p in sitedirs]):\\n if _is_64bit:\\n sitedirs.insert(0, lib64_dir)\\n else:\\n sitedirs.append(lib64_dir)\\n try:\\n # sys.getobjects only available in --with-pydebug build\\n sys.getobjects\\n sitedirs.insert(0, os.path.join(sitedirs[0], \\'debug\\'))\\n except attributeerror:\\n pass\\n # debian-specific dist-packages directories:\\n sitedirs.append(os.path.join(prefix, \"local/lib\",\\n \"python\" + sys.version[:3],\\n \"dist-packages\"))\\n if sys.version[0] == \\'2\\':\\n sitedirs.append(os.path.join(prefix, \"lib\",\\n \"python\" + sys.version[:3],\\n \"dist-packages\"))\\n else:\\n sitedirs.append(os.path.join(prefix, \"lib\",\\n \"python\" + sys.version[0],\\n \"dist-packages\"))\\n sitedirs.append(os.path.join(prefix, \"lib\", \"dist-python\"))\\n else:\\n sitedirs = [prefix, os.path.join(prefix, \"lib\", \"site-packages\")]\\n if sys.platform == \\'darwin\\':\\n # for framework builds *only* we add the standard apple\\n # locations. currently only per-user, but /library and\\n # /network/library could be added too\\n if \\'python.framework\\' in prefix:\\n home = os.environ.get(\\'home\\')\\n if home:\\n sitedirs.append(\\n os.path.join(home,\\n \\'library\\',\\n \\'python\\',\\n sys.version[:3],\\n \\'site-packages\\'))\\n for sitedir in sitedirs:\\n if os.path.isdir(sitedir):\\n addsitedir(sitedir, known_paths)\\n return none\\n def check_enableusersite():\\n \"\"\"check if user site directory is safe for inclusion\\n the function tests for the command line flag (including environment var),\\n process uid/gid equal to effective uid/gid.\\n none: disabled for security reasons\\n false: disabled by user (command line option)\\n true: safe and enabled\\n \"\"\"\\n if hasattr(sys, \\'flags\\') and getattr(sys.flags, \\'no_user_site\\', false):\\n return false\\n if hasattr(os, \"getuid\") and hasattr(os, \"geteuid\"):\\n # check process uid == effective uid\\n if os.geteuid() != os.getuid():\\n return none\\n if hasattr(os, \"getgid\") and hasattr(os, \"getegid\"):\\n # check process gid == effective gid\\n if os.getegid() != os.getgid():\\n return none\\n return true\\n def addusersitepackages(known_paths):\\n \"\"\"add a per user site-package to sys.path\\n each user has its own python directory with site-packages in the\\n home directory.\\n user_base is the root directory for all python versions\\n user_site is the user specific site-packages directory\\n user_site/.. can be used for data.\\n \"\"\"\\n global user_base, user_site, enable_user_site\\n env_base = os.environ.get(\"pythonuserbase\", none)\\n def joinuser(*args):\\n return os.path.expanduser(os.path.join(*args))\\n #if sys.platform in (\\'os2emx\\', \\'riscos\\'):\\n #    # don\\'t know what to put here\\n #    user_base = \\'\\'\\n #    user_site = \\'\\'\\n if os.name == \"nt\":\\n base = os.environ.get(\"appdata\") or \"~\"\\n if env_base:\\n user_base = env_base\\n else:\\n user_base = joinuser(base, \"python\")\\n user_site = os.path.join(user_base,\\n \"python\" + sys.version[0] + sys.version[2],\\n \"site-packages\")\\n else:\\n if env_base:\\n user_base = env_base\\n else:\\n user_base = joinuser(\"~\", \".local\")\\n user_site = os.path.join(user_base, \"lib\",\\n \"python\" + sys.version[:3],\\n \"site-packages\")\\n if enable_user_site and os.path.isdir(user_site):\\n addsitedir(user_site, known_paths)\\n if enable_user_site:\\n for dist_libdir in (\"lib\", \"local/lib\"):\\n user_site = os.path.join(user_base, dist_libdir,\\n \"python\" + sys.version[:3],\\n \"dist-packages\")\\n if os.path.isdir(user_site):\\n addsitedir(user_site, known_paths)\\n return known_paths\\n def setbeginlibpath():\\n \"\"\"the os/2 emx port has optional extension modules that do double duty\\n as dlls (and must use the .dll file extension) for other extensions.\\n the library search path needs to be amended so these will be found\\n during module import.  use beginlibpath so that these are at the start\\n of the library search path.\\n \"\"\"\\n dllpath = os.path.join(sys.prefix, \"lib\", \"lib-dynload\")\\n libpath = os.environ[\\'beginlibpath\\'].split(\\';\\')\\n if libpath[-1]:\\n libpath.append(dllpath)\\n else:\\n libpath[-1] = dllpath\\n os.environ[\\'beginlibpath\\'] = \\';\\'.join(libpath)\\n def setquit():\\n \"\"\"define new built-ins \\'quit\\' and \\'exit\\'.\\n these are simply strings that display a hint on how to exit.\\n \"\"\"\\n if os.sep == \\':\\':\\n eof = \\'cmd-q\\'\\n elif os.sep == \\'\\\\\\\\\\':\\n eof = \\'ctrl-z plus return\\'\\n else:\\n eof = \\'ctrl-d (i.e. eof)\\'\\n class quitter(object):\\n def __init__(self, name):\\n self.name = name\\n def __repr__(self):\\n return \\'use %s() or %s to exit\\' % (self.name, eof)\\n def __call__(self, code=none):\\n # shells like idle catch the systemexit, but listen when their\\n # stdin wrapper is closed.\\n try:\\n sys.stdin.close()\\n except:\\n pass\\n raise systemexit(code)\\n builtins.quit = quitter(\\'quit\\')\\n builtins.exit = quitter(\\'exit\\')\\n class _printer(object):\\n \"\"\"interactive prompt objects for printing the license text, a list of\\n contributors and the copyright notice.\"\"\"\\n maxlines = 23\\n def __init__(self, name, data, files=(), dirs=()):\\n self.__name = name\\n self.__data = data\\n self.__files = files\\n self.__dirs = dirs\\n self.__lines = none\\n def __setup(self):\\n if self.__lines:\\n return\\n data = none\\n for dir in self.__dirs:\\n for filename in self.__files:\\n filename = os.path.join(dir, filename)\\n try:\\n fp = open(filename, \"ru\")\\n data = fp.read()\\n fp.close()\\n break\\n except ioerror:\\n pass\\n if data:\\n break\\n if not data:\\n data = self.__data\\n self.__lines = data.split(\\'\\\\n\\')\\n self.__linecnt = len(self.__lines)\\n def __repr__(self):\\n self.__setup()\\n if len(self.__lines) <= self.maxlines:\\n return \"\\\\n\".join(self.__lines)\\n else:\\n return \"type %s() to see the full %s text\" % ((self.__name,)*2)\\n def __call__(self):\\n self.__setup()\\n prompt = \\'hit return for more, or q (and return) to quit: \\'\\n lineno = 0\\n while 1:\\n try:\\n for i in range(lineno, lineno + self.maxlines):\\n print(self.__lines[i])\\n except indexerror:\\n break\\n else:\\n lineno += self.maxlines\\n key = none\\n while key is none:\\n try:\\n key = raw_input(prompt)\\n except nameerror:\\n key = input(prompt)\\n if key not in (\\'\\', \\'q\\'):\\n key = none\\n if key == \\'q\\':\\n break\\n def setcopyright():\\n \"\"\"set \\'copyright\\' and \\'credits\\' in __builtin__\"\"\"\\n builtins.copyright = _printer(\"copyright\", sys.copyright)\\n if _is_jython:\\n builtins.credits = _printer(\\n \"credits\",\\n \"jython is maintained by the jython developers (www.jython.org).\")\\n elif _is_pypy:\\n builtins.credits = _printer(\\n \"credits\",\\n \"pypy is maintained by the pypy developers: http://pypy.org/\")\\n else:\\n builtins.credits = _printer(\"credits\", \"\"\"\\\\\\n thanks to cwi, cnri, beopen.com, zope corporation and a cast of thousands\\n for supporting python development.  see www.python.org for more information.\"\"\")\\n here = os.path.dirname(os.__file__)\\n builtins.license = _printer(\\n \"license\", \"see http://www.python.org/%.3s/license.html\" % sys.version,\\n [\"license.txt\", \"license\"],\\n [os.path.join(here, os.pardir), here, os.curdir])\\n class _helper(object):\\n \"\"\"define the built-in \\'help\\'.\\n this is a wrapper around pydoc.help (with a twist).\\n \"\"\"\\n def __repr__(self):\\n return \"type help() for interactive help, \" \\\\\\n \"or help(object) for help about object.\"\\n def __call__(self, *args, **kwds):\\n import pydoc\\n return pydoc.help(*args, **kwds)\\n def sethelper():\\n builtins.help = _helper()\\n def aliasmbcs():\\n \"\"\"on windows, some default encodings are not provided by python,\\n while they are always available as \"mbcs\" in each locale. make\\n them usable by aliasing to \"mbcs\" in such a case.\"\"\"\\n if sys.platform == \\'win32\\':\\n import locale, codecs\\n enc = locale.getdefaultlocale()[1]\\n if enc.startswith(\\'cp\\'):            # \"cp***\" ?\\n try:\\n codecs.lookup(enc)\\n except lookuperror:\\n import encodings\\n encodings._cache[enc] = encodings._unknown\\n encodings.aliases.aliases[enc] = \\'mbcs\\'\\n def setencoding():\\n \"\"\"set the string encoding used by the unicode implementation.  the\\n default is \\'ascii\\', but if you\\'re willing to experiment, you can\\n change this.\"\"\"\\n encoding = \"ascii\" # default value set by _pyunicode_init()\\n if 0:\\n # enable to support locale aware default string encodings.\\n import locale\\n loc = locale.getdefaultlocale()\\n if loc[1]:\\n encoding = loc[1]\\n if 0:\\n # enable to switch off string to unicode coercion and implicit\\n # unicode to string conversion.\\n encoding = \"undefined\"\\n if encoding != \"ascii\":\\n # on non-unicode builds this will raise an attributeerror...\\n sys.setdefaultencoding(encoding) # needs python unicode build !\\n def execsitecustomize():\\n \"\"\"run custom site specific code, if available.\"\"\"\\n try:\\n import sitecustomize\\n except importerror:\\n pass\\n def virtual_install_main_packages():\\n f = open(os.path.join(os.path.dirname(__file__), \\'orig-prefix.txt\\'))\\n sys.real_prefix = f.read().strip()\\n f.close()\\n pos = 2\\n hardcoded_relative_dirs = []\\n if sys.path[0] == \\'\\':\\n pos += 1\\n if _is_jython:\\n paths = [os.path.join(sys.real_prefix, \\'lib\\')]\\n elif _is_pypy:\\n if sys.version_info > (3, 2):\\n cpyver = \\'%d\\' % sys.version_info[0]\\n elif sys.pypy_version_info >= (1, 5):\\n cpyver = \\'%d.%d\\' % sys.version_info[:2]\\n else:\\n cpyver = \\'%d.%d.%d\\' % sys.version_info[:3]\\n paths = [os.path.join(sys.real_prefix, \\'lib_pypy\\'),\\n os.path.join(sys.real_prefix, \\'lib-python\\', cpyver)]\\n if sys.pypy_version_info < (1, 9):\\n paths.insert(1, os.path.join(sys.real_prefix,\\n \\'lib-python\\', \\'modified-%s\\' % cpyver))\\n hardcoded_relative_dirs = paths[:] # for the special \\'darwin\\' case below\\n #\\n # this is hardcoded in the python executable, but relative to sys.prefix:\\n for path in paths[:]:\\n plat_path = os.path.join(path, \\'plat-%s\\' % sys.platform)\\n if os.path.exists(plat_path):\\n paths.append(plat_path)\\n elif sys.platform == \\'win32\\':\\n paths = [os.path.join(sys.real_prefix, \\'lib\\'), os.path.join(sys.real_prefix, \\'dlls\\')]\\n else:\\n paths = [os.path.join(sys.real_prefix, \\'lib\\', \\'python\\'+sys.version[:3])]\\n hardcoded_relative_dirs = paths[:] # for the special \\'darwin\\' case below\\n lib64_path = os.path.join(sys.real_prefix, \\'lib64\\', \\'python\\'+sys.version[:3])\\n if os.path.exists(lib64_path):\\n if _is_64bit:\\n paths.insert(0, lib64_path)\\n else:\\n paths.append(lib64_path)\\n # this is hardcoded in the python executable, but relative to\\n # sys.prefix.  debian change: we need to add the multiarch triplet\\n # here, which is where the real stuff lives.  as per pep 421, in\\n # python 3.3+, this lives in sys.implementation, while in python 2.7\\n # it lives in sys.\\n try:\\n arch = getattr(sys, \\'implementation\\', sys)._multiarch\\n except attributeerror:\\n # this is a non-multiarch aware python.  fallback to the old way.\\n arch = sys.platform\\n plat_path = os.path.join(sys.real_prefix, \\'lib\\',\\n \\'python\\'+sys.version[:3],\\n \\'plat-%s\\' % arch)\\n if os.path.exists(plat_path):\\n paths.append(plat_path)\\n # this is hardcoded in the python executable, but\\n # relative to sys.prefix, so we have to fix up:\\n for path in list(paths):\\n tk_dir = os.path.join(path, \\'lib-tk\\')\\n if os.path.exists(tk_dir):\\n paths.append(tk_dir)\\n # these are hardcoded in the apple\\'s python executable,\\n # but relative to sys.prefix, so we have to fix them up:\\n if sys.platform == \\'darwin\\':\\n hardcoded_paths = [os.path.join(relative_dir, module)\\n for relative_dir in hardcoded_relative_dirs\\n for module in (\\'plat-darwin\\', \\'plat-mac\\', \\'plat-mac/lib-scriptpackages\\')]\\n for path in hardcoded_paths:\\n if os.path.exists(path):\\n paths.append(path)\\n sys.path.extend(paths)\\n def force_global_eggs_after_local_site_packages():\\n \"\"\"\\n force easy_installed eggs in the global environment to get placed\\n in sys.path after all packages inside the virtualenv.  this\\n maintains the \"least surprise\" result that packages in the\\n virtualenv always mask global packages, never the other way\\n around.\\n \"\"\"\\n egginsert = getattr(sys, \\'__egginsert\\', 0)\\n for i, path in enumerate(sys.path):\\n if i > egginsert and path.startswith(sys.prefix):\\n egginsert = i\\n sys.__egginsert = egginsert + 1\\n def virtual_addsitepackages(known_paths):\\n force_global_eggs_after_local_site_packages()\\n return addsitepackages(known_paths, sys_prefix=sys.real_prefix)\\n def fixclasspath():\\n \"\"\"adjust the special classpath sys.path entries for jython. these\\n entries should follow the base virtualenv lib directories.\\n \"\"\"\\n paths = []\\n classpaths = []\\n for path in sys.path:\\n if path == \\'__classpath__\\' or path.startswith(\\'__pyclasspath__\\'):\\n classpaths.append(path)\\n else:\\n paths.append(path)\\n sys.path = paths\\n sys.path.extend(classpaths)\\n def execusercustomize():\\n \"\"\"run custom user specific code, if available.\"\"\"\\n try:\\n import usercustomize\\n except importerror:\\n pass\\n def main():\\n global enable_user_site\\n virtual_install_main_packages()\\n abs__file__()\\n paths_in_sys = removeduppaths()\\n if (os.name == \"posix\" and sys.path and\\n os.path.basename(sys.path[-1]) == \"modules\"):\\n addbuilddir()\\n if _is_jython:\\n fixclasspath()\\n global_site_packages = not os.path.exists(os.path.join(os.path.dirname(__file__), \\'no-global-site-packages.txt\\'))\\n if not global_site_packages:\\n enable_user_site = false\\n if enable_user_site is none:\\n enable_user_site = check_enableusersite()\\n paths_in_sys = addsitepackages(paths_in_sys)\\n paths_in_sys = addusersitepackages(paths_in_sys)\\n if global_site_packages:\\n paths_in_sys = virtual_addsitepackages(paths_in_sys)\\n if sys.platform == \\'os2emx\\':\\n setbeginlibpath()\\n setquit()\\n setcopyright()\\n sethelper()\\n aliasmbcs()\\n setencoding()\\n execsitecustomize()\\n if enable_user_site:\\n execusercustomize()\\n # remove sys.setdefaultencoding() so that users cannot change the\\n # encoding after initialization.  the test for presence is needed when\\n # this module is run as a script, because this code is executed twice.\\n if hasattr(sys, \"setdefaultencoding\"):\\n del sys.setdefaultencoding\\n main()\\n def _script():\\n help = \"\"\"\\\\\\n %s [--user-base] [--user-site]\\n without arguments print some useful information\\n with arguments print the value of user_base and/or user_site separated\\n by \\'%s\\'.\\n exit codes with --user-base or --user-site:\\n 0 - user site directory is enabled\\n 1 - user site directory is disabled by user\\n 2 - uses site directory is disabled by super user\\n or for security reasons\\n >2 - unknown error\\n \"\"\"\\n args = sys.argv[1:]\\n if not args:\\n print(\"sys.path = [\")\\n for dir in sys.path:\\n print(\"    %r,\" % (dir,))\\n print(\"]\")\\n def exists(path):\\n if os.path.isdir(path):\\n return \"exists\"\\n else:\\n return \"doesn\\'t exist\"\\n print(\"user_base: %r (%s)\" % (user_base, exists(user_base)))\\n print(\"user_site: %r (%s)\" % (user_site, exists(user_base)))\\n print(\"enable_user_site: %r\" %  enable_user_site)\\n sys.exit(0)\\n buffer = []\\n if \\'--user-base\\' in args:\\n buffer.append(user_base)\\n if \\'--user-site\\' in args:\\n buffer.append(user_site)\\n if buffer:\\n print(os.pathsep.join(buffer))\\n if enable_user_site:\\n sys.exit(0)\\n elif enable_user_site is false:\\n sys.exit(1)\\n elif enable_user_site is none:\\n sys.exit(2)\\n else:\\n sys.exit(3)\\n else:\\n import textwrap\\n print(textwrap.dedent(help % (sys.argv[0], os.pathsep)))\\n sys.exit(10)\\n if __name__ == \\'__main__\\':\\n _script()\\n #\\n # secret labs\\' regular expression engine\\n #\\n # convert template to internal format\\n #\\n # copyright (c) 1997-2001 by secret labs ab.  all rights reserved.\\n #\\n # see the sre.py file for information on usage and redistribution.\\n #\\n \"\"\"internal support module for sre\"\"\"\\n import _sre\\n import sre_parse\\n from sre_constants import *\\n assert _sre.magic == magic, \"sre module mismatch\"\\n _literal_codes = {literal, not_literal}\\n _repeating_codes = {repeat, min_repeat, max_repeat}\\n _success_codes = {success, failure}\\n _assert_codes = {assert, assert_not}\\n # sets of lowercase characters which have the same uppercase.\\n _equivalences = (\\n # latin small letter i, latin small letter dotless i\\n (0x69, 0x131), # i\\n # latin small letter s, latin small letter long s\\n (0x73, 0x17f), # s\\n # micro sign, greek small letter mu\\n (0xb5, 0x3bc), # \\n # combining greek ypogegrammeni, greek small letter iota, greek prosgegrammeni\\n (0x345, 0x3b9, 0x1fbe), # \\\\u0345\\n # greek small letter iota with dialytika and tonos, greek small letter iota with dialytika and oxia\\n (0x390, 0x1fd3), # \\n # greek small letter upsilon with dialytika and tonos, greek small letter upsilon with dialytika and oxia\\n (0x3b0, 0x1fe3), # \\n # greek small letter beta, greek beta symbol\\n (0x3b2, 0x3d0), # \\n # greek small letter epsilon, greek lunate epsilon symbol\\n (0x3b5, 0x3f5), # \\n # greek small letter theta, greek theta symbol\\n (0x3b8, 0x3d1), # \\n # greek small letter kappa, greek kappa symbol\\n (0x3ba, 0x3f0), # \\n # greek small letter pi, greek pi symbol\\n (0x3c0, 0x3d6), # \\n # greek small letter rho, greek rho symbol\\n (0x3c1, 0x3f1), # \\n # greek small letter final sigma, greek small letter sigma\\n (0x3c2, 0x3c3), # \\n # greek small letter phi, greek phi symbol\\n (0x3c6, 0x3d5), # \\n # latin small letter s with dot above, latin small letter long s with dot above\\n (0x1e61, 0x1e9b), # \\n # latin small ligature long s t, latin small ligature st\\n (0xfb05, 0xfb06), # \\n )\\n # maps the lowercase code to lowercase codes which have the same uppercase.\\n _ignorecase_fixes = {i: tuple(j for j in t if i != j)\\n for t in _equivalences for i in t}\\n def _compile(code, pattern, flags):\\n # internal: compile a (sub)pattern\\n emit = code.append\\n _len = len\\n literal_codes = _literal_codes\\n repeating_codes = _repeating_codes\\n success_codes = _success_codes\\n assert_codes = _assert_codes\\n if (flags & sre_flag_ignorecase and\\n not (flags & sre_flag_locale) and\\n flags & sre_flag_unicode):\\n fixes = _ignorecase_fixes\\n else:\\n fixes = none\\n for op, av in pattern:\\n if op in literal_codes:\\n if flags & sre_flag_ignorecase:\\n lo = _sre.getlower(av, flags)\\n if fixes and lo in fixes:\\n emit(in_ignore)\\n skip = _len(code); emit(0)\\n if op is not_literal:\\n emit(negate)\\n for k in (lo,) + fixes[lo]:\\n emit(literal)\\n emit(k)\\n emit(failure)\\n code[skip] = _len(code) - skip\\n else:\\n emit(op_ignore[op])\\n emit(lo)\\n else:\\n emit(op)\\n emit(av)\\n elif op is in:\\n if flags & sre_flag_ignorecase:\\n emit(op_ignore[op])\\n def fixup(literal, flags=flags):\\n return _sre.getlower(literal, flags)\\n else:\\n emit(op)\\n fixup = none\\n skip = _len(code); emit(0)\\n _compile_charset(av, flags, code, fixup, fixes)\\n code[skip] = _len(code) - skip\\n elif op is any:\\n if flags & sre_flag_dotall:\\n emit(any_all)\\n else:\\n emit(any)\\n elif op in repeating_codes:\\n if flags & sre_flag_template:\\n raise error(\"internal: unsupported template operator %r\" % (op,))\\n elif _simple(av) and op is not repeat:\\n if op is max_repeat:\\n emit(repeat_one)\\n else:\\n emit(min_repeat_one)\\n skip = _len(code); emit(0)\\n emit(av[0])\\n emit(av[1])\\n _compile(code, av[2], flags)\\n emit(success)\\n code[skip] = _len(code) - skip\\n else:\\n emit(repeat)\\n skip = _len(code); emit(0)\\n emit(av[0])\\n emit(av[1])\\n _compile(code, av[2], flags)\\n code[skip] = _len(code) - skip\\n if op is max_repeat:\\n emit(max_until)\\n else:\\n emit(min_until)\\n elif op is subpattern:\\n if av[0]:\\n emit(mark)\\n emit((av[0]-1)*2)\\n # _compile_info(code, av[1], flags)\\n _compile(code, av[1], flags)\\n if av[0]:\\n emit(mark)\\n emit((av[0]-1)*2+1)\\n elif op in success_codes:\\n emit(op)\\n elif op in assert_codes:\\n emit(op)\\n skip = _len(code); emit(0)\\n if av[0] >= 0:\\n emit(0) # look ahead\\n else:\\n lo, hi = av[1].getwidth()\\n if lo != hi:\\n raise error(\"look-behind requires fixed-width pattern\")\\n emit(lo) # look behind\\n _compile(code, av[1], flags)\\n emit(success)\\n code[skip] = _len(code) - skip\\n elif op is call:\\n emit(op)\\n skip = _len(code); emit(0)\\n _compile(code, av, flags)\\n emit(success)\\n code[skip] = _len(code) - skip\\n elif op is at:\\n emit(op)\\n if flags & sre_flag_multiline:\\n av = at_multiline.get(av, av)\\n if flags & sre_flag_locale:\\n av = at_locale.get(av, av)\\n elif flags & sre_flag_unicode:\\n av = at_unicode.get(av, av)\\n emit(av)\\n elif op is branch:\\n emit(op)\\n tail = []\\n tailappend = tail.append\\n for av in av[1]:\\n skip = _len(code); emit(0)\\n # _compile_info(code, av, flags)\\n _compile(code, av, flags)\\n emit(jump)\\n tailappend(_len(code)); emit(0)\\n code[skip] = _len(code) - skip\\n emit(failure) # end of branch\\n for tail in tail:\\n code[tail] = _len(code) - tail\\n elif op is category:\\n emit(op)\\n if flags & sre_flag_locale:\\n av = ch_locale[av]\\n elif flags & sre_flag_unicode:\\n av = ch_unicode[av]\\n emit(av)\\n elif op is groupref:\\n if flags & sre_flag_ignorecase:\\n emit(op_ignore[op])\\n else:\\n emit(op)\\n emit(av-1)\\n elif op is groupref_exists:\\n emit(op)\\n emit(av[0]-1)\\n skipyes = _len(code); emit(0)\\n _compile(code, av[1], flags)\\n if av[2]:\\n emit(jump)\\n skipno = _len(code); emit(0)\\n code[skipyes] = _len(code) - skipyes + 1\\n _compile(code, av[2], flags)\\n code[skipno] = _len(code) - skipno\\n else:\\n code[skipyes] = _len(code) - skipyes + 1\\n else:\\n raise error(\"internal: unsupported operand type %r\" % (op,))\\n def _compile_charset(charset, flags, code, fixup=none, fixes=none):\\n # compile charset subprogram\\n emit = code.append\\n for op, av in _optimize_charset(charset, fixup, fixes):\\n emit(op)\\n if op is negate:\\n pass\\n elif op is literal:\\n emit(av)\\n elif op is range or op is range_ignore:\\n emit(av[0])\\n emit(av[1])\\n elif op is charset:\\n code.extend(av)\\n elif op is bigcharset:\\n code.extend(av)\\n elif op is category:\\n if flags & sre_flag_locale:\\n emit(ch_locale[av])\\n elif flags & sre_flag_unicode:\\n emit(ch_unicode[av])\\n else:\\n emit(av)\\n else:\\n raise error(\"internal: unsupported set operator %r\" % (op,))\\n emit(failure)\\n def _optimize_charset(charset, fixup, fixes):\\n # internal: optimize character set\\n out = []\\n tail = []\\n charmap = bytearray(256)\\n for op, av in charset:\\n while true:\\n try:\\n if op is literal:\\n if fixup:\\n lo = fixup(av)\\n charmap[lo] = 1\\n if fixes and lo in fixes:\\n for k in fixes[lo]:\\n charmap[k] = 1\\n else:\\n charmap[av] = 1\\n elif op is range:\\n r = range(av[0], av[1]+1)\\n if fixup:\\n r = map(fixup, r)\\n if fixup and fixes:\\n for i in r:\\n charmap[i] = 1\\n if i in fixes:\\n for k in fixes[i]:\\n charmap[k] = 1\\n else:\\n for i in r:\\n charmap[i] = 1\\n elif op is negate:\\n out.append((op, av))\\n else:\\n tail.append((op, av))\\n except indexerror:\\n if len(charmap) == 256:\\n # character set contains non-ucs1 character codes\\n charmap += b\\'\\\\0\\' * 0xff00\\n continue\\n # character set contains non-bmp character codes.\\n # there are only two ranges of cased non-bmp characters:\\n # 10400-1044f (deseret) and 118a0-118df (warang citi),\\n # and for both ranges range_ignore works.\\n if fixup and op is range:\\n op = range_ignore\\n tail.append((op, av))\\n break\\n # compress character map\\n runs = []\\n q = 0\\n while true:\\n p = charmap.find(1, q)\\n if p < 0:\\n break\\n if len(runs) >= 2:\\n runs = none\\n break\\n q = charmap.find(0, p)\\n if q < 0:\\n runs.append((p, len(charmap)))\\n break\\n runs.append((p, q))\\n if runs is not none:\\n # use literal/range\\n for p, q in runs:\\n if q - p == 1:\\n out.append((literal, p))\\n else:\\n out.append((range, (p, q - 1)))\\n out += tail\\n # if the case was changed or new representation is more compact\\n if fixup or len(out) < len(charset):\\n return out\\n # else original character set is good enough\\n return charset\\n # use bitmap\\n if len(charmap) == 256:\\n data = _mk_bitmap(charmap)\\n out.append((charset, data))\\n out += tail\\n return out\\n # to represent a big charset, first a bitmap of all characters in the\\n # set is constructed. then, this bitmap is sliced into chunks of 256\\n # characters, duplicate chunks are eliminated, and each chunk is\\n # given a number. in the compiled expression, the charset is\\n # represented by a 32-bit word sequence, consisting of one word for\\n # the number of different chunks, a sequence of 256 bytes (64 words)\\n # of chunk numbers indexed by their original chunk position, and a\\n # sequence of 256-bit chunks (8 words each).\\n # compression is normally good: in a typical charset, large ranges of\\n # unicode will be either completely excluded (e.g. if only cyrillic\\n # letters are to be matched), or completely included (e.g. if large\\n # subranges of kanji match). these ranges will be represented by\\n # chunks of all one-bits or all zero-bits.\\n # matching can be also done efficiently: the more significant byte of\\n # the unicode character is an index into the chunk number, and the\\n # less significant byte is a bit index in the chunk (just like the\\n # charset matching).\\n charmap = bytes(charmap) # should be hashable\\n comps = {}\\n mapping = bytearray(256)\\n block = 0\\n data = bytearray()\\n for i in range(0, 65536, 256):\\n chunk = charmap[i: i + 256]\\n if chunk in comps:\\n mapping[i // 256] = comps[chunk]\\n else:\\n mapping[i // 256] = comps[chunk] = block\\n block += 1\\n data += chunk\\n data = _mk_bitmap(data)\\n data[0:0] = [block] + _bytes_to_codes(mapping)\\n out.append((bigcharset, data))\\n out += tail\\n return out\\n _codebits = _sre.codesize * 8\\n maxcode = (1 << _codebits) - 1\\n _bits_trans = b\\'0\\' + b\\'1\\' * 255\\n def _mk_bitmap(bits, _codebits=_codebits, _int=int):\\n s = bits.translate(_bits_trans)[::-1]\\n return [_int(s[i - _codebits: i], 2)\\n for i in range(len(s), 0, -_codebits)]\\n def _bytes_to_codes(b):\\n # convert block indices to word array\\n a = memoryview(b).cast(\\'i\\')\\n assert a.itemsize == _sre.codesize\\n assert len(a) * a.itemsize == len(b)\\n return a.tolist()\\n def _simple(av):\\n # check if av is a \"simple\" operator\\n lo, hi = av[2].getwidth()\\n return lo == hi == 1 and av[2][0][0] != subpattern\\n def _generate_overlap_table(prefix):\\n \"\"\"\\n generate an overlap table for the following prefix.\\n an overlap table is a table of the same size as the prefix which\\n informs about the potential self-overlap for each index in the prefix:\\n - if overlap[i] == 0, prefix[i:] can\\'t overlap prefix[0:...]\\n - if overlap[i] == k with 0 < k <= i, prefix[i-k+1:i+1] overlaps with\\n prefix[0:k]\\n \"\"\"\\n table = [0] * len(prefix)\\n for i in range(1, len(prefix)):\\n idx = table[i - 1]\\n while prefix[i] != prefix[idx]:\\n if idx == 0:\\n table[i] = 0\\n break\\n idx = table[idx - 1]\\n else:\\n table[i] = idx + 1\\n return table\\n def _compile_info(code, pattern, flags):\\n # internal: compile an info block.  in the current version,\\n # this contains min/max pattern width, and an optional literal\\n # prefix or a character map\\n lo, hi = pattern.getwidth()\\n if hi > maxcode:\\n hi = maxcode\\n if lo == 0:\\n code.extend([info, 4, 0, lo, hi])\\n return\\n # look for a literal prefix\\n prefix = []\\n prefixappend = prefix.append\\n prefix_skip = 0\\n charset = [] # not used\\n charsetappend = charset.append\\n if not (flags & sre_flag_ignorecase):\\n # look for literal prefix\\n for op, av in pattern.data:\\n if op is literal:\\n if len(prefix) == prefix_skip:\\n prefix_skip = prefix_skip + 1\\n prefixappend(av)\\n elif op is subpattern and len(av[1]) == 1:\\n op, av = av[1][0]\\n if op is literal:\\n prefixappend(av)\\n else:\\n break\\n else:\\n break\\n # if no prefix, look for charset prefix\\n if not prefix and pattern.data:\\n op, av = pattern.data[0]\\n if op is subpattern and av[1]:\\n op, av = av[1][0]\\n if op is literal:\\n charsetappend((op, av))\\n elif op is branch:\\n c = []\\n cappend = c.append\\n for p in av[1]:\\n if not p:\\n break\\n op, av = p[0]\\n if op is literal:\\n cappend((op, av))\\n else:\\n break\\n else:\\n charset = c\\n elif op is branch:\\n c = []\\n cappend = c.append\\n for p in av[1]:\\n if not p:\\n break\\n op, av = p[0]\\n if op is literal:\\n cappend((op, av))\\n else:\\n break\\n else:\\n charset = c\\n elif op is in:\\n charset = av\\n ##     if prefix:\\n ##         print(\"*** prefix\", prefix, prefix_skip)\\n ##     if charset:\\n ##         print(\"*** charset\", charset)\\n # add an info block\\n emit = code.append\\n emit(info)\\n skip = len(code); emit(0)\\n # literal flag\\n mask = 0\\n if prefix:\\n mask = sre_info_prefix\\n if len(prefix) == prefix_skip == len(pattern.data):\\n mask = mask | sre_info_literal\\n elif charset:\\n mask = mask | sre_info_charset\\n emit(mask)\\n # pattern length\\n if lo < maxcode:\\n emit(lo)\\n else:\\n emit(maxcode)\\n prefix = prefix[:maxcode]\\n emit(min(hi, maxcode))\\n # add literal prefix\\n if prefix:\\n emit(len(prefix)) # length\\n emit(prefix_skip) # skip\\n code.extend(prefix)\\n # generate overlap table\\n code.extend(_generate_overlap_table(prefix))\\n elif charset:\\n _compile_charset(charset, flags, code)\\n code[skip] = len(code) - skip\\n def isstring(obj):\\n return isinstance(obj, (str, bytes))\\n def _code(p, flags):\\n flags = p.pattern.flags | flags\\n code = []\\n # compile info block\\n _compile_info(code, p, flags)\\n # compile the pattern\\n _compile(code, p.data, flags)\\n code.append(success)\\n return code\\n def compile(p, flags=0):\\n # internal: convert pattern list to internal format\\n if isstring(p):\\n pattern = p\\n p = sre_parse.parse(p, flags)\\n else:\\n pattern = none\\n code = _code(p, flags)\\n # print(code)\\n # map in either direction\\n groupindex = p.pattern.groupdict\\n indexgroup = [none] * p.pattern.groups\\n for k, i in groupindex.items():\\n indexgroup[i] = k\\n return _sre.compile(\\n pattern, flags | p.pattern.flags, code,\\n p.pattern.groups-1,\\n groupindex, indexgroup\\n )\\n #\\n # secret labs\\' regular expression engine\\n #\\n # various symbols used by the regular expression engine.\\n # run this script to update the _sre include files!\\n #\\n # copyright (c) 1998-2001 by secret labs ab.  all rights reserved.\\n #\\n # see the sre.py file for information on usage and redistribution.\\n #\\n \"\"\"internal support module for sre\"\"\"\\n # update when constants are added or removed\\n magic = 20140917\\n from _sre import maxrepeat, maxgroups\\n # sre standard exception (access as sre.error)\\n # should this really be here?\\n class error(exception):\\n def __init__(self, msg, pattern=none, pos=none):\\n self.msg = msg\\n self.pattern = pattern\\n self.pos = pos\\n if pattern is not none and pos is not none:\\n msg = \\'%s at position %d\\' % (msg, pos)\\n if isinstance(pattern, str):\\n newline = \\'\\\\n\\'\\n else:\\n newline = b\\'\\\\n\\'\\n self.lineno = pattern.count(newline, 0, pos) + 1\\n self.colno = pos - pattern.rfind(newline, 0, pos)\\n if newline in pattern:\\n msg = \\'%s (line %d, column %d)\\' % (msg, self.lineno, self.colno)\\n else:\\n self.lineno = self.colno = none\\n super().__init__(msg)\\n class _namedintconstant(int):\\n def __new__(cls, value, name):\\n self = super(_namedintconstant, cls).__new__(cls, value)\\n self.name = name\\n return self\\n def __str__(self):\\n return self.name\\n __repr__ = __str__\\n maxrepeat = _namedintconstant(maxrepeat, \\'maxrepeat\\')\\n def _makecodes(names):\\n names = names.strip().split()\\n items = [_namedintconstant(i, name) for i, name in enumerate(names)]\\n globals().update({item.name: item for item in items})\\n return items\\n # operators\\n # failure=0 success=1 (just because it looks better that way :-)\\n opcodes = _makecodes(\"\"\"\\n failure success\\n any any_all\\n assert assert_not\\n at\\n branch\\n call\\n category\\n charset bigcharset\\n groupref groupref_exists groupref_ignore\\n in in_ignore\\n info\\n jump\\n literal literal_ignore\\n mark\\n max_until\\n min_until\\n not_literal not_literal_ignore\\n negate\\n range\\n repeat\\n repeat_one\\n subpattern\\n min_repeat_one\\n range_ignore\\n min_repeat max_repeat\\n \"\"\")\\n del opcodes[-2:] # remove min_repeat and max_repeat\\n # positions\\n atcodes = _makecodes(\"\"\"\\n at_beginning at_beginning_line at_beginning_string\\n at_boundary at_non_boundary\\n at_end at_end_line at_end_string\\n at_loc_boundary at_loc_non_boundary\\n at_uni_boundary at_uni_non_boundary\\n \"\"\")\\n # categories\\n chcodes = _makecodes(\"\"\"\\n category_digit category_not_digit\\n category_space category_not_space\\n category_word category_not_word\\n category_linebreak category_not_linebreak\\n category_loc_word category_loc_not_word\\n category_uni_digit category_uni_not_digit\\n category_uni_space category_uni_not_space\\n category_uni_word category_uni_not_word\\n category_uni_linebreak category_uni_not_linebreak\\n \"\"\")\\n # replacement operations for \"ignore case\" mode\\n op_ignore = {\\n groupref: groupref_ignore,\\n in: in_ignore,\\n literal: literal_ignore,\\n not_literal: not_literal_ignore,\\n range: range_ignore,\\n }\\n at_multiline = {\\n at_beginning: at_beginning_line,\\n at_end: at_end_line\\n }\\n at_locale = {\\n at_boundary: at_loc_boundary,\\n at_non_boundary: at_loc_non_boundary\\n }\\n at_unicode = {\\n at_boundary: at_uni_boundary,\\n at_non_boundary: at_uni_non_boundary\\n }\\n ch_locale = {\\n category_digit: category_digit,\\n category_not_digit: category_not_digit,\\n category_space: category_space,\\n category_not_space: category_not_space,\\n category_word: category_loc_word,\\n category_not_word: category_loc_not_word,\\n category_linebreak: category_linebreak,\\n category_not_linebreak: category_not_linebreak\\n }\\n ch_unicode = {\\n category_digit: category_uni_digit,\\n category_not_digit: category_uni_not_digit,\\n category_space: category_uni_space,\\n category_not_space: category_uni_not_space,\\n category_word: category_uni_word,\\n category_not_word: category_uni_not_word,\\n category_linebreak: category_uni_linebreak,\\n category_not_linebreak: category_uni_not_linebreak\\n }\\n # flags\\n sre_flag_template = 1 # template mode (disable backtracking)\\n sre_flag_ignorecase = 2 # case insensitive\\n sre_flag_locale = 4 # honour system locale\\n sre_flag_multiline = 8 # treat target as multiline string\\n sre_flag_dotall = 16 # treat target as a single string\\n sre_flag_unicode = 32 # use unicode \"locale\"\\n sre_flag_verbose = 64 # ignore whitespace and comments\\n sre_flag_debug = 128 # debugging\\n sre_flag_ascii = 256 # use ascii \"locale\"\\n # flags for info primitive\\n sre_info_prefix = 1 # has prefix\\n sre_info_literal = 2 # entire pattern is literal (given by prefix)\\n sre_info_charset = 4 # pattern starts with character from given set\\n if __name__ == \"__main__\":\\n def dump(f, d, prefix):\\n items = sorted(d)\\n for item in items:\\n f.write(\"#define %s_%s %d\\\\n\" % (prefix, item, item))\\n with open(\"sre_constants.h\", \"w\") as f:\\n f.write(\"\"\"\\\\\\n /*\\n * secret labs\\' regular expression engine\\n *\\n * regular expression matching engine\\n *\\n * note: this file is generated by sre_constants.py.  if you need\\n * to change anything in here, edit sre_constants.py and run it.\\n *\\n * copyright (c) 1997-2001 by secret labs ab.  all rights reserved.\\n *\\n * see the _sre.c file for information on usage and redistribution.\\n */\\n \"\"\")\\n f.write(\"#define sre_magic %d\\\\n\" % magic)\\n dump(f, opcodes, \"sre_op\")\\n dump(f, atcodes, \"sre\")\\n dump(f, chcodes, \"sre\")\\n f.write(\"#define sre_flag_template %d\\\\n\" % sre_flag_template)\\n f.write(\"#define sre_flag_ignorecase %d\\\\n\" % sre_flag_ignorecase)\\n f.write(\"#define sre_flag_locale %d\\\\n\" % sre_flag_locale)\\n f.write(\"#define sre_flag_multiline %d\\\\n\" % sre_flag_multiline)\\n f.write(\"#define sre_flag_dotall %d\\\\n\" % sre_flag_dotall)\\n f.write(\"#define sre_flag_unicode %d\\\\n\" % sre_flag_unicode)\\n f.write(\"#define sre_flag_verbose %d\\\\n\" % sre_flag_verbose)\\n f.write(\"#define sre_flag_debug %d\\\\n\" % sre_flag_debug)\\n f.write(\"#define sre_flag_ascii %d\\\\n\" % sre_flag_ascii)\\n f.write(\"#define sre_info_prefix %d\\\\n\" % sre_info_prefix)\\n f.write(\"#define sre_info_literal %d\\\\n\" % sre_info_literal)\\n f.write(\"#define sre_info_charset %d\\\\n\" % sre_info_charset)\\n print(\"done\")\\n #\\n # secret labs\\' regular expression engine\\n #\\n # convert re-style regular expression to sre pattern\\n #\\n # copyright (c) 1998-2001 by secret labs ab.  all rights reserved.\\n #\\n # see the sre.py file for information on usage and redistribution.\\n #\\n \"\"\"internal support module for sre\"\"\"\\n # xxx: show string offset and offending character for all errors\\n from sre_constants import *\\n special_chars = \".\\\\\\\\[{()*+?^$|\"\\n repeat_chars = \"*+?{\"\\n digits = frozenset(\"0123456789\")\\n octdigits = frozenset(\"01234567\")\\n hexdigits = frozenset(\"0123456789abcdefabcdef\")\\n asciiletters = frozenset(\"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz\")\\n whitespace = frozenset(\" \\\\t\\\\n\\\\r\\\\v\\\\f\")\\n _repeatcodes = frozenset({min_repeat, max_repeat})\\n _unitcodes = frozenset({any, range, in, literal, not_literal, category})\\n escapes = {\\n r\"\\\\a\": (literal, ord(\"\\\\a\")),\\n r\"\\\\b\": (literal, ord(\"\\\\b\")),\\n r\"\\\\f\": (literal, ord(\"\\\\f\")),\\n r\"\\\\n\": (literal, ord(\"\\\\n\")),\\n r\"\\\\r\": (literal, ord(\"\\\\r\")),\\n r\"\\\\t\": (literal, ord(\"\\\\t\")),\\n r\"\\\\v\": (literal, ord(\"\\\\v\")),\\n r\"\\\\\\\\\": (literal, ord(\"\\\\\\\\\"))\\n }\\n categories = {\\n r\"\\\\a\": (at, at_beginning_string), # start of string\\n r\"\\\\b\": (at, at_boundary),\\n r\"\\\\b\": (at, at_non_boundary),\\n r\"\\\\d\": (in, [(category, category_digit)]),\\n r\"\\\\d\": (in, [(category, category_not_digit)]),\\n r\"\\\\s\": (in, [(category, category_space)]),\\n r\"\\\\s\": (in, [(category, category_not_space)]),\\n r\"\\\\w\": (in, [(category, category_word)]),\\n r\"\\\\w\": (in, [(category, category_not_word)]),\\n r\"\\\\z\": (at, at_end_string), # end of string\\n }\\n flags = {\\n # standard flags\\n \"i\": sre_flag_ignorecase,\\n \"l\": sre_flag_locale,\\n \"m\": sre_flag_multiline,\\n \"s\": sre_flag_dotall,\\n \"x\": sre_flag_verbose,\\n # extensions\\n \"a\": sre_flag_ascii,\\n \"t\": sre_flag_template,\\n \"u\": sre_flag_unicode,\\n }\\n class pattern:\\n # master pattern object.  keeps track of global attributes\\n def __init__(self):\\n self.flags = 0\\n self.groupdict = {}\\n self.groupwidths = [none]  # group 0\\n self.lookbehindgroups = none\\n @property\\n def groups(self):\\n return len(self.groupwidths)\\n def opengroup(self, name=none):\\n gid = self.groups\\n self.groupwidths.append(none)\\n if self.groups > maxgroups:\\n raise error(\"too many groups\")\\n if name is not none:\\n ogid = self.groupdict.get(name, none)\\n if ogid is not none:\\n raise error(\"redefinition of group name %r as group %d; \"\\n \"was group %d\" % (name, gid,  ogid))\\n self.groupdict[name] = gid\\n return gid\\n def closegroup(self, gid, p):\\n self.groupwidths[gid] = p.getwidth()\\n def checkgroup(self, gid):\\n return gid < self.groups and self.groupwidths[gid] is not none\\n def checklookbehindgroup(self, gid, source):\\n if self.lookbehindgroups is not none:\\n if not self.checkgroup(gid):\\n raise source.error(\\'cannot refer to an open group\\')\\n if gid >= self.lookbehindgroups:\\n raise source.error(\\'cannot refer to group defined in the same \\'\\n \\'lookbehind subpattern\\')\\n class subpattern:\\n # a subpattern, in intermediate form\\n def __init__(self, pattern, data=none):\\n self.pattern = pattern\\n if data is none:\\n data = []\\n self.data = data\\n self.width = none\\n def dump(self, level=0):\\n nl = true\\n seqtypes = (tuple, list)\\n for op, av in self.data:\\n print(level*\"  \" + str(op), end=\\'\\')\\n if op is in:\\n # member sublanguage\\n print()\\n for op, a in av:\\n print((level+1)*\"  \" + str(op), a)\\n elif op is branch:\\n print()\\n for i, a in enumerate(av[1]):\\n if i:\\n print(level*\"  \" + \"or\")\\n a.dump(level+1)\\n elif op is groupref_exists:\\n condgroup, item_yes, item_no = av\\n print(\\'\\', condgroup)\\n item_yes.dump(level+1)\\n if item_no:\\n print(level*\"  \" + \"else\")\\n item_no.dump(level+1)\\n elif isinstance(av, seqtypes):\\n nl = false\\n for a in av:\\n if isinstance(a, subpattern):\\n if not nl:\\n print()\\n a.dump(level+1)\\n nl = true\\n else:\\n if not nl:\\n print(\\' \\', end=\\'\\')\\n print(a, end=\\'\\')\\n nl = false\\n if not nl:\\n print()\\n else:\\n print(\\'\\', av)\\n def __repr__(self):\\n return repr(self.data)\\n def __len__(self):\\n return len(self.data)\\n def __delitem__(self, index):\\n del self.data[index]\\n def __getitem__(self, index):\\n if isinstance(index, slice):\\n return subpattern(self.pattern, self.data[index])\\n return self.data[index]\\n def __setitem__(self, index, code):\\n self.data[index] = code\\n def insert(self, index, code):\\n self.data.insert(index, code)\\n def append(self, code):\\n self.data.append(code)\\n def getwidth(self):\\n # determine the width (min, max) for this subpattern\\n if self.width is not none:\\n return self.width\\n lo = hi = 0\\n for op, av in self.data:\\n if op is branch:\\n i = maxrepeat - 1\\n j = 0\\n for av in av[1]:\\n l, h = av.getwidth()\\n i = min(i, l)\\n j = max(j, h)\\n lo = lo + i\\n hi = hi + j\\n elif op is call:\\n i, j = av.getwidth()\\n lo = lo + i\\n hi = hi + j\\n elif op is subpattern:\\n i, j = av[1].getwidth()\\n lo = lo + i\\n hi = hi + j\\n elif op in _repeatcodes:\\n i, j = av[2].getwidth()\\n lo = lo + i * av[0]\\n hi = hi + j * av[1]\\n elif op in _unitcodes:\\n lo = lo + 1\\n hi = hi + 1\\n elif op is groupref:\\n i, j = self.pattern.groupwidths[av]\\n lo = lo + i\\n hi = hi + j\\n elif op is groupref_exists:\\n i, j = av[1].getwidth()\\n if av[2] is not none:\\n l, h = av[2].getwidth()\\n i = min(i, l)\\n j = max(j, h)\\n else:\\n i = 0\\n lo = lo + i\\n hi = hi + j\\n elif op is success:\\n break\\n self.width = min(lo, maxrepeat - 1), min(hi, maxrepeat)\\n return self.width\\n class tokenizer:\\n def __init__(self, string):\\n self.istext = isinstance(string, str)\\n self.string = string\\n if not self.istext:\\n string = str(string, \\'latin1\\')\\n self.decoded_string = string\\n self.index = 0\\n self.next = none\\n self.__next()\\n def __next(self):\\n index = self.index\\n try:\\n char = self.decoded_string[index]\\n except indexerror:\\n self.next = none\\n return\\n if char == \"\\\\\\\\\":\\n index += 1\\n try:\\n char += self.decoded_string[index]\\n except indexerror:\\n raise error(\"bad escape (end of pattern)\",\\n self.string, len(self.string) - 1) from none\\n self.index = index + 1\\n self.next = char\\n def match(self, char):\\n if char == self.next:\\n self.__next()\\n return true\\n return false\\n def get(self):\\n this = self.next\\n self.__next()\\n return this\\n def getwhile(self, n, charset):\\n result = \\'\\'\\n for _ in range(n):\\n c = self.next\\n if c not in charset:\\n break\\n result += c\\n self.__next()\\n return result\\n def getuntil(self, terminator):\\n result = \\'\\'\\n while true:\\n c = self.next\\n self.__next()\\n if c is none:\\n if not result:\\n raise self.error(\"missing group name\")\\n raise self.error(\"missing %s, unterminated name\" % terminator,\\n len(result))\\n if c == terminator:\\n if not result:\\n raise self.error(\"missing group name\", 1)\\n break\\n result += c\\n return result\\n def tell(self):\\n return self.index - len(self.next or \\'\\')\\n def seek(self, index):\\n self.index = index\\n self.__next()\\n def error(self, msg, offset=0):\\n return error(msg, self.string, self.tell() - offset)\\n # the following three functions are not used in this module anymore, but we keep\\n # them here (with deprecationwarnings) for backwards compatibility.\\n def isident(char):\\n import warnings\\n warnings.warn(\\'sre_parse.isident() will be removed in 3.5\\',\\n deprecationwarning, stacklevel=2)\\n return \"a\" <= char <= \"z\" or \"a\" <= char <= \"z\" or char == \"_\"\\n def isdigit(char):\\n import warnings\\n warnings.warn(\\'sre_parse.isdigit() will be removed in 3.5\\',\\n deprecationwarning, stacklevel=2)\\n return \"0\" <= char <= \"9\"\\n def isname(name):\\n import warnings\\n warnings.warn(\\'sre_parse.isname() will be removed in 3.5\\',\\n deprecationwarning, stacklevel=2)\\n # check that group name is a valid string\\n if not isident(name[0]):\\n return false\\n for char in name[1:]:\\n if not isident(char) and not isdigit(char):\\n return false\\n return true\\n def _class_escape(source, escape):\\n # handle escape code inside character class\\n code = escapes.get(escape)\\n if code:\\n return code\\n code = categories.get(escape)\\n if code and code[0] is in:\\n return code\\n try:\\n c = escape[1:2]\\n if c == \"x\":\\n # hexadecimal escape (exactly two digits)\\n escape += source.getwhile(2, hexdigits)\\n if len(escape) != 4:\\n raise source.error(\"incomplete escape %s\" % escape, len(escape))\\n return literal, int(escape[2:], 16)\\n elif c == \"u\" and source.istext:\\n # unicode escape (exactly four digits)\\n escape += source.getwhile(4, hexdigits)\\n if len(escape) != 6:\\n raise source.error(\"incomplete escape %s\" % escape, len(escape))\\n return literal, int(escape[2:], 16)\\n elif c == \"u\" and source.istext:\\n # unicode escape (exactly eight digits)\\n escape += source.getwhile(8, hexdigits)\\n if len(escape) != 10:\\n raise source.error(\"incomplete escape %s\" % escape, len(escape))\\n c = int(escape[2:], 16)\\n chr(c) # raise valueerror for invalid code\\n return literal, c\\n elif c in octdigits:\\n # octal escape (up to three digits)\\n escape += source.getwhile(2, octdigits)\\n c = int(escape[1:], 8)\\n if c > 0o377:\\n raise source.error(\\'octal escape value %s outside of \\'\\n \\'range 0-0o377\\' % escape, len(escape))\\n return literal, c\\n elif c in digits:\\n raise valueerror\\n if len(escape) == 2:\\n if c in asciiletters:\\n import warnings\\n warnings.warn(\\'bad escape %s\\' % escape,\\n deprecationwarning, stacklevel=8)\\n return literal, ord(escape[1])\\n except valueerror:\\n pass\\n raise source.error(\"bad escape %s\" % escape, len(escape))\\n def _escape(source, escape, state):\\n # handle escape code in expression\\n code = categories.get(escape)\\n if code:\\n return code\\n code = escapes.get(escape)\\n if code:\\n return code\\n try:\\n c = escape[1:2]\\n if c == \"x\":\\n # hexadecimal escape\\n escape += source.getwhile(2, hexdigits)\\n if len(escape) != 4:\\n raise source.error(\"incomplete escape %s\" % escape, len(escape))\\n return literal, int(escape[2:], 16)\\n elif c == \"u\" and source.istext:\\n # unicode escape (exactly four digits)\\n escape += source.getwhile(4, hexdigits)\\n if len(escape) != 6:\\n raise source.error(\"incomplete escape %s\" % escape, len(escape))\\n return literal, int(escape[2:], 16)\\n elif c == \"u\" and source.istext:\\n # unicode escape (exactly eight digits)\\n escape += source.getwhile(8, hexdigits)\\n if len(escape) != 10:\\n raise source.error(\"incomplete escape %s\" % escape, len(escape))\\n c = int(escape[2:], 16)\\n chr(c) # raise valueerror for invalid code\\n return literal, c\\n elif c == \"0\":\\n # octal escape\\n escape += source.getwhile(2, octdigits)\\n return literal, int(escape[1:], 8)\\n elif c in digits:\\n # octal escape *or* decimal group reference (sigh)\\n if source.next in digits:\\n escape += source.get()\\n if (escape[1] in octdigits and escape[2] in octdigits and\\n source.next in octdigits):\\n # got three octal digits; this is an octal escape\\n escape += source.get()\\n c = int(escape[1:], 8)\\n if c > 0o377:\\n raise source.error(\\'octal escape value %s outside of \\'\\n \\'range 0-0o377\\' % escape,\\n len(escape))\\n return literal, c\\n # not an octal escape, so this is a group reference\\n group = int(escape[1:])\\n if group < state.groups:\\n if not state.checkgroup(group):\\n raise source.error(\"cannot refer to an open group\",\\n len(escape))\\n state.checklookbehindgroup(group, source)\\n return groupref, group\\n raise source.error(\"invalid group reference\", len(escape))\\n if len(escape) == 2:\\n if c in asciiletters:\\n import warnings\\n warnings.warn(\\'bad escape %s\\' % escape,\\n deprecationwarning, stacklevel=8)\\n return literal, ord(escape[1])\\n except valueerror:\\n pass\\n raise source.error(\"bad escape %s\" % escape, len(escape))\\n def _parse_sub(source, state, nested=true):\\n # parse an alternation: a|b|c\\n items = []\\n itemsappend = items.append\\n sourcematch = source.match\\n start = source.tell()\\n while true:\\n itemsappend(_parse(source, state))\\n if not sourcematch(\"|\"):\\n break\\n if len(items) == 1:\\n return items[0]\\n subpattern = subpattern(state)\\n subpatternappend = subpattern.append\\n # check if all items share a common prefix\\n while true:\\n prefix = none\\n for item in items:\\n if not item:\\n break\\n if prefix is none:\\n prefix = item[0]\\n elif item[0] != prefix:\\n break\\n else:\\n # all subitems start with a common \"prefix\".\\n # move it out of the branch\\n for item in items:\\n del item[0]\\n subpatternappend(prefix)\\n continue # check next one\\n break\\n # check if the branch can be replaced by a character set\\n for item in items:\\n if len(item) != 1 or item[0][0] is not literal:\\n break\\n else:\\n # we can store this as a character set instead of a\\n # branch (the compiler may optimize this even more)\\n subpatternappend((in, [item[0] for item in items]))\\n return subpattern\\n subpattern.append((branch, (none, items)))\\n return subpattern\\n def _parse_sub_cond(source, state, condgroup):\\n item_yes = _parse(source, state)\\n if source.match(\"|\"):\\n item_no = _parse(source, state)\\n if source.next == \"|\":\\n raise source.error(\"conditional backref with more than two branches\")\\n else:\\n item_no = none\\n subpattern = subpattern(state)\\n subpattern.append((groupref_exists, (condgroup, item_yes, item_no)))\\n return subpattern\\n def _parse(source, state):\\n # parse a simple pattern\\n subpattern = subpattern(state)\\n # precompute constants into local variables\\n subpatternappend = subpattern.append\\n sourceget = source.get\\n sourcematch = source.match\\n _len = len\\n _ord = ord\\n verbose = state.flags & sre_flag_verbose\\n while true:\\n this = source.next\\n if this is none:\\n break # end of pattern\\n if this in \"|)\":\\n break # end of subpattern\\n sourceget()\\n if verbose:\\n # skip whitespace and comments\\n if this in whitespace:\\n continue\\n if this == \"#\":\\n while true:\\n this = sourceget()\\n if this is none or this == \"\\\\n\":\\n break\\n continue\\n if this[0] == \"\\\\\\\\\":\\n code = _escape(source, this, state)\\n subpatternappend(code)\\n elif this not in special_chars:\\n subpatternappend((literal, _ord(this)))\\n elif this == \"[\":\\n here = source.tell() - 1\\n # character set\\n set = []\\n setappend = set.append\\n ##          if sourcematch(\":\"):\\n ##              pass # handle character classes\\n if sourcematch(\"^\"):\\n setappend((negate, none))\\n # check remaining characters\\n start = set[:]\\n while true:\\n this = sourceget()\\n if this is none:\\n raise source.error(\"unterminated character set\",\\n source.tell() - here)\\n if this == \"]\" and set != start:\\n break\\n elif this[0] == \"\\\\\\\\\":\\n code1 = _class_escape(source, this)\\n else:\\n code1 = literal, _ord(this)\\n if sourcematch(\"-\"):\\n # potential range\\n that = sourceget()\\n if that is none:\\n raise source.error(\"unterminated character set\",\\n source.tell() - here)\\n if that == \"]\":\\n if code1[0] is in:\\n code1 = code1[1][0]\\n setappend(code1)\\n setappend((literal, _ord(\"-\")))\\n break\\n if that[0] == \"\\\\\\\\\":\\n code2 = _class_escape(source, that)\\n else:\\n code2 = literal, _ord(that)\\n if code1[0] != literal or code2[0] != literal:\\n msg = \"bad character range %s-%s\" % (this, that)\\n raise source.error(msg, len(this) + 1 + len(that))\\n lo = code1[1]\\n hi = code2[1]\\n if hi < lo:\\n msg = \"bad character range %s-%s\" % (this, that)\\n raise source.error(msg, len(this) + 1 + len(that))\\n setappend((range, (lo, hi)))\\n else:\\n if code1[0] is in:\\n code1 = code1[1][0]\\n setappend(code1)\\n # xxx: <fl> should move set optimization to compiler!\\n if _len(set)==1 and set[0][0] is literal:\\n subpatternappend(set[0]) # optimization\\n elif _len(set)==2 and set[0][0] is negate and set[1][0] is literal:\\n subpatternappend((not_literal, set[1][1])) # optimization\\n else:\\n # xxx: <fl> should add charmap optimization here\\n subpatternappend((in, set))\\n elif this in repeat_chars:\\n # repeat previous item\\n here = source.tell()\\n if this == \"?\":\\n min, max = 0, 1\\n elif this == \"*\":\\n min, max = 0, maxrepeat\\n elif this == \"+\":\\n min, max = 1, maxrepeat\\n elif this == \"{\":\\n if source.next == \"}\":\\n subpatternappend((literal, _ord(this)))\\n continue\\n min, max = 0, maxrepeat\\n lo = hi = \"\"\\n while source.next in digits:\\n lo += sourceget()\\n if sourcematch(\",\"):\\n while source.next in digits:\\n hi += sourceget()\\n else:\\n hi = lo\\n if not sourcematch(\"}\"):\\n subpatternappend((literal, _ord(this)))\\n source.seek(here)\\n continue\\n if lo:\\n min = int(lo)\\n if min >= maxrepeat:\\n raise overflowerror(\"the repetition number is too large\")\\n if hi:\\n max = int(hi)\\n if max >= maxrepeat:\\n raise overflowerror(\"the repetition number is too large\")\\n if max < min:\\n raise source.error(\"min repeat greater than max repeat\",\\n source.tell() - here)\\n else:\\n raise assertionerror(\"unsupported quantifier %r\" % (char,))\\n # figure out which item to repeat\\n if subpattern:\\n item = subpattern[-1:]\\n else:\\n item = none\\n if not item or (_len(item) == 1 and item[0][0] is at):\\n raise source.error(\"nothing to repeat\",\\n source.tell() - here + len(this))\\n if item[0][0] in _repeatcodes:\\n raise source.error(\"multiple repeat\",\\n source.tell() - here + len(this))\\n if sourcematch(\"?\"):\\n subpattern[-1] = (min_repeat, (min, max, item))\\n else:\\n subpattern[-1] = (max_repeat, (min, max, item))\\n elif this == \".\":\\n subpatternappend((any, none))\\n elif this == \"(\":\\n start = source.tell() - 1\\n group = true\\n name = none\\n condgroup = none\\n if sourcematch(\"?\"):\\n # options\\n char = sourceget()\\n if char is none:\\n raise source.error(\"unexpected end of pattern\")\\n if char == \"p\":\\n # python extensions\\n if sourcematch(\"<\"):\\n # named group: skip forward to end of name\\n name = source.getuntil(\">\")\\n if not name.isidentifier():\\n msg = \"bad character in group name %r\" % name\\n raise source.error(msg, len(name) + 1)\\n elif sourcematch(\"=\"):\\n # named backreference\\n name = source.getuntil(\")\")\\n if not name.isidentifier():\\n msg = \"bad character in group name %r\" % name\\n raise source.error(msg, len(name) + 1)\\n gid = state.groupdict.get(name)\\n if gid is none:\\n msg = \"unknown group name %r\" % name\\n raise source.error(msg, len(name) + 1)\\n if not state.checkgroup(gid):\\n raise source.error(\"cannot refer to an open group\",\\n len(name) + 1)\\n state.checklookbehindgroup(gid, source)\\n subpatternappend((groupref, gid))\\n continue\\n else:\\n char = sourceget()\\n if char is none:\\n raise source.error(\"unexpected end of pattern\")\\n raise source.error(\"unknown extension ?p\" + char,\\n len(char) + 2)\\n elif char == \":\":\\n # non-capturing group\\n group = none\\n elif char == \"#\":\\n # comment\\n while true:\\n if source.next is none:\\n raise source.error(\"missing ), unterminated comment\",\\n source.tell() - start)\\n if sourceget() == \")\":\\n break\\n continue\\n elif char in \"=!<\":\\n # lookahead assertions\\n dir = 1\\n if char == \"<\":\\n char = sourceget()\\n if char is none:\\n raise source.error(\"unexpected end of pattern\")\\n if char not in \"=!\":\\n raise source.error(\"unknown extension ?<\" + char,\\n len(char) + 2)\\n dir = -1 # lookbehind\\n lookbehindgroups = state.lookbehindgroups\\n if lookbehindgroups is none:\\n state.lookbehindgroups = state.groups\\n p = _parse_sub(source, state)\\n if dir < 0:\\n if lookbehindgroups is none:\\n state.lookbehindgroups = none\\n if not sourcematch(\")\"):\\n raise source.error(\"missing ), unterminated subpattern\",\\n source.tell() - start)\\n if char == \"=\":\\n subpatternappend((assert, (dir, p)))\\n else:\\n subpatternappend((assert_not, (dir, p)))\\n continue\\n elif char == \"(\":\\n # conditional backreference group\\n condname = source.getuntil(\")\")\\n group = none\\n if condname.isidentifier():\\n condgroup = state.groupdict.get(condname)\\n if condgroup is none:\\n msg = \"unknown group name %r\" % condname\\n raise source.error(msg, len(condname) + 1)\\n else:\\n try:\\n condgroup = int(condname)\\n if condgroup < 0:\\n raise valueerror\\n except valueerror:\\n msg = \"bad character in group name %r\" % condname\\n raise source.error(msg, len(condname) + 1) from none\\n if not condgroup:\\n raise source.error(\"bad group number\",\\n len(condname) + 1)\\n if condgroup >= maxgroups:\\n raise source.error(\"invalid group reference\",\\n len(condname) + 1)\\n state.checklookbehindgroup(condgroup, source)\\n elif char in flags:\\n # flags\\n while true:\\n state.flags |= flags[char]\\n char = sourceget()\\n if char is none:\\n raise source.error(\"missing )\")\\n if char == \")\":\\n break\\n if char not in flags:\\n raise source.error(\"unknown flag\", len(char))\\n verbose = state.flags & sre_flag_verbose\\n continue\\n else:\\n raise source.error(\"unknown extension ?\" + char,\\n len(char) + 1)\\n # parse group contents\\n if group is not none:\\n try:\\n group = state.opengroup(name)\\n except error as err:\\n raise source.error(err.msg, len(name) + 1) from none\\n if condgroup:\\n p = _parse_sub_cond(source, state, condgroup)\\n else:\\n p = _parse_sub(source, state)\\n if not source.match(\")\"):\\n raise source.error(\"missing ), unterminated subpattern\",\\n source.tell() - start)\\n if group is not none:\\n state.closegroup(group, p)\\n subpatternappend((subpattern, (group, p)))\\n elif this == \"^\":\\n subpatternappend((at, at_beginning))\\n elif this == \"$\":\\n subpattern.append((at, at_end))\\n else:\\n raise assertionerror(\"unsupported special character %r\" % (char,))\\n return subpattern\\n def fix_flags(src, flags):\\n # check and fix flags according to the type of pattern (str or bytes)\\n if isinstance(src, str):\\n if flags & sre_flag_locale:\\n import warnings\\n warnings.warn(\"locale flag with a str pattern is deprecated. \"\\n \"will be an error in 3.6\",\\n deprecationwarning, stacklevel=6)\\n if not flags & sre_flag_ascii:\\n flags |= sre_flag_unicode\\n elif flags & sre_flag_unicode:\\n raise valueerror(\"ascii and unicode flags are incompatible\")\\n else:\\n if flags & sre_flag_unicode:\\n raise valueerror(\"cannot use unicode flag with a bytes pattern\")\\n if flags & sre_flag_locale and flags & sre_flag_ascii:\\n import warnings\\n warnings.warn(\"ascii and locale flags are incompatible. \"\\n \"will be an error in 3.6\",\\n deprecationwarning, stacklevel=6)\\n return flags\\n def parse(str, flags=0, pattern=none):\\n # parse \\'re\\' pattern into list of (opcode, argument) tuples\\n source = tokenizer(str)\\n if pattern is none:\\n pattern = pattern()\\n pattern.flags = flags\\n pattern.str = str\\n p = _parse_sub(source, pattern, 0)\\n p.pattern.flags = fix_flags(str, p.pattern.flags)\\n if source.next is not none:\\n assert source.next == \")\"\\n raise source.error(\"unbalanced parenthesis\")\\n if not (flags & sre_flag_verbose) and p.pattern.flags & sre_flag_verbose:\\n # the verbose flag was switched on inside the pattern.  to be\\n # on the safe side, we\\'ll parse the whole thing again...\\n return parse(str, p.pattern.flags)\\n if flags & sre_flag_debug:\\n p.dump()\\n return p\\n def parse_template(source, pattern):\\n # parse \\'re\\' replacement string into list of literals and\\n # group references\\n s = tokenizer(source)\\n sget = s.get\\n groups = []\\n literals = []\\n literal = []\\n lappend = literal.append\\n def addgroup(index):\\n if literal:\\n literals.append(\\'\\'.join(literal))\\n del literal[:]\\n groups.append((len(literals), index))\\n literals.append(none)\\n groupindex = pattern.groupindex\\n while true:\\n this = sget()\\n if this is none:\\n break # end of replacement string\\n if this[0] == \"\\\\\\\\\":\\n # group\\n c = this[1]\\n if c == \"g\":\\n name = \"\"\\n if not s.match(\"<\"):\\n raise s.error(\"missing <\")\\n name = s.getuntil(\">\")\\n if name.isidentifier():\\n try:\\n index = groupindex[name]\\n except keyerror:\\n raise indexerror(\"unknown group name %r\" % name)\\n else:\\n try:\\n index = int(name)\\n if index < 0:\\n raise valueerror\\n except valueerror:\\n raise s.error(\"bad character in group name %r\" % name,\\n len(name) + 1) from none\\n if index >= maxgroups:\\n raise s.error(\"invalid group reference\",\\n len(name) + 1)\\n addgroup(index)\\n elif c == \"0\":\\n if s.next in octdigits:\\n this += sget()\\n if s.next in octdigits:\\n this += sget()\\n lappend(chr(int(this[1:], 8) & 0xff))\\n elif c in digits:\\n isoctal = false\\n if s.next in digits:\\n this += sget()\\n if (c in octdigits and this[2] in octdigits and\\n s.next in octdigits):\\n this += sget()\\n isoctal = true\\n c = int(this[1:], 8)\\n if c > 0o377:\\n raise s.error(\\'octal escape value %s outside of \\'\\n \\'range 0-0o377\\' % this, len(this))\\n lappend(chr(c))\\n if not isoctal:\\n addgroup(int(this[1:]))\\n else:\\n try:\\n this = chr(escapes[this][1])\\n except keyerror:\\n if c in asciiletters:\\n import warnings\\n warnings.warn(\\'bad escape %s\\' % this,\\n deprecationwarning, stacklevel=4)\\n lappend(this)\\n else:\\n lappend(this)\\n if literal:\\n literals.append(\\'\\'.join(literal))\\n if not isinstance(source, str):\\n # the tokenizer implicitly decodes bytes objects as latin-1, we must\\n # therefore re-encode the final representation.\\n literals = [none if s is none else s.encode(\\'latin-1\\') for s in literals]\\n return groups, literals\\n def expand_template(template, match):\\n g = match.group\\n empty = match.string[:0]\\n groups, literals = template\\n literals = literals[:]\\n try:\\n for index, group in groups:\\n literals[index] = g(group) or empty\\n except indexerror:\\n raise error(\"invalid group reference\")\\n return empty.join(literals)\\n \"\"\"constants/functions for interpreting results of os.stat() and os.lstat().\\n suggested usage: from stat import *\\n \"\"\"\\n # indices for stat struct members in the tuple returned by os.stat()\\n st_mode  = 0\\n st_ino   = 1\\n st_dev   = 2\\n st_nlink = 3\\n st_uid   = 4\\n st_gid   = 5\\n st_size  = 6\\n st_atime = 7\\n st_mtime = 8\\n st_ctime = 9\\n # extract bits from the mode\\n def s_imode(mode):\\n \"\"\"return the portion of the file\\'s mode that can be set by\\n os.chmod().\\n \"\"\"\\n return mode & 0o7777\\n def s_ifmt(mode):\\n \"\"\"return the portion of the file\\'s mode that describes the\\n file type.\\n \"\"\"\\n return mode & 0o170000\\n # constants used as s_ifmt() for various file types\\n # (not all are implemented on all systems)\\n s_ifdir  = 0o040000  # directory\\n s_ifchr  = 0o020000  # character device\\n s_ifblk  = 0o060000  # block device\\n s_ifreg  = 0o100000  # regular file\\n s_ififo  = 0o010000  # fifo (named pipe)\\n s_iflnk  = 0o120000  # symbolic link\\n s_ifsock = 0o140000  # socket file\\n # functions to test for each file type\\n def s_isdir(mode):\\n \"\"\"return true if mode is from a directory.\"\"\"\\n return s_ifmt(mode) == s_ifdir\\n def s_ischr(mode):\\n \"\"\"return true if mode is from a character special device file.\"\"\"\\n return s_ifmt(mode) == s_ifchr\\n def s_isblk(mode):\\n \"\"\"return true if mode is from a block special device file.\"\"\"\\n return s_ifmt(mode) == s_ifblk\\n def s_isreg(mode):\\n \"\"\"return true if mode is from a regular file.\"\"\"\\n return s_ifmt(mode) == s_ifreg\\n def s_isfifo(mode):\\n \"\"\"return true if mode is from a fifo (named pipe).\"\"\"\\n return s_ifmt(mode) == s_ififo\\n def s_islnk(mode):\\n \"\"\"return true if mode is from a symbolic link.\"\"\"\\n return s_ifmt(mode) == s_iflnk\\n def s_issock(mode):\\n \"\"\"return true if mode is from a socket.\"\"\"\\n return s_ifmt(mode) == s_ifsock\\n # names for permission bits\\n s_isuid = 0o4000  # set uid bit\\n s_isgid = 0o2000  # set gid bit\\n s_enfmt = s_isgid # file locking enforcement\\n s_isvtx = 0o1000  # sticky bit\\n s_iread = 0o0400  # unix v7 synonym for s_irusr\\n s_iwrite = 0o0200 # unix v7 synonym for s_iwusr\\n s_iexec = 0o0100  # unix v7 synonym for s_ixusr\\n s_irwxu = 0o0700  # mask for owner permissions\\n s_irusr = 0o0400  # read by owner\\n s_iwusr = 0o0200  # write by owner\\n s_ixusr = 0o0100  # execute by owner\\n s_irwxg = 0o0070  # mask for group permissions\\n s_irgrp = 0o0040  # read by group\\n s_iwgrp = 0o0020  # write by group\\n s_ixgrp = 0o0010  # execute by group\\n s_irwxo = 0o0007  # mask for others (not in group) permissions\\n s_iroth = 0o0004  # read by others\\n s_iwoth = 0o0002  # write by others\\n s_ixoth = 0o0001  # execute by others\\n # names for file flags\\n uf_nodump    = 0x00000001  # do not dump file\\n uf_immutable = 0x00000002  # file may not be changed\\n uf_append    = 0x00000004  # file may only be appended to\\n uf_opaque    = 0x00000008  # directory is opaque when viewed through a union stack\\n uf_nounlink  = 0x00000010  # file may not be renamed or deleted\\n uf_compressed = 0x00000020 # os x: file is hfs-compressed\\n uf_hidden    = 0x00008000  # os x: file should not be displayed\\n sf_archived  = 0x00010000  # file may be archived\\n sf_immutable = 0x00020000  # file may not be changed\\n sf_append    = 0x00040000  # file may only be appended to\\n sf_nounlink  = 0x00100000  # file may not be renamed or deleted\\n sf_snapshot  = 0x00200000  # file is a snapshot file\\n _filemode_table = (\\n ((s_iflnk,         \"l\"),\\n (s_ifreg,         \"-\"),\\n (s_ifblk,         \"b\"),\\n (s_ifdir,         \"d\"),\\n (s_ifchr,         \"c\"),\\n (s_ififo,         \"p\")),\\n ((s_irusr,         \"r\"),),\\n ((s_iwusr,         \"w\"),),\\n ((s_ixusr|s_isuid, \"s\"),\\n (s_isuid,         \"s\"),\\n (s_ixusr,         \"x\")),\\n ((s_irgrp,         \"r\"),),\\n ((s_iwgrp,         \"w\"),),\\n ((s_ixgrp|s_isgid, \"s\"),\\n (s_isgid,         \"s\"),\\n (s_ixgrp,         \"x\")),\\n ((s_iroth,         \"r\"),),\\n ((s_iwoth,         \"w\"),),\\n ((s_ixoth|s_isvtx, \"t\"),\\n (s_isvtx,         \"t\"),\\n (s_ixoth,         \"x\"))\\n )\\n def filemode(mode):\\n \"\"\"convert a file\\'s mode to a string of the form \\'-rwxrwxrwx\\'.\"\"\"\\n perm = []\\n for table in _filemode_table:\\n for bit, char in table:\\n if mode & bit == bit:\\n perm.append(char)\\n break\\n else:\\n perm.append(\"-\")\\n return \"\".join(perm)\\n # windows file_attribute constants for interpreting os.stat()\\'s\\n # \"st_file_attributes\" member\\n file_attribute_archive = 32\\n file_attribute_compressed = 2048\\n file_attribute_device = 64\\n file_attribute_directory = 16\\n file_attribute_encrypted = 16384\\n file_attribute_hidden = 2\\n file_attribute_integrity_stream = 32768\\n file_attribute_normal = 128\\n file_attribute_not_content_indexed = 8192\\n file_attribute_no_scrub_data = 131072\\n file_attribute_offline = 4096\\n file_attribute_readonly = 1\\n file_attribute_reparse_point = 1024\\n file_attribute_sparse_file = 512\\n file_attribute_system = 4\\n file_attribute_temporary = 256\\n file_attribute_virtual = 65536\\n # if available, use c implementation\\n try:\\n from _stat import *\\n except importerror:\\n pass\\n __all__ = [\\n # functions\\n \\'calcsize\\', \\'pack\\', \\'pack_into\\', \\'unpack\\', \\'unpack_from\\',\\n \\'iter_unpack\\',\\n # classes\\n \\'struct\\',\\n # exceptions\\n \\'error\\'\\n ]\\n from _struct import *\\n from _struct import _clearcache\\n from _struct import __doc__\\n #! /usr/bin/python3.5\\n #-------------------------------------------------------------------\\n # tarfile.py\\n #-------------------------------------------------------------------\\n # copyright (c) 2002 lars gustaebel <lars@gustaebel.de>\\n # all rights reserved.\\n #\\n # permission  is  hereby granted,  free  of charge,  to  any person\\n # obtaining a  copy of  this software  and associated documentation\\n # files  (the  \"software\"),  to   deal  in  the  software   without\\n # restriction,  including  without limitation  the  rights to  use,\\n # copy, modify, merge, publish, distribute, sublicense, and/or sell\\n # copies  of  the  software,  and to  permit  persons  to  whom the\\n # software  is  furnished  to  do  so,  subject  to  the  following\\n # conditions:\\n #\\n # the above copyright  notice and this  permission notice shall  be\\n # included in all copies or substantial portions of the software.\\n #\\n # the software is provided \"as  is\", without warranty of any  kind,\\n # express or implied, including  but not limited to  the warranties\\n # of  merchantability,  fitness   for  a  particular   purpose  and\\n # noninfringement.  in  no  event shall  the  authors  or copyright\\n # holders  be liable  for any  claim, damages  or other  liability,\\n # whether  in an  action of  contract, tort  or otherwise,  arising\\n # from, out of or in connection with the software or the use or\\n # other dealings in the software.\\n #\\n \"\"\"read from and write to tar format archives.\\n \"\"\"\\n version     = \"0.9.0\"\\n __author__  = \"lars gust\\\\u00e4bel (lars@gustaebel.de)\"\\n __date__    = \"$date: 2011-02-25 17:42:01 +0200 (fri, 25 feb 2011) $\"\\n __cvsid__   = \"$id: tarfile.py 88586 2011-02-25 15:42:01z marc-andre.lemburg $\"\\n __credits__ = \"gustavo niemeyer, niels gust\\\\u00e4bel, richard townsend.\"\\n #---------\\n # imports\\n #---------\\n from builtins import open as bltn_open\\n import sys\\n import os\\n import io\\n import shutil\\n import stat\\n import time\\n import struct\\n import copy\\n import re\\n try:\\n import grp, pwd\\n except importerror:\\n grp = pwd = none\\n # os.symlink on windows prior to 6.0 raises notimplementederror\\n symlink_exception = (attributeerror, notimplementederror)\\n try:\\n # oserror (winerror=1314) will be raised if the caller does not hold the\\n # secreatesymboliclinkprivilege privilege\\n symlink_exception += (oserror,)\\n except nameerror:\\n pass\\n # from tarfile import *\\n __all__ = [\"tarfile\", \"tarinfo\", \"is_tarfile\", \"tarerror\"]\\n #---------------------------------------------------------\\n # tar constants\\n #---------------------------------------------------------\\n nul = b\"\\\\0\"                     # the null character\\n blocksize = 512                 # length of processing blocks\\n recordsize = blocksize * 20     # length of records\\n gnu_magic = b\"ustar  \\\\0\"        # magic gnu tar string\\n posix_magic = b\"ustar\\\\x0000\"    # magic posix tar string\\n length_name = 100               # maximum length of a filename\\n length_link = 100               # maximum length of a linkname\\n length_prefix = 155             # maximum length of the prefix field\\n regtype = b\"0\"                  # regular file\\n aregtype = b\"\\\\0\"                # regular file\\n lnktype = b\"1\"                  # link (inside tarfile)\\n symtype = b\"2\"                  # symbolic link\\n chrtype = b\"3\"                  # character special device\\n blktype = b\"4\"                  # block special device\\n dirtype = b\"5\"                  # directory\\n fifotype = b\"6\"                 # fifo special device\\n conttype = b\"7\"                 # contiguous file\\n gnutype_longname = b\"l\"         # gnu tar longname\\n gnutype_longlink = b\"k\"         # gnu tar longlink\\n gnutype_sparse = b\"s\"           # gnu tar sparse file\\n xhdtype = b\"x\"                  # posix.1-2001 extended header\\n xgltype = b\"g\"                  # posix.1-2001 global header\\n solaris_xhdtype = b\"x\"          # solaris extended header\\n ustar_format = 0                # posix.1-1988 (ustar) format\\n gnu_format = 1                  # gnu tar format\\n pax_format = 2                  # posix.1-2001 (pax) format\\n default_format = gnu_format\\n #---------------------------------------------------------\\n # tarfile constants\\n #---------------------------------------------------------\\n # file types that tarfile supports:\\n supported_types = (regtype, aregtype, lnktype,\\n symtype, dirtype, fifotype,\\n conttype, chrtype, blktype,\\n gnutype_longname, gnutype_longlink,\\n gnutype_sparse)\\n # file types that will be treated as a regular file.\\n regular_types = (regtype, aregtype,\\n conttype, gnutype_sparse)\\n # file types that are part of the gnu tar format.\\n gnu_types = (gnutype_longname, gnutype_longlink,\\n gnutype_sparse)\\n # fields from a pax header that override a tarinfo attribute.\\n pax_fields = (\"path\", \"linkpath\", \"size\", \"mtime\",\\n \"uid\", \"gid\", \"uname\", \"gname\")\\n # fields from a pax header that are affected by hdrcharset.\\n pax_name_fields = {\"path\", \"linkpath\", \"uname\", \"gname\"}\\n # fields in a pax header that are numbers, all other fields\\n # are treated as strings.\\n pax_number_fields = {\\n \"atime\": float,\\n \"ctime\": float,\\n \"mtime\": float,\\n \"uid\": int,\\n \"gid\": int,\\n \"size\": int\\n }\\n #---------------------------------------------------------\\n # initialization\\n #---------------------------------------------------------\\n if os.name in (\"nt\", \"ce\"):\\n encoding = \"utf-8\"\\n else:\\n encoding = sys.getfilesystemencoding()\\n #---------------------------------------------------------\\n # some useful functions\\n #---------------------------------------------------------\\n def stn(s, length, encoding, errors):\\n \"\"\"convert a string to a null-terminated bytes object.\\n \"\"\"\\n s = s.encode(encoding, errors)\\n return s[:length] + (length - len(s)) * nul\\n def nts(s, encoding, errors):\\n \"\"\"convert a null-terminated bytes object to a string.\\n \"\"\"\\n p = s.find(b\"\\\\0\")\\n if p != -1:\\n s = s[:p]\\n return s.decode(encoding, errors)\\n def nti(s):\\n \"\"\"convert a number field to a python number.\\n \"\"\"\\n # there are two possible encodings for a number field, see\\n # itn() below.\\n if s[0] in (0o200, 0o377):\\n n = 0\\n for i in range(len(s) - 1):\\n n <<= 8\\n n += s[i + 1]\\n if s[0] == 0o377:\\n n = -(256 ** (len(s) - 1) - n)\\n else:\\n try:\\n s = nts(s, \"ascii\", \"strict\")\\n n = int(s.strip() or \"0\", 8)\\n except valueerror:\\n raise invalidheadererror(\"invalid header\")\\n return n\\n def itn(n, digits=8, format=default_format):\\n \"\"\"convert a python number to a number field.\\n \"\"\"\\n # posix 1003.1-1988 requires numbers to be encoded as a string of\\n # octal digits followed by a null-byte, this allows values up to\\n # (8**(digits-1))-1. gnu tar allows storing numbers greater than\\n # that if necessary. a leading 0o200 or 0o377 byte indicate this\\n # particular encoding, the following digits-1 bytes are a big-endian\\n # base-256 representation. this allows values up to (256**(digits-1))-1.\\n # a 0o200 byte indicates a positive number, a 0o377 byte a negative\\n # number.\\n if 0 <= n < 8 ** (digits - 1):\\n s = bytes(\"%0*o\" % (digits - 1, int(n)), \"ascii\") + nul\\n elif format == gnu_format and -256 ** (digits - 1) <= n < 256 ** (digits - 1):\\n if n >= 0:\\n s = bytearray([0o200])\\n else:\\n s = bytearray([0o377])\\n n = 256 ** digits + n\\n for i in range(digits - 1):\\n s.insert(1, n & 0o377)\\n n >>= 8\\n else:\\n raise valueerror(\"overflow in number field\")\\n return s\\n def calc_chksums(buf):\\n \"\"\"calculate the checksum for a member\\'s header by summing up all\\n characters except for the chksum field which is treated as if\\n it was filled with spaces. according to the gnu tar sources,\\n some tars (sun and next) calculate chksum with signed char,\\n which will be different if there are chars in the buffer with\\n the high bit set. so we calculate two checksums, unsigned and\\n signed.\\n \"\"\"\\n unsigned_chksum = 256 + sum(struct.unpack_from(\"148b8x356b\", buf))\\n signed_chksum = 256 + sum(struct.unpack_from(\"148b8x356b\", buf))\\n return unsigned_chksum, signed_chksum\\n def copyfileobj(src, dst, length=none, exception=oserror):\\n \"\"\"copy length bytes from fileobj src to fileobj dst.\\n if length is none, copy the entire content.\\n \"\"\"\\n if length == 0:\\n return\\n if length is none:\\n shutil.copyfileobj(src, dst)\\n return\\n bufsize = 16 * 1024\\n blocks, remainder = divmod(length, bufsize)\\n for b in range(blocks):\\n buf = src.read(bufsize)\\n if len(buf) < bufsize:\\n raise exception(\"unexpected end of data\")\\n dst.write(buf)\\n if remainder != 0:\\n buf = src.read(remainder)\\n if len(buf) < remainder:\\n raise exception(\"unexpected end of data\")\\n dst.write(buf)\\n return\\n def filemode(mode):\\n \"\"\"deprecated in this location; use stat.filemode.\"\"\"\\n import warnings\\n warnings.warn(\"deprecated in favor of stat.filemode\",\\n deprecationwarning, 2)\\n return stat.filemode(mode)\\n def _safe_print(s):\\n encoding = getattr(sys.stdout, \\'encoding\\', none)\\n if encoding is not none:\\n s = s.encode(encoding, \\'backslashreplace\\').decode(encoding)\\n print(s, end=\\' \\')\\n class tarerror(exception):\\n \"\"\"base exception.\"\"\"\\n pass\\n class extracterror(tarerror):\\n \"\"\"general exception for extract errors.\"\"\"\\n pass\\n class readerror(tarerror):\\n \"\"\"exception for unreadable tar archives.\"\"\"\\n pass\\n class compressionerror(tarerror):\\n \"\"\"exception for unavailable compression methods.\"\"\"\\n pass\\n class streamerror(tarerror):\\n \"\"\"exception for unsupported operations on stream-like tarfiles.\"\"\"\\n pass\\n class headererror(tarerror):\\n \"\"\"base exception for header errors.\"\"\"\\n pass\\n class emptyheadererror(headererror):\\n \"\"\"exception for empty headers.\"\"\"\\n pass\\n class truncatedheadererror(headererror):\\n \"\"\"exception for truncated headers.\"\"\"\\n pass\\n class eofheadererror(headererror):\\n \"\"\"exception for end of file headers.\"\"\"\\n pass\\n class invalidheadererror(headererror):\\n \"\"\"exception for invalid headers.\"\"\"\\n pass\\n class subsequentheadererror(headererror):\\n \"\"\"exception for missing and invalid extended headers.\"\"\"\\n pass\\n #---------------------------\\n # internal stream interface\\n #---------------------------\\n class _lowlevelfile:\\n \"\"\"low-level file object. supports reading and writing.\\n it is used instead of a regular file object for streaming\\n access.\\n \"\"\"\\n def __init__(self, name, mode):\\n mode = {\\n \"r\": os.o_rdonly,\\n \"w\": os.o_wronly | os.o_creat | os.o_trunc,\\n }[mode]\\n if hasattr(os, \"o_binary\"):\\n mode |= os.o_binary\\n self.fd = os.open(name, mode, 0o666)\\n def close(self):\\n os.close(self.fd)\\n def read(self, size):\\n return os.read(self.fd, size)\\n def write(self, s):\\n os.write(self.fd, s)\\n class _stream:\\n \"\"\"class that serves as an adapter between tarfile and\\n a stream-like object.  the stream-like object only\\n needs to have a read() or write() method and is accessed\\n blockwise.  use of gzip or bzip2 compression is possible.\\n a stream-like object could be for example: sys.stdin,\\n sys.stdout, a socket, a tape device etc.\\n _stream is intended to be used only internally.\\n \"\"\"\\n def __init__(self, name, mode, comptype, fileobj, bufsize):\\n \"\"\"construct a _stream object.\\n \"\"\"\\n self._extfileobj = true\\n if fileobj is none:\\n fileobj = _lowlevelfile(name, mode)\\n self._extfileobj = false\\n if comptype == \\'*\\':\\n # enable transparent compression detection for the\\n # stream interface\\n fileobj = _streamproxy(fileobj)\\n comptype = fileobj.getcomptype()\\n self.name     = name or \"\"\\n self.mode     = mode\\n self.comptype = comptype\\n self.fileobj  = fileobj\\n self.bufsize  = bufsize\\n self.buf      = b\"\"\\n self.pos      = 0\\n self.closed   = false\\n try:\\n if comptype == \"gz\":\\n try:\\n import zlib\\n except importerror:\\n raise compressionerror(\"zlib module is not available\")\\n self.zlib = zlib\\n self.crc = zlib.crc32(b\"\")\\n if mode == \"r\":\\n self._init_read_gz()\\n self.exception = zlib.error\\n else:\\n self._init_write_gz()\\n elif comptype == \"bz2\":\\n try:\\n import bz2\\n except importerror:\\n raise compressionerror(\"bz2 module is not available\")\\n if mode == \"r\":\\n self.dbuf = b\"\"\\n self.cmp = bz2.bz2decompressor()\\n self.exception = oserror\\n else:\\n self.cmp = bz2.bz2compressor()\\n elif comptype == \"xz\":\\n try:\\n import lzma\\n except importerror:\\n raise compressionerror(\"lzma module is not available\")\\n if mode == \"r\":\\n self.dbuf = b\"\"\\n self.cmp = lzma.lzmadecompressor()\\n self.exception = lzma.lzmaerror\\n else:\\n self.cmp = lzma.lzmacompressor()\\n elif comptype != \"tar\":\\n raise compressionerror(\"unknown compression type %r\" % comptype)\\n except:\\n if not self._extfileobj:\\n self.fileobj.close()\\n self.closed = true\\n raise\\n def __del__(self):\\n if hasattr(self, \"closed\") and not self.closed:\\n self.close()\\n def _init_write_gz(self):\\n \"\"\"initialize for writing with gzip compression.\\n \"\"\"\\n self.cmp = self.zlib.compressobj(9, self.zlib.deflated,\\n -self.zlib.max_wbits,\\n self.zlib.def_mem_level,\\n 0)\\n timestamp = struct.pack(\"<l\", int(time.time()))\\n self.__write(b\"\\\\037\\\\213\\\\010\\\\010\" + timestamp + b\"\\\\002\\\\377\")\\n if self.name.endswith(\".gz\"):\\n self.name = self.name[:-3]\\n # rfc1952 says we must use iso-8859-1 for the fname field.\\n self.__write(self.name.encode(\"iso-8859-1\", \"replace\") + nul)\\n def write(self, s):\\n \"\"\"write string s to the stream.\\n \"\"\"\\n if self.comptype == \"gz\":\\n self.crc = self.zlib.crc32(s, self.crc)\\n self.pos += len(s)\\n if self.comptype != \"tar\":\\n s = self.cmp.compress(s)\\n self.__write(s)\\n def __write(self, s):\\n \"\"\"write string s to the stream if a whole new block\\n is ready to be written.\\n \"\"\"\\n self.buf += s\\n while len(self.buf) > self.bufsize:\\n self.fileobj.write(self.buf[:self.bufsize])\\n self.buf = self.buf[self.bufsize:]\\n def close(self):\\n \"\"\"close the _stream object. no operation should be\\n done on it afterwards.\\n \"\"\"\\n if self.closed:\\n return\\n self.closed = true\\n try:\\n if self.mode == \"w\" and self.comptype != \"tar\":\\n self.buf += self.cmp.flush()\\n if self.mode == \"w\" and self.buf:\\n self.fileobj.write(self.buf)\\n self.buf = b\"\"\\n if self.comptype == \"gz\":\\n self.fileobj.write(struct.pack(\"<l\", self.crc))\\n self.fileobj.write(struct.pack(\"<l\", self.pos & 0xffffffff))\\n finally:\\n if not self._extfileobj:\\n self.fileobj.close()\\n def _init_read_gz(self):\\n \"\"\"initialize for reading a gzip compressed fileobj.\\n \"\"\"\\n self.cmp = self.zlib.decompressobj(-self.zlib.max_wbits)\\n self.dbuf = b\"\"\\n # taken from gzip.gzipfile with some alterations\\n if self.__read(2) != b\"\\\\037\\\\213\":\\n raise readerror(\"not a gzip file\")\\n if self.__read(1) != b\"\\\\010\":\\n raise compressionerror(\"unsupported compression method\")\\n flag = ord(self.__read(1))\\n self.__read(6)\\n if flag & 4:\\n xlen = ord(self.__read(1)) + 256 * ord(self.__read(1))\\n self.read(xlen)\\n if flag & 8:\\n while true:\\n s = self.__read(1)\\n if not s or s == nul:\\n break\\n if flag & 16:\\n while true:\\n s = self.__read(1)\\n if not s or s == nul:\\n break\\n if flag & 2:\\n self.__read(2)\\n def tell(self):\\n \"\"\"return the stream\\'s file pointer position.\\n \"\"\"\\n return self.pos\\n def seek(self, pos=0):\\n \"\"\"set the stream\\'s file pointer to pos. negative seeking\\n is forbidden.\\n \"\"\"\\n if pos - self.pos >= 0:\\n blocks, remainder = divmod(pos - self.pos, self.bufsize)\\n for i in range(blocks):\\n self.read(self.bufsize)\\n self.read(remainder)\\n else:\\n raise streamerror(\"seeking backwards is not allowed\")\\n return self.pos\\n def read(self, size=none):\\n \"\"\"return the next size number of bytes from the stream.\\n if size is not defined, return all bytes of the stream\\n up to eof.\\n \"\"\"\\n if size is none:\\n t = []\\n while true:\\n buf = self._read(self.bufsize)\\n if not buf:\\n break\\n t.append(buf)\\n buf = \"\".join(t)\\n else:\\n buf = self._read(size)\\n self.pos += len(buf)\\n return buf\\n def _read(self, size):\\n \"\"\"return size bytes from the stream.\\n \"\"\"\\n if self.comptype == \"tar\":\\n return self.__read(size)\\n c = len(self.dbuf)\\n while c < size:\\n buf = self.__read(self.bufsize)\\n if not buf:\\n break\\n try:\\n buf = self.cmp.decompress(buf)\\n except self.exception:\\n raise readerror(\"invalid compressed data\")\\n self.dbuf += buf\\n c += len(buf)\\n buf = self.dbuf[:size]\\n self.dbuf = self.dbuf[size:]\\n return buf\\n def __read(self, size):\\n \"\"\"return size bytes from stream. if internal buffer is empty,\\n read another block from the stream.\\n \"\"\"\\n c = len(self.buf)\\n while c < size:\\n buf = self.fileobj.read(self.bufsize)\\n if not buf:\\n break\\n self.buf += buf\\n c += len(buf)\\n buf = self.buf[:size]\\n self.buf = self.buf[size:]\\n return buf\\n # class _stream\\n class _streamproxy(object):\\n \"\"\"small proxy class that enables transparent compression\\n detection for the stream interface (mode \\'r|*\\').\\n \"\"\"\\n def __init__(self, fileobj):\\n self.fileobj = fileobj\\n self.buf = self.fileobj.read(blocksize)\\n def read(self, size):\\n self.read = self.fileobj.read\\n return self.buf\\n def getcomptype(self):\\n if self.buf.startswith(b\"\\\\x1f\\\\x8b\\\\x08\"):\\n return \"gz\"\\n elif self.buf[0:3] == b\"bzh\" and self.buf[4:10] == b\"1ay&sy\":\\n return \"bz2\"\\n elif self.buf.startswith((b\"\\\\x5d\\\\x00\\\\x00\\\\x80\", b\"\\\\xfd7zxz\")):\\n return \"xz\"\\n else:\\n return \"tar\"\\n def close(self):\\n self.fileobj.close()\\n # class streamproxy\\n #------------------------\\n # extraction file object\\n #------------------------\\n class _fileinfile(object):\\n \"\"\"a thin wrapper around an existing file object that\\n provides a part of its data as an individual file\\n object.\\n \"\"\"\\n def __init__(self, fileobj, offset, size, blockinfo=none):\\n self.fileobj = fileobj\\n self.offset = offset\\n self.size = size\\n self.position = 0\\n self.name = getattr(fileobj, \"name\", none)\\n self.closed = false\\n if blockinfo is none:\\n blockinfo = [(0, size)]\\n # construct a map with data and zero blocks.\\n self.map_index = 0\\n self.map = []\\n lastpos = 0\\n realpos = self.offset\\n for offset, size in blockinfo:\\n if offset > lastpos:\\n self.map.append((false, lastpos, offset, none))\\n self.map.append((true, offset, offset + size, realpos))\\n realpos += size\\n lastpos = offset + size\\n if lastpos < self.size:\\n self.map.append((false, lastpos, self.size, none))\\n def flush(self):\\n pass\\n def readable(self):\\n return true\\n def writable(self):\\n return false\\n def seekable(self):\\n return self.fileobj.seekable()\\n def tell(self):\\n \"\"\"return the current file position.\\n \"\"\"\\n return self.position\\n def seek(self, position, whence=io.seek_set):\\n \"\"\"seek to a position in the file.\\n \"\"\"\\n if whence == io.seek_set:\\n self.position = min(max(position, 0), self.size)\\n elif whence == io.seek_cur:\\n if position < 0:\\n self.position = max(self.position + position, 0)\\n else:\\n self.position = min(self.position + position, self.size)\\n elif whence == io.seek_end:\\n self.position = max(min(self.size + position, self.size), 0)\\n else:\\n raise valueerror(\"invalid argument\")\\n return self.position\\n def read(self, size=none):\\n \"\"\"read data from the file.\\n \"\"\"\\n if size is none:\\n size = self.size - self.position\\n else:\\n size = min(size, self.size - self.position)\\n buf = b\"\"\\n while size > 0:\\n while true:\\n data, start, stop, offset = self.map[self.map_index]\\n if start <= self.position < stop:\\n break\\n else:\\n self.map_index += 1\\n if self.map_index == len(self.map):\\n self.map_index = 0\\n length = min(size, stop - self.position)\\n if data:\\n self.fileobj.seek(offset + (self.position - start))\\n b = self.fileobj.read(length)\\n if len(b) != length:\\n raise readerror(\"unexpected end of data\")\\n buf += b\\n else:\\n buf += nul * length\\n size -= length\\n self.position += length\\n return buf\\n def readinto(self, b):\\n buf = self.read(len(b))\\n b[:len(buf)] = buf\\n return len(buf)\\n def close(self):\\n self.closed = true\\n #class _fileinfile\\n class exfileobject(io.bufferedreader):\\n def __init__(self, tarfile, tarinfo):\\n fileobj = _fileinfile(tarfile.fileobj, tarinfo.offset_data,\\n tarinfo.size, tarinfo.sparse)\\n super().__init__(fileobj)\\n #class exfileobject\\n #------------------\\n # exported classes\\n #------------------\\n class tarinfo(object):\\n \"\"\"informational class which holds the details about an\\n archive member given by a tar header block.\\n tarinfo objects are returned by tarfile.getmember(),\\n tarfile.getmembers() and tarfile.gettarinfo() and are\\n usually created internally.\\n \"\"\"\\n __slots__ = (\"name\", \"mode\", \"uid\", \"gid\", \"size\", \"mtime\",\\n \"chksum\", \"type\", \"linkname\", \"uname\", \"gname\",\\n \"devmajor\", \"devminor\",\\n \"offset\", \"offset_data\", \"pax_headers\", \"sparse\",\\n \"tarfile\", \"_sparse_structs\", \"_link_target\")\\n def __init__(self, name=\"\"):\\n \"\"\"construct a tarinfo object. name is the optional name\\n of the member.\\n \"\"\"\\n self.name = name        # member name\\n self.mode = 0o644       # file permissions\\n self.uid = 0            # user id\\n self.gid = 0            # group id\\n self.size = 0           # file size\\n self.mtime = 0          # modification time\\n self.chksum = 0         # header checksum\\n self.type = regtype     # member type\\n self.linkname = \"\"      # link name\\n self.uname = \"\"         # user name\\n self.gname = \"\"         # group name\\n self.devmajor = 0       # device major number\\n self.devminor = 0       # device minor number\\n self.offset = 0         # the tar header starts here\\n self.offset_data = 0    # the file\\'s data starts here\\n self.sparse = none      # sparse member information\\n self.pax_headers = {}   # pax header information\\n # in pax headers the \"name\" and \"linkname\" field are called\\n # \"path\" and \"linkpath\".\\n def _getpath(self):\\n return self.name\\n def _setpath(self, name):\\n self.name = name\\n path = property(_getpath, _setpath)\\n def _getlinkpath(self):\\n return self.linkname\\n def _setlinkpath(self, linkname):\\n self.linkname = linkname\\n linkpath = property(_getlinkpath, _setlinkpath)\\n def __repr__(self):\\n return \"<%s %r at %#x>\" % (self.__class__.__name__,self.name,id(self))\\n def get_info(self):\\n \"\"\"return the tarinfo\\'s attributes as a dictionary.\\n \"\"\"\\n info = {\\n \"name\":     self.name,\\n \"mode\":     self.mode & 0o7777,\\n \"uid\":      self.uid,\\n \"gid\":      self.gid,\\n \"size\":     self.size,\\n \"mtime\":    self.mtime,\\n \"chksum\":   self.chksum,\\n \"type\":     self.type,\\n \"linkname\": self.linkname,\\n \"uname\":    self.uname,\\n \"gname\":    self.gname,\\n \"devmajor\": self.devmajor,\\n \"devminor\": self.devminor\\n }\\n if info[\"type\"] == dirtype and not info[\"name\"].endswith(\"/\"):\\n info[\"name\"] += \"/\"\\n return info\\n def tobuf(self, format=default_format, encoding=encoding, errors=\"surrogateescape\"):\\n \"\"\"return a tar header as a string of 512 byte blocks.\\n \"\"\"\\n info = self.get_info()\\n if format == ustar_format:\\n return self.create_ustar_header(info, encoding, errors)\\n elif format == gnu_format:\\n return self.create_gnu_header(info, encoding, errors)\\n elif format == pax_format:\\n return self.create_pax_header(info, encoding)\\n else:\\n raise valueerror(\"invalid format\")\\n def create_ustar_header(self, info, encoding, errors):\\n \"\"\"return the object as a ustar header block.\\n \"\"\"\\n info[\"magic\"] = posix_magic\\n if len(info[\"linkname\"].encode(encoding, errors)) > length_link:\\n raise valueerror(\"linkname is too long\")\\n if len(info[\"name\"].encode(encoding, errors)) > length_name:\\n info[\"prefix\"], info[\"name\"] = self._posix_split_name(info[\"name\"], encoding, errors)\\n return self._create_header(info, ustar_format, encoding, errors)\\n def create_gnu_header(self, info, encoding, errors):\\n \"\"\"return the object as a gnu header block sequence.\\n \"\"\"\\n info[\"magic\"] = gnu_magic\\n buf = b\"\"\\n if len(info[\"linkname\"].encode(encoding, errors)) > length_link:\\n buf += self._create_gnu_long_header(info[\"linkname\"], gnutype_longlink, encoding, errors)\\n if len(info[\"name\"].encode(encoding, errors)) > length_name:\\n buf += self._create_gnu_long_header(info[\"name\"], gnutype_longname, encoding, errors)\\n return buf + self._create_header(info, gnu_format, encoding, errors)\\n def create_pax_header(self, info, encoding):\\n \"\"\"return the object as a ustar header block. if it cannot be\\n represented this way, prepend a pax extended header sequence\\n with supplement information.\\n \"\"\"\\n info[\"magic\"] = posix_magic\\n pax_headers = self.pax_headers.copy()\\n # test string fields for values that exceed the field length or cannot\\n # be represented in ascii encoding.\\n for name, hname, length in (\\n (\"name\", \"path\", length_name), (\"linkname\", \"linkpath\", length_link),\\n (\"uname\", \"uname\", 32), (\"gname\", \"gname\", 32)):\\n if hname in pax_headers:\\n # the pax header has priority.\\n continue\\n # try to encode the string as ascii.\\n try:\\n info[name].encode(\"ascii\", \"strict\")\\n except unicodeencodeerror:\\n pax_headers[hname] = info[name]\\n continue\\n if len(info[name]) > length:\\n pax_headers[hname] = info[name]\\n # test number fields for values that exceed the field limit or values\\n # that like to be stored as float.\\n for name, digits in ((\"uid\", 8), (\"gid\", 8), (\"size\", 12), (\"mtime\", 12)):\\n if name in pax_headers:\\n # the pax header has priority. avoid overflow.\\n info[name] = 0\\n continue\\n val = info[name]\\n if not 0 <= val < 8 ** (digits - 1) or isinstance(val, float):\\n pax_headers[name] = str(val)\\n info[name] = 0\\n # create a pax extended header if necessary.\\n if pax_headers:\\n buf = self._create_pax_generic_header(pax_headers, xhdtype, encoding)\\n else:\\n buf = b\"\"\\n return buf + self._create_header(info, ustar_format, \"ascii\", \"replace\")\\n @classmethod\\n def create_pax_global_header(cls, pax_headers):\\n \"\"\"return the object as a pax global header block sequence.\\n \"\"\"\\n return cls._create_pax_generic_header(pax_headers, xgltype, \"utf-8\")\\n def _posix_split_name(self, name, encoding, errors):\\n \"\"\"split a name longer than 100 chars into a prefix\\n and a name part.\\n \"\"\"\\n components = name.split(\"/\")\\n for i in range(1, len(components)):\\n prefix = \"/\".join(components[:i])\\n name = \"/\".join(components[i:])\\n if len(prefix.encode(encoding, errors)) <= length_prefix and \\\\\\n len(name.encode(encoding, errors)) <= length_name:\\n break\\n else:\\n raise valueerror(\"name is too long\")\\n return prefix, name\\n @staticmethod\\n def _create_header(info, format, encoding, errors):\\n \"\"\"return a header block. info is a dictionary with file\\n information, format must be one of the *_format constants.\\n \"\"\"\\n parts = [\\n stn(info.get(\"name\", \"\"), 100, encoding, errors),\\n itn(info.get(\"mode\", 0) & 0o7777, 8, format),\\n itn(info.get(\"uid\", 0), 8, format),\\n itn(info.get(\"gid\", 0), 8, format),\\n itn(info.get(\"size\", 0), 12, format),\\n itn(info.get(\"mtime\", 0), 12, format),\\n b\"        \", # checksum field\\n info.get(\"type\", regtype),\\n stn(info.get(\"linkname\", \"\"), 100, encoding, errors),\\n info.get(\"magic\", posix_magic),\\n stn(info.get(\"uname\", \"\"), 32, encoding, errors),\\n stn(info.get(\"gname\", \"\"), 32, encoding, errors),\\n itn(info.get(\"devmajor\", 0), 8, format),\\n itn(info.get(\"devminor\", 0), 8, format),\\n stn(info.get(\"prefix\", \"\"), 155, encoding, errors)\\n ]\\n buf = struct.pack(\"%ds\" % blocksize, b\"\".join(parts))\\n chksum = calc_chksums(buf[-blocksize:])[0]\\n buf = buf[:-364] + bytes(\"%06o\\\\0\" % chksum, \"ascii\") + buf[-357:]\\n return buf\\n @staticmethod\\n def _create_payload(payload):\\n \"\"\"return the string payload filled with zero bytes\\n up to the next 512 byte border.\\n \"\"\"\\n blocks, remainder = divmod(len(payload), blocksize)\\n if remainder > 0:\\n payload += (blocksize - remainder) * nul\\n return payload\\n @classmethod\\n def _create_gnu_long_header(cls, name, type, encoding, errors):\\n \"\"\"return a gnutype_longname or gnutype_longlink sequence\\n for name.\\n \"\"\"\\n name = name.encode(encoding, errors) + nul\\n info = {}\\n info[\"name\"] = \"././@longlink\"\\n info[\"type\"] = type\\n info[\"size\"] = len(name)\\n info[\"magic\"] = gnu_magic\\n # create extended header + name blocks.\\n return cls._create_header(info, ustar_format, encoding, errors) + \\\\\\n cls._create_payload(name)\\n @classmethod\\n def _create_pax_generic_header(cls, pax_headers, type, encoding):\\n \"\"\"return a posix.1-2008 extended or global header sequence\\n that contains a list of keyword, value pairs. the values\\n must be strings.\\n \"\"\"\\n # check if one of the fields contains surrogate characters and thereby\\n # forces hdrcharset=binary, see _proc_pax() for more information.\\n binary = false\\n for keyword, value in pax_headers.items():\\n try:\\n value.encode(\"utf-8\", \"strict\")\\n except unicodeencodeerror:\\n binary = true\\n break\\n records = b\"\"\\n if binary:\\n # put the hdrcharset field at the beginning of the header.\\n records += b\"21 hdrcharset=binary\\\\n\"\\n for keyword, value in pax_headers.items():\\n keyword = keyword.encode(\"utf-8\")\\n if binary:\\n # try to restore the original byte representation of `value\\'.\\n # needless to say, that the encoding must match the string.\\n value = value.encode(encoding, \"surrogateescape\")\\n else:\\n value = value.encode(\"utf-8\")\\n l = len(keyword) + len(value) + 3   # \\' \\' + \\'=\\' + \\'\\\\n\\'\\n n = p = 0\\n while true:\\n n = l + len(str(p))\\n if n == p:\\n break\\n p = n\\n records += bytes(str(p), \"ascii\") + b\" \" + keyword + b\"=\" + value + b\"\\\\n\"\\n # we use a hardcoded \"././@paxheader\" name like star does\\n # instead of the one that posix recommends.\\n info = {}\\n info[\"name\"] = \"././@paxheader\"\\n info[\"type\"] = type\\n info[\"size\"] = len(records)\\n info[\"magic\"] = posix_magic\\n # create pax header + record blocks.\\n return cls._create_header(info, ustar_format, \"ascii\", \"replace\") + \\\\\\n cls._create_payload(records)\\n @classmethod\\n def frombuf(cls, buf, encoding, errors):\\n \"\"\"construct a tarinfo object from a 512 byte bytes object.\\n \"\"\"\\n if len(buf) == 0:\\n raise emptyheadererror(\"empty header\")\\n if len(buf) != blocksize:\\n raise truncatedheadererror(\"truncated header\")\\n if buf.count(nul) == blocksize:\\n raise eofheadererror(\"end of file header\")\\n chksum = nti(buf[148:156])\\n if chksum not in calc_chksums(buf):\\n raise invalidheadererror(\"bad checksum\")\\n obj = cls()\\n obj.name = nts(buf[0:100], encoding, errors)\\n obj.mode = nti(buf[100:108])\\n obj.uid = nti(buf[108:116])\\n obj.gid = nti(buf[116:124])\\n obj.size = nti(buf[124:136])\\n obj.mtime = nti(buf[136:148])\\n obj.chksum = chksum\\n obj.type = buf[156:157]\\n obj.linkname = nts(buf[157:257], encoding, errors)\\n obj.uname = nts(buf[265:297], encoding, errors)\\n obj.gname = nts(buf[297:329], encoding, errors)\\n obj.devmajor = nti(buf[329:337])\\n obj.devminor = nti(buf[337:345])\\n prefix = nts(buf[345:500], encoding, errors)\\n # old v7 tar format represents a directory as a regular\\n # file with a trailing slash.\\n if obj.type == aregtype and obj.name.endswith(\"/\"):\\n obj.type = dirtype\\n # the old gnu sparse format occupies some of the unused\\n # space in the buffer for up to 4 sparse structures.\\n # save the them for later processing in _proc_sparse().\\n if obj.type == gnutype_sparse:\\n pos = 386\\n structs = []\\n for i in range(4):\\n try:\\n offset = nti(buf[pos:pos + 12])\\n numbytes = nti(buf[pos + 12:pos + 24])\\n except valueerror:\\n break\\n structs.append((offset, numbytes))\\n pos += 24\\n isextended = bool(buf[482])\\n origsize = nti(buf[483:495])\\n obj._sparse_structs = (structs, isextended, origsize)\\n # remove redundant slashes from directories.\\n if obj.isdir():\\n obj.name = obj.name.rstrip(\"/\")\\n # reconstruct a ustar longname.\\n if prefix and obj.type not in gnu_types:\\n obj.name = prefix + \"/\" + obj.name\\n return obj\\n @classmethod\\n def fromtarfile(cls, tarfile):\\n \"\"\"return the next tarinfo object from tarfile object\\n tarfile.\\n \"\"\"\\n buf = tarfile.fileobj.read(blocksize)\\n obj = cls.frombuf(buf, tarfile.encoding, tarfile.errors)\\n obj.offset = tarfile.fileobj.tell() - blocksize\\n return obj._proc_member(tarfile)\\n #--------------------------------------------------------------------------\\n # the following are methods that are called depending on the type of a\\n # member. the entry point is _proc_member() which can be overridden in a\\n # subclass to add custom _proc_*() methods. a _proc_*() method must\\n # implement the following\\n # operations:\\n # 1. set self.offset_data to the position where the data blocks begin,\\n #    if there is data that follows.\\n # 2. set tarfile.offset to the position where the next member\\'s header will\\n #    begin.\\n # 3. return self or another valid tarinfo object.\\n def _proc_member(self, tarfile):\\n \"\"\"choose the right processing method depending on\\n the type and call it.\\n \"\"\"\\n if self.type in (gnutype_longname, gnutype_longlink):\\n return self._proc_gnulong(tarfile)\\n elif self.type == gnutype_sparse:\\n return self._proc_sparse(tarfile)\\n elif self.type in (xhdtype, xgltype, solaris_xhdtype):\\n return self._proc_pax(tarfile)\\n else:\\n return self._proc_builtin(tarfile)\\n def _proc_builtin(self, tarfile):\\n \"\"\"process a builtin type or an unknown type which\\n will be treated as a regular file.\\n \"\"\"\\n self.offset_data = tarfile.fileobj.tell()\\n offset = self.offset_data\\n if self.isreg() or self.type not in supported_types:\\n # skip the following data blocks.\\n offset += self._block(self.size)\\n tarfile.offset = offset\\n # patch the tarinfo object with saved global\\n # header information.\\n self._apply_pax_info(tarfile.pax_headers, tarfile.encoding, tarfile.errors)\\n return self\\n def _proc_gnulong(self, tarfile):\\n \"\"\"process the blocks that hold a gnu longname\\n or longlink member.\\n \"\"\"\\n buf = tarfile.fileobj.read(self._block(self.size))\\n # fetch the next header and process it.\\n try:\\n next = self.fromtarfile(tarfile)\\n except headererror:\\n raise subsequentheadererror(\"missing or bad subsequent header\")\\n # patch the tarinfo object from the next header with\\n # the longname information.\\n next.offset = self.offset\\n if self.type == gnutype_longname:\\n next.name = nts(buf, tarfile.encoding, tarfile.errors)\\n elif self.type == gnutype_longlink:\\n next.linkname = nts(buf, tarfile.encoding, tarfile.errors)\\n return next\\n def _proc_sparse(self, tarfile):\\n \"\"\"process a gnu sparse header plus extra headers.\\n \"\"\"\\n # we already collected some sparse structures in frombuf().\\n structs, isextended, origsize = self._sparse_structs\\n del self._sparse_structs\\n # collect sparse structures from extended header blocks.\\n while isextended:\\n buf = tarfile.fileobj.read(blocksize)\\n pos = 0\\n for i in range(21):\\n try:\\n offset = nti(buf[pos:pos + 12])\\n numbytes = nti(buf[pos + 12:pos + 24])\\n except valueerror:\\n break\\n if offset and numbytes:\\n structs.append((offset, numbytes))\\n pos += 24\\n isextended = bool(buf[504])\\n self.sparse = structs\\n self.offset_data = tarfile.fileobj.tell()\\n tarfile.offset = self.offset_data + self._block(self.size)\\n self.size = origsize\\n return self\\n def _proc_pax(self, tarfile):\\n \"\"\"process an extended or global header as described in\\n posix.1-2008.\\n \"\"\"\\n # read the header information.\\n buf = tarfile.fileobj.read(self._block(self.size))\\n # a pax header stores supplemental information for either\\n # the following file (extended) or all following files\\n # (global).\\n if self.type == xgltype:\\n pax_headers = tarfile.pax_headers\\n else:\\n pax_headers = tarfile.pax_headers.copy()\\n # check if the pax header contains a hdrcharset field. this tells us\\n # the encoding of the path, linkpath, uname and gname fields. normally,\\n # these fields are utf-8 encoded but since posix.1-2008 tar\\n # implementations are allowed to store them as raw binary strings if\\n # the translation to utf-8 fails.\\n match = re.search(br\"\\\\d+ hdrcharset=([^\\\\n]+)\\\\n\", buf)\\n if match is not none:\\n pax_headers[\"hdrcharset\"] = match.group(1).decode(\"utf-8\")\\n # for the time being, we don\\'t care about anything other than \"binary\".\\n # the only other value that is currently allowed by the standard is\\n # \"iso-ir 10646 2000 utf-8\" in other words utf-8.\\n hdrcharset = pax_headers.get(\"hdrcharset\")\\n if hdrcharset == \"binary\":\\n encoding = tarfile.encoding\\n else:\\n encoding = \"utf-8\"\\n # parse pax header information. a record looks like that:\\n # \"%d %s=%s\\\\n\" % (length, keyword, value). length is the size\\n # of the complete record including the length field itself and\\n # the newline. keyword and value are both utf-8 encoded strings.\\n regex = re.compile(br\"(\\\\d+) ([^=]+)=\")\\n pos = 0\\n while true:\\n match = regex.match(buf, pos)\\n if not match:\\n break\\n length, keyword = match.groups()\\n length = int(length)\\n value = buf[match.end(2) + 1:match.start(1) + length - 1]\\n # normally, we could just use \"utf-8\" as the encoding and \"strict\"\\n # as the error handler, but we better not take the risk. for\\n # example, gnu tar <= 1.23 is known to store filenames it cannot\\n # translate to utf-8 as raw strings (unfortunately without a\\n # hdrcharset=binary header).\\n # we first try the strict standard encoding, and if that fails we\\n # fall back on the user\\'s encoding and error handler.\\n keyword = self._decode_pax_field(keyword, \"utf-8\", \"utf-8\",\\n tarfile.errors)\\n if keyword in pax_name_fields:\\n value = self._decode_pax_field(value, encoding, tarfile.encoding,\\n tarfile.errors)\\n else:\\n value = self._decode_pax_field(value, \"utf-8\", \"utf-8\",\\n tarfile.errors)\\n pax_headers[keyword] = value\\n pos += length\\n # fetch the next header.\\n try:\\n next = self.fromtarfile(tarfile)\\n except headererror:\\n raise subsequentheadererror(\"missing or bad subsequent header\")\\n # process gnu sparse information.\\n if \"gnu.sparse.map\" in pax_headers:\\n # gnu extended sparse format version 0.1.\\n self._proc_gnusparse_01(next, pax_headers)\\n elif \"gnu.sparse.size\" in pax_headers:\\n # gnu extended sparse format version 0.0.\\n self._proc_gnusparse_00(next, pax_headers, buf)\\n elif pax_headers.get(\"gnu.sparse.major\") == \"1\" and pax_headers.get(\"gnu.sparse.minor\") == \"0\":\\n # gnu extended sparse format version 1.0.\\n self._proc_gnusparse_10(next, pax_headers, tarfile)\\n if self.type in (xhdtype, solaris_xhdtype):\\n # patch the tarinfo object with the extended header info.\\n next._apply_pax_info(pax_headers, tarfile.encoding, tarfile.errors)\\n next.offset = self.offset\\n if \"size\" in pax_headers:\\n # if the extended header replaces the size field,\\n # we need to recalculate the offset where the next\\n # header starts.\\n offset = next.offset_data\\n if next.isreg() or next.type not in supported_types:\\n offset += next._block(next.size)\\n tarfile.offset = offset\\n return next\\n def _proc_gnusparse_00(self, next, pax_headers, buf):\\n \"\"\"process a gnu tar extended sparse header, version 0.0.\\n \"\"\"\\n offsets = []\\n for match in re.finditer(br\"\\\\d+ gnu.sparse.offset=(\\\\d+)\\\\n\", buf):\\n offsets.append(int(match.group(1)))\\n numbytes = []\\n for match in re.finditer(br\"\\\\d+ gnu.sparse.numbytes=(\\\\d+)\\\\n\", buf):\\n numbytes.append(int(match.group(1)))\\n next.sparse = list(zip(offsets, numbytes))\\n def _proc_gnusparse_01(self, next, pax_headers):\\n \"\"\"process a gnu tar extended sparse header, version 0.1.\\n \"\"\"\\n sparse = [int(x) for x in pax_headers[\"gnu.sparse.map\"].split(\",\")]\\n next.sparse = list(zip(sparse[::2], sparse[1::2]))\\n def _proc_gnusparse_10(self, next, pax_headers, tarfile):\\n \"\"\"process a gnu tar extended sparse header, version 1.0.\\n \"\"\"\\n fields = none\\n sparse = []\\n buf = tarfile.fileobj.read(blocksize)\\n fields, buf = buf.split(b\"\\\\n\", 1)\\n fields = int(fields)\\n while len(sparse) < fields * 2:\\n if b\"\\\\n\" not in buf:\\n buf += tarfile.fileobj.read(blocksize)\\n number, buf = buf.split(b\"\\\\n\", 1)\\n sparse.append(int(number))\\n next.offset_data = tarfile.fileobj.tell()\\n next.sparse = list(zip(sparse[::2], sparse[1::2]))\\n def _apply_pax_info(self, pax_headers, encoding, errors):\\n \"\"\"replace fields with supplemental information from a previous\\n pax extended or global header.\\n \"\"\"\\n for keyword, value in pax_headers.items():\\n if keyword == \"gnu.sparse.name\":\\n setattr(self, \"path\", value)\\n elif keyword == \"gnu.sparse.size\":\\n setattr(self, \"size\", int(value))\\n elif keyword == \"gnu.sparse.realsize\":\\n setattr(self, \"size\", int(value))\\n elif keyword in pax_fields:\\n if keyword in pax_number_fields:\\n try:\\n value = pax_number_fields[keyword](value)\\n except valueerror:\\n value = 0\\n if keyword == \"path\":\\n value = value.rstrip(\"/\")\\n setattr(self, keyword, value)\\n self.pax_headers = pax_headers.copy()\\n def _decode_pax_field(self, value, encoding, fallback_encoding, fallback_errors):\\n \"\"\"decode a single field from a pax record.\\n \"\"\"\\n try:\\n return value.decode(encoding, \"strict\")\\n except unicodedecodeerror:\\n return value.decode(fallback_encoding, fallback_errors)\\n def _block(self, count):\\n \"\"\"round up a byte count by blocksize and return it,\\n e.g. _block(834) => 1024.\\n \"\"\"\\n blocks, remainder = divmod(count, blocksize)\\n if remainder:\\n blocks += 1\\n return blocks * blocksize\\n def isreg(self):\\n return self.type in regular_types\\n def isfile(self):\\n return self.isreg()\\n def isdir(self):\\n return self.type == dirtype\\n def issym(self):\\n return self.type == symtype\\n def islnk(self):\\n return self.type == lnktype\\n def ischr(self):\\n return self.type == chrtype\\n def isblk(self):\\n return self.type == blktype\\n def isfifo(self):\\n return self.type == fifotype\\n def issparse(self):\\n return self.sparse is not none\\n def isdev(self):\\n return self.type in (chrtype, blktype, fifotype)\\n # class tarinfo\\n class tarfile(object):\\n \"\"\"the tarfile class provides an interface to tar archives.\\n \"\"\"\\n debug = 0                   # may be set from 0 (no msgs) to 3 (all msgs)\\n dereference = false         # if true, add content of linked file to the\\n # tar file, else the link.\\n ignore_zeros = false        # if true, skips empty or invalid blocks and\\n # continues processing.\\n errorlevel = 1              # if 0, fatal errors only appear in debug\\n # messages (if debug >= 0). if > 0, errors\\n # are passed to the caller as exceptions.\\n format = default_format     # the format to use when creating an archive.\\n encoding = encoding         # encoding for 8-bit character strings.\\n errors = none               # error handler for unicode conversion.\\n tarinfo = tarinfo           # the default tarinfo class to use.\\n fileobject = exfileobject   # the file-object for extractfile().\\n def __init__(self, name=none, mode=\"r\", fileobj=none, format=none,\\n tarinfo=none, dereference=none, ignore_zeros=none, encoding=none,\\n errors=\"surrogateescape\", pax_headers=none, debug=none, errorlevel=none):\\n \"\"\"open an (uncompressed) tar archive `name\\'. `mode\\' is either \\'r\\' to\\n read from an existing archive, \\'a\\' to append data to an existing\\n file or \\'w\\' to create a new file overwriting an existing one. `mode\\'\\n defaults to \\'r\\'.\\n if `fileobj\\' is given, it is used for reading or writing data. if it\\n can be determined, `mode\\' is overridden by `fileobj\\'s mode.\\n `fileobj\\' is not closed, when tarfile is closed.\\n \"\"\"\\n modes = {\"r\": \"rb\", \"a\": \"r+b\", \"w\": \"wb\", \"x\": \"xb\"}\\n if mode not in modes:\\n raise valueerror(\"mode must be \\'r\\', \\'a\\', \\'w\\' or \\'x\\'\")\\n self.mode = mode\\n self._mode = modes[mode]\\n if not fileobj:\\n if self.mode == \"a\" and not os.path.exists(name):\\n # create nonexistent files in append mode.\\n self.mode = \"w\"\\n self._mode = \"wb\"\\n fileobj = bltn_open(name, self._mode)\\n self._extfileobj = false\\n else:\\n if (name is none and hasattr(fileobj, \"name\") and\\n isinstance(fileobj.name, (str, bytes))):\\n name = fileobj.name\\n if hasattr(fileobj, \"mode\"):\\n self._mode = fileobj.mode\\n self._extfileobj = true\\n self.name = os.path.abspath(name) if name else none\\n self.fileobj = fileobj\\n # init attributes.\\n if format is not none:\\n self.format = format\\n if tarinfo is not none:\\n self.tarinfo = tarinfo\\n if dereference is not none:\\n self.dereference = dereference\\n if ignore_zeros is not none:\\n self.ignore_zeros = ignore_zeros\\n if encoding is not none:\\n self.encoding = encoding\\n self.errors = errors\\n if pax_headers is not none and self.format == pax_format:\\n self.pax_headers = pax_headers\\n else:\\n self.pax_headers = {}\\n if debug is not none:\\n self.debug = debug\\n if errorlevel is not none:\\n self.errorlevel = errorlevel\\n # init datastructures.\\n self.closed = false\\n self.members = []       # list of members as tarinfo objects\\n self._loaded = false    # flag if all members have been read\\n self.offset = self.fileobj.tell()\\n # current position in the archive file\\n self.inodes = {}        # dictionary caching the inodes of\\n # archive members already added\\n try:\\n if self.mode == \"r\":\\n self.firstmember = none\\n self.firstmember = self.next()\\n if self.mode == \"a\":\\n # move to the end of the archive,\\n # before the first empty block.\\n while true:\\n self.fileobj.seek(self.offset)\\n try:\\n tarinfo = self.tarinfo.fromtarfile(self)\\n self.members.append(tarinfo)\\n except eofheadererror:\\n self.fileobj.seek(self.offset)\\n break\\n except headererror as e:\\n raise readerror(str(e))\\n if self.mode in (\"a\", \"w\", \"x\"):\\n self._loaded = true\\n if self.pax_headers:\\n buf = self.tarinfo.create_pax_global_header(self.pax_headers.copy())\\n self.fileobj.write(buf)\\n self.offset += len(buf)\\n except:\\n if not self._extfileobj:\\n self.fileobj.close()\\n self.closed = true\\n raise\\n #--------------------------------------------------------------------------\\n # below are the classmethods which act as alternate constructors to the\\n # tarfile class. the open() method is the only one that is needed for\\n # public use; it is the \"super\"-constructor and is able to select an\\n # adequate \"sub\"-constructor for a particular compression using the mapping\\n # from open_meth.\\n #\\n # this concept allows one to subclass tarfile without losing the comfort of\\n # the super-constructor. a sub-constructor is registered and made available\\n # by adding it to the mapping in open_meth.\\n @classmethod\\n def open(cls, name=none, mode=\"r\", fileobj=none, bufsize=recordsize, **kwargs):\\n \"\"\"open a tar archive for reading, writing or appending. return\\n an appropriate tarfile class.\\n mode:\\n \\'r\\' or \\'r:*\\' open for reading with transparent compression\\n \\'r:\\'         open for reading exclusively uncompressed\\n \\'r:gz\\'       open for reading with gzip compression\\n \\'r:bz2\\'      open for reading with bzip2 compression\\n \\'r:xz\\'       open for reading with lzma compression\\n \\'a\\' or \\'a:\\'  open for appending, creating the file if necessary\\n \\'w\\' or \\'w:\\'  open for writing without compression\\n \\'w:gz\\'       open for writing with gzip compression\\n \\'w:bz2\\'      open for writing with bzip2 compression\\n \\'w:xz\\'       open for writing with lzma compression\\n \\'x\\' or \\'x:\\'  create a tarfile exclusively without compression, raise\\n an exception if the file is already created\\n \\'x:gz\\'       create a gzip compressed tarfile, raise an exception\\n if the file is already created\\n \\'x:bz2\\'      create a bzip2 compressed tarfile, raise an exception\\n if the file is already created\\n \\'x:xz\\'       create an lzma compressed tarfile, raise an exception\\n if the file is already created\\n \\'r|*\\'        open a stream of tar blocks with transparent compression\\n \\'r|\\'         open an uncompressed stream of tar blocks for reading\\n \\'r|gz\\'       open a gzip compressed stream of tar blocks\\n \\'r|bz2\\'      open a bzip2 compressed stream of tar blocks\\n \\'r|xz\\'       open an lzma compressed stream of tar blocks\\n \\'w|\\'         open an uncompressed stream for writing\\n \\'w|gz\\'       open a gzip compressed stream for writing\\n \\'w|bz2\\'      open a bzip2 compressed stream for writing\\n \\'w|xz\\'       open an lzma compressed stream for writing\\n \"\"\"\\n if not name and not fileobj:\\n raise valueerror(\"nothing to open\")\\n if mode in (\"r\", \"r:*\"):\\n # find out which *open() is appropriate for opening the file.\\n for comptype in cls.open_meth:\\n func = getattr(cls, cls.open_meth[comptype])\\n if fileobj is not none:\\n saved_pos = fileobj.tell()\\n try:\\n return func(name, \"r\", fileobj, **kwargs)\\n except (readerror, compressionerror) as e:\\n if fileobj is not none:\\n fileobj.seek(saved_pos)\\n continue\\n raise readerror(\"file could not be opened successfully\")\\n elif \":\" in mode:\\n filemode, comptype = mode.split(\":\", 1)\\n filemode = filemode or \"r\"\\n comptype = comptype or \"tar\"\\n # select the *open() function according to\\n # given compression.\\n if comptype in cls.open_meth:\\n func = getattr(cls, cls.open_meth[comptype])\\n else:\\n raise compressionerror(\"unknown compression type %r\" % comptype)\\n return func(name, filemode, fileobj, **kwargs)\\n elif \"|\" in mode:\\n filemode, comptype = mode.split(\"|\", 1)\\n filemode = filemode or \"r\"\\n comptype = comptype or \"tar\"\\n if filemode not in (\"r\", \"w\"):\\n raise valueerror(\"mode must be \\'r\\' or \\'w\\'\")\\n stream = _stream(name, filemode, comptype, fileobj, bufsize)\\n try:\\n t = cls(name, filemode, stream, **kwargs)\\n except:\\n stream.close()\\n raise\\n t._extfileobj = false\\n return t\\n elif mode in (\"a\", \"w\", \"x\"):\\n return cls.taropen(name, mode, fileobj, **kwargs)\\n raise valueerror(\"undiscernible mode\")\\n @classmethod\\n def taropen(cls, name, mode=\"r\", fileobj=none, **kwargs):\\n \"\"\"open uncompressed tar archive name for reading or writing.\\n \"\"\"\\n if mode not in (\"r\", \"a\", \"w\", \"x\"):\\n raise valueerror(\"mode must be \\'r\\', \\'a\\', \\'w\\' or \\'x\\'\")\\n return cls(name, mode, fileobj, **kwargs)\\n @classmethod\\n def gzopen(cls, name, mode=\"r\", fileobj=none, compresslevel=9, **kwargs):\\n \"\"\"open gzip compressed tar archive name for reading or writing.\\n appending is not allowed.\\n \"\"\"\\n if mode not in (\"r\", \"w\", \"x\"):\\n raise valueerror(\"mode must be \\'r\\', \\'w\\' or \\'x\\'\")\\n try:\\n import gzip\\n gzip.gzipfile\\n except (importerror, attributeerror):\\n raise compressionerror(\"gzip module is not available\")\\n try:\\n fileobj = gzip.gzipfile(name, mode + \"b\", compresslevel, fileobj)\\n except oserror:\\n if fileobj is not none and mode == \\'r\\':\\n raise readerror(\"not a gzip file\")\\n raise\\n try:\\n t = cls.taropen(name, mode, fileobj, **kwargs)\\n except oserror:\\n fileobj.close()\\n if mode == \\'r\\':\\n raise readerror(\"not a gzip file\")\\n raise\\n except:\\n fileobj.close()\\n raise\\n t._extfileobj = false\\n return t\\n @classmethod\\n def bz2open(cls, name, mode=\"r\", fileobj=none, compresslevel=9, **kwargs):\\n \"\"\"open bzip2 compressed tar archive name for reading or writing.\\n appending is not allowed.\\n \"\"\"\\n if mode not in (\"r\", \"w\", \"x\"):\\n raise valueerror(\"mode must be \\'r\\', \\'w\\' or \\'x\\'\")\\n try:\\n import bz2\\n except importerror:\\n raise compressionerror(\"bz2 module is not available\")\\n fileobj = bz2.bz2file(fileobj or name, mode,\\n compresslevel=compresslevel)\\n try:\\n t = cls.taropen(name, mode, fileobj, **kwargs)\\n except (oserror, eoferror):\\n fileobj.close()\\n if mode == \\'r\\':\\n raise readerror(\"not a bzip2 file\")\\n raise\\n except:\\n fileobj.close()\\n raise\\n t._extfileobj = false\\n return t\\n @classmethod\\n def xzopen(cls, name, mode=\"r\", fileobj=none, preset=none, **kwargs):\\n \"\"\"open lzma compressed tar archive name for reading or writing.\\n appending is not allowed.\\n \"\"\"\\n if mode not in (\"r\", \"w\", \"x\"):\\n raise valueerror(\"mode must be \\'r\\', \\'w\\' or \\'x\\'\")\\n try:\\n import lzma\\n except importerror:\\n raise compressionerror(\"lzma module is not available\")\\n fileobj = lzma.lzmafile(fileobj or name, mode, preset=preset)\\n try:\\n t = cls.taropen(name, mode, fileobj, **kwargs)\\n except (lzma.lzmaerror, eoferror):\\n fileobj.close()\\n if mode == \\'r\\':\\n raise readerror(\"not an lzma file\")\\n raise\\n except:\\n fileobj.close()\\n raise\\n t._extfileobj = false\\n return t\\n # all *open() methods are registered here.\\n open_meth = {\\n \"tar\": \"taropen\",   # uncompressed tar\\n \"gz\":  \"gzopen\",    # gzip compressed tar\\n \"bz2\": \"bz2open\",   # bzip2 compressed tar\\n \"xz\":  \"xzopen\"     # lzma compressed tar\\n }\\n #--------------------------------------------------------------------------\\n # the public methods which tarfile provides:\\n def close(self):\\n \"\"\"close the tarfile. in write-mode, two finishing zero blocks are\\n appended to the archive.\\n \"\"\"\\n if self.closed:\\n return\\n self.closed = true\\n try:\\n if self.mode in (\"a\", \"w\", \"x\"):\\n self.fileobj.write(nul * (blocksize * 2))\\n self.offset += (blocksize * 2)\\n # fill up the end with zero-blocks\\n # (like option -b20 for tar does)\\n blocks, remainder = divmod(self.offset, recordsize)\\n if remainder > 0:\\n self.fileobj.write(nul * (recordsize - remainder))\\n finally:\\n if not self._extfileobj:\\n self.fileobj.close()\\n def getmember(self, name):\\n \"\"\"return a tarinfo object for member `name\\'. if `name\\' can not be\\n found in the archive, keyerror is raised. if a member occurs more\\n than once in the archive, its last occurrence is assumed to be the\\n most up-to-date version.\\n \"\"\"\\n tarinfo = self._getmember(name)\\n if tarinfo is none:\\n raise keyerror(\"filename %r not found\" % name)\\n return tarinfo\\n def getmembers(self):\\n \"\"\"return the members of the archive as a list of tarinfo objects. the\\n list has the same order as the members in the archive.\\n \"\"\"\\n self._check()\\n if not self._loaded:    # if we want to obtain a list of\\n self._load()        # all members, we first have to\\n # scan the whole archive.\\n return self.members\\n def getnames(self):\\n \"\"\"return the members of the archive as a list of their names. it has\\n the same order as the list returned by getmembers().\\n \"\"\"\\n return [tarinfo.name for tarinfo in self.getmembers()]\\n def gettarinfo(self, name=none, arcname=none, fileobj=none):\\n \"\"\"create a tarinfo object from the result of os.stat or equivalent\\n on an existing file. the file is either named by `name\\', or\\n specified as a file object `fileobj\\' with a file descriptor. if\\n given, `arcname\\' specifies an alternative name for the file in the\\n archive, otherwise, the name is taken from the \\'name\\' attribute of\\n \\'fileobj\\', or the \\'name\\' argument. the name should be a text\\n string.\\n \"\"\"\\n self._check(\"awx\")\\n # when fileobj is given, replace name by\\n # fileobj\\'s real name.\\n if fileobj is not none:\\n name = fileobj.name\\n # building the name of the member in the archive.\\n # backward slashes are converted to forward slashes,\\n # absolute paths are turned to relative paths.\\n if arcname is none:\\n arcname = name\\n drv, arcname = os.path.splitdrive(arcname)\\n arcname = arcname.replace(os.sep, \"/\")\\n arcname = arcname.lstrip(\"/\")\\n # now, fill the tarinfo object with\\n # information specific for the file.\\n tarinfo = self.tarinfo()\\n tarinfo.tarfile = self  # not needed\\n # use os.stat or os.lstat, depending on platform\\n # and if symlinks shall be resolved.\\n if fileobj is none:\\n if hasattr(os, \"lstat\") and not self.dereference:\\n statres = os.lstat(name)\\n else:\\n statres = os.stat(name)\\n else:\\n statres = os.fstat(fileobj.fileno())\\n linkname = \"\"\\n stmd = statres.st_mode\\n if stat.s_isreg(stmd):\\n inode = (statres.st_ino, statres.st_dev)\\n if not self.dereference and statres.st_nlink > 1 and \\\\\\n inode in self.inodes and arcname != self.inodes[inode]:\\n # is it a hardlink to an already\\n # archived file?\\n type = lnktype\\n linkname = self.inodes[inode]\\n else:\\n # the inode is added only if its valid.\\n # for win32 it is always 0.\\n type = regtype\\n if inode[0]:\\n self.inodes[inode] = arcname\\n elif stat.s_isdir(stmd):\\n type = dirtype\\n elif stat.s_isfifo(stmd):\\n type = fifotype\\n elif stat.s_islnk(stmd):\\n type = symtype\\n linkname = os.readlink(name)\\n elif stat.s_ischr(stmd):\\n type = chrtype\\n elif stat.s_isblk(stmd):\\n type = blktype\\n else:\\n return none\\n # fill the tarinfo object with all\\n # information we can get.\\n tarinfo.name = arcname\\n tarinfo.mode = stmd\\n tarinfo.uid = statres.st_uid\\n tarinfo.gid = statres.st_gid\\n if type == regtype:\\n tarinfo.size = statres.st_size\\n else:\\n tarinfo.size = 0\\n tarinfo.mtime = statres.st_mtime\\n tarinfo.type = type\\n tarinfo.linkname = linkname\\n if pwd:\\n try:\\n tarinfo.uname = pwd.getpwuid(tarinfo.uid)[0]\\n except keyerror:\\n pass\\n if grp:\\n try:\\n tarinfo.gname = grp.getgrgid(tarinfo.gid)[0]\\n except keyerror:\\n pass\\n if type in (chrtype, blktype):\\n if hasattr(os, \"major\") and hasattr(os, \"minor\"):\\n tarinfo.devmajor = os.major(statres.st_rdev)\\n tarinfo.devminor = os.minor(statres.st_rdev)\\n return tarinfo\\n def list(self, verbose=true, *, members=none):\\n \"\"\"print a table of contents to sys.stdout. if `verbose\\' is false, only\\n the names of the members are printed. if it is true, an `ls -l\\'-like\\n output is produced. `members\\' is optional and must be a subset of the\\n list returned by getmembers().\\n \"\"\"\\n self._check()\\n if members is none:\\n members = self\\n for tarinfo in members:\\n if verbose:\\n _safe_print(stat.filemode(tarinfo.mode))\\n _safe_print(\"%s/%s\" % (tarinfo.uname or tarinfo.uid,\\n tarinfo.gname or tarinfo.gid))\\n if tarinfo.ischr() or tarinfo.isblk():\\n _safe_print(\"%10s\" %\\n (\"%d,%d\" % (tarinfo.devmajor, tarinfo.devminor)))\\n else:\\n _safe_print(\"%10d\" % tarinfo.size)\\n _safe_print(\"%d-%02d-%02d %02d:%02d:%02d\" \\\\\\n % time.localtime(tarinfo.mtime)[:6])\\n _safe_print(tarinfo.name + (\"/\" if tarinfo.isdir() else \"\"))\\n if verbose:\\n if tarinfo.issym():\\n _safe_print(\"-> \" + tarinfo.linkname)\\n if tarinfo.islnk():\\n _safe_print(\"link to \" + tarinfo.linkname)\\n print()\\n def add(self, name, arcname=none, recursive=true, exclude=none, *, filter=none):\\n \"\"\"add the file `name\\' to the archive. `name\\' may be any type of file\\n (directory, fifo, symbolic link, etc.). if given, `arcname\\'\\n specifies an alternative name for the file in the archive.\\n directories are added recursively by default. this can be avoided by\\n setting `recursive\\' to false. `exclude\\' is a function that should\\n return true for each filename to be excluded. `filter\\' is a function\\n that expects a tarinfo object argument and returns the changed\\n tarinfo object, if it returns none the tarinfo object will be\\n excluded from the archive.\\n \"\"\"\\n self._check(\"awx\")\\n if arcname is none:\\n arcname = name\\n # exclude pathnames.\\n if exclude is not none:\\n import warnings\\n warnings.warn(\"use the filter argument instead\",\\n deprecationwarning, 2)\\n if exclude(name):\\n self._dbg(2, \"tarfile: excluded %r\" % name)\\n return\\n # skip if somebody tries to archive the archive...\\n if self.name is not none and os.path.abspath(name) == self.name:\\n self._dbg(2, \"tarfile: skipped %r\" % name)\\n return\\n self._dbg(1, name)\\n # create a tarinfo object from the file.\\n tarinfo = self.gettarinfo(name, arcname)\\n if tarinfo is none:\\n self._dbg(1, \"tarfile: unsupported type %r\" % name)\\n return\\n # change or exclude the tarinfo object.\\n if filter is not none:\\n tarinfo = filter(tarinfo)\\n if tarinfo is none:\\n self._dbg(2, \"tarfile: excluded %r\" % name)\\n return\\n # append the tar header and data to the archive.\\n if tarinfo.isreg():\\n with bltn_open(name, \"rb\") as f:\\n self.addfile(tarinfo, f)\\n elif tarinfo.isdir():\\n self.addfile(tarinfo)\\n if recursive:\\n for f in os.listdir(name):\\n self.add(os.path.join(name, f), os.path.join(arcname, f),\\n recursive, exclude, filter=filter)\\n else:\\n self.addfile(tarinfo)\\n def addfile(self, tarinfo, fileobj=none):\\n \"\"\"add the tarinfo object `tarinfo\\' to the archive. if `fileobj\\' is\\n given, it should be a binary file, and tarinfo.size bytes are read\\n from it and added to the archive. you can create tarinfo objects\\n directly, or by using gettarinfo().\\n \"\"\"\\n self._check(\"awx\")\\n tarinfo = copy.copy(tarinfo)\\n buf = tarinfo.tobuf(self.format, self.encoding, self.errors)\\n self.fileobj.write(buf)\\n self.offset += len(buf)\\n # if there\\'s data to follow, append it.\\n if fileobj is not none:\\n copyfileobj(fileobj, self.fileobj, tarinfo.size)\\n blocks, remainder = divmod(tarinfo.size, blocksize)\\n if remainder > 0:\\n self.fileobj.write(nul * (blocksize - remainder))\\n blocks += 1\\n self.offset += blocks * blocksize\\n self.members.append(tarinfo)\\n def extractall(self, path=\".\", members=none, *, numeric_owner=false):\\n \"\"\"extract all members from the archive to the current working\\n directory and set owner, modification time and permissions on\\n directories afterwards. `path\\' specifies a different directory\\n to extract to. `members\\' is optional and must be a subset of the\\n list returned by getmembers(). if `numeric_owner` is true, only\\n the numbers for user/group names are used and not the names.\\n \"\"\"\\n directories = []\\n if members is none:\\n members = self\\n for tarinfo in members:\\n if tarinfo.isdir():\\n # extract directories with a safe mode.\\n directories.append(tarinfo)\\n tarinfo = copy.copy(tarinfo)\\n tarinfo.mode = 0o700\\n # do not set_attrs directories, as we will do that further down\\n self.extract(tarinfo, path, set_attrs=not tarinfo.isdir(),\\n numeric_owner=numeric_owner)\\n # reverse sort directories.\\n directories.sort(key=lambda a: a.name)\\n directories.reverse()\\n # set correct owner, mtime and filemode on directories.\\n for tarinfo in directories:\\n dirpath = os.path.join(path, tarinfo.name)\\n try:\\n self.chown(tarinfo, dirpath, numeric_owner=numeric_owner)\\n self.utime(tarinfo, dirpath)\\n self.chmod(tarinfo, dirpath)\\n except extracterror as e:\\n if self.errorlevel > 1:\\n raise\\n else:\\n self._dbg(1, \"tarfile: %s\" % e)\\n def extract(self, member, path=\"\", set_attrs=true, *, numeric_owner=false):\\n \"\"\"extract a member from the archive to the current working directory,\\n using its full name. its file information is extracted as accurately\\n as possible. `member\\' may be a filename or a tarinfo object. you can\\n specify a different directory using `path\\'. file attributes (owner,\\n mtime, mode) are set unless `set_attrs\\' is false. if `numeric_owner`\\n is true, only the numbers for user/group names are used and not\\n the names.\\n \"\"\"\\n self._check(\"r\")\\n if isinstance(member, str):\\n tarinfo = self.getmember(member)\\n else:\\n tarinfo = member\\n # prepare the link target for makelink().\\n if tarinfo.islnk():\\n tarinfo._link_target = os.path.join(path, tarinfo.linkname)\\n try:\\n self._extract_member(tarinfo, os.path.join(path, tarinfo.name),\\n set_attrs=set_attrs,\\n numeric_owner=numeric_owner)\\n except oserror as e:\\n if self.errorlevel > 0:\\n raise\\n else:\\n if e.filename is none:\\n self._dbg(1, \"tarfile: %s\" % e.strerror)\\n else:\\n self._dbg(1, \"tarfile: %s %r\" % (e.strerror, e.filename))\\n except extracterror as e:\\n if self.errorlevel > 1:\\n raise\\n else:\\n self._dbg(1, \"tarfile: %s\" % e)\\n def extractfile(self, member):\\n \"\"\"extract a member from the archive as a file object. `member\\' may be\\n a filename or a tarinfo object. if `member\\' is a regular file or a\\n link, an io.bufferedreader object is returned. otherwise, none is\\n returned.\\n \"\"\"\\n self._check(\"r\")\\n if isinstance(member, str):\\n tarinfo = self.getmember(member)\\n else:\\n tarinfo = member\\n if tarinfo.isreg() or tarinfo.type not in supported_types:\\n # members with unknown types are treated as regular files.\\n return self.fileobject(self, tarinfo)\\n elif tarinfo.islnk() or tarinfo.issym():\\n if isinstance(self.fileobj, _stream):\\n # a small but ugly workaround for the case that someone tries\\n # to extract a (sym)link as a file-object from a non-seekable\\n # stream of tar blocks.\\n raise streamerror(\"cannot extract (sym)link as file object\")\\n else:\\n # a (sym)link\\'s file object is its target\\'s file object.\\n return self.extractfile(self._find_link_target(tarinfo))\\n else:\\n # if there\\'s no data associated with the member (directory, chrdev,\\n # blkdev, etc.), return none instead of a file object.\\n return none\\n def _extract_member(self, tarinfo, targetpath, set_attrs=true,\\n numeric_owner=false):\\n \"\"\"extract the tarinfo object tarinfo to a physical\\n file called targetpath.\\n \"\"\"\\n # fetch the tarinfo object for the given name\\n # and build the destination pathname, replacing\\n # forward slashes to platform specific separators.\\n targetpath = targetpath.rstrip(\"/\")\\n targetpath = targetpath.replace(\"/\", os.sep)\\n # create all upper directories.\\n upperdirs = os.path.dirname(targetpath)\\n if upperdirs and not os.path.exists(upperdirs):\\n # create directories that are not part of the archive with\\n # default permissions.\\n os.makedirs(upperdirs)\\n if tarinfo.islnk() or tarinfo.issym():\\n self._dbg(1, \"%s -> %s\" % (tarinfo.name, tarinfo.linkname))\\n else:\\n self._dbg(1, tarinfo.name)\\n if tarinfo.isreg():\\n self.makefile(tarinfo, targetpath)\\n elif tarinfo.isdir():\\n self.makedir(tarinfo, targetpath)\\n elif tarinfo.isfifo():\\n self.makefifo(tarinfo, targetpath)\\n elif tarinfo.ischr() or tarinfo.isblk():\\n self.makedev(tarinfo, targetpath)\\n elif tarinfo.islnk() or tarinfo.issym():\\n self.makelink(tarinfo, targetpath)\\n elif tarinfo.type not in supported_types:\\n self.makeunknown(tarinfo, targetpath)\\n else:\\n self.makefile(tarinfo, targetpath)\\n if set_attrs:\\n self.chown(tarinfo, targetpath, numeric_owner)\\n if not tarinfo.issym():\\n self.chmod(tarinfo, targetpath)\\n self.utime(tarinfo, targetpath)\\n #--------------------------------------------------------------------------\\n # below are the different file methods. they are called via\\n # _extract_member() when extract() is called. they can be replaced in a\\n # subclass to implement other functionality.\\n def makedir(self, tarinfo, targetpath):\\n \"\"\"make a directory called targetpath.\\n \"\"\"\\n try:\\n # use a safe mode for the directory, the real mode is set\\n # later in _extract_member().\\n os.mkdir(targetpath, 0o700)\\n except fileexistserror:\\n pass\\n def makefile(self, tarinfo, targetpath):\\n \"\"\"make a file called targetpath.\\n \"\"\"\\n source = self.fileobj\\n source.seek(tarinfo.offset_data)\\n with bltn_open(targetpath, \"wb\") as target:\\n if tarinfo.sparse is not none:\\n for offset, size in tarinfo.sparse:\\n target.seek(offset)\\n copyfileobj(source, target, size, readerror)\\n target.seek(tarinfo.size)\\n target.truncate()\\n else:\\n copyfileobj(source, target, tarinfo.size, readerror)\\n def makeunknown(self, tarinfo, targetpath):\\n \"\"\"make a file from a tarinfo object with an unknown type\\n at targetpath.\\n \"\"\"\\n self.makefile(tarinfo, targetpath)\\n self._dbg(1, \"tarfile: unknown file type %r, \" \\\\\\n \"extracted as regular file.\" % tarinfo.type)\\n def makefifo(self, tarinfo, targetpath):\\n \"\"\"make a fifo called targetpath.\\n \"\"\"\\n if hasattr(os, \"mkfifo\"):\\n os.mkfifo(targetpath)\\n else:\\n raise extracterror(\"fifo not supported by system\")\\n def makedev(self, tarinfo, targetpath):\\n \"\"\"make a character or block device called targetpath.\\n \"\"\"\\n if not hasattr(os, \"mknod\") or not hasattr(os, \"makedev\"):\\n raise extracterror(\"special devices not supported by system\")\\n mode = tarinfo.mode\\n if tarinfo.isblk():\\n mode |= stat.s_ifblk\\n else:\\n mode |= stat.s_ifchr\\n os.mknod(targetpath, mode,\\n os.makedev(tarinfo.devmajor, tarinfo.devminor))\\n def makelink(self, tarinfo, targetpath):\\n \"\"\"make a (symbolic) link called targetpath. if it cannot be created\\n (platform limitation), we try to make a copy of the referenced file\\n instead of a link.\\n \"\"\"\\n try:\\n # for systems that support symbolic and hard links.\\n if tarinfo.issym():\\n os.symlink(tarinfo.linkname, targetpath)\\n else:\\n # see extract().\\n if os.path.exists(tarinfo._link_target):\\n os.link(tarinfo._link_target, targetpath)\\n else:\\n self._extract_member(self._find_link_target(tarinfo),\\n targetpath)\\n except symlink_exception:\\n try:\\n self._extract_member(self._find_link_target(tarinfo),\\n targetpath)\\n except keyerror:\\n raise extracterror(\"unable to resolve link inside archive\")\\n def chown(self, tarinfo, targetpath, numeric_owner):\\n \"\"\"set owner of targetpath according to tarinfo. if numeric_owner\\n is true, use .gid/.uid instead of .gname/.uname.\\n \"\"\"\\n if pwd and hasattr(os, \"geteuid\") and os.geteuid() == 0:\\n # we have to be root to do so.\\n if numeric_owner:\\n g = tarinfo.gid\\n u = tarinfo.uid\\n else:\\n try:\\n g = grp.getgrnam(tarinfo.gname)[2]\\n except keyerror:\\n g = tarinfo.gid\\n try:\\n u = pwd.getpwnam(tarinfo.uname)[2]\\n except keyerror:\\n u = tarinfo.uid\\n try:\\n if tarinfo.issym() and hasattr(os, \"lchown\"):\\n os.lchown(targetpath, u, g)\\n else:\\n os.chown(targetpath, u, g)\\n except oserror as e:\\n raise extracterror(\"could not change owner\")\\n def chmod(self, tarinfo, targetpath):\\n \"\"\"set file permissions of targetpath according to tarinfo.\\n \"\"\"\\n if hasattr(os, \\'chmod\\'):\\n try:\\n os.chmod(targetpath, tarinfo.mode)\\n except oserror as e:\\n raise extracterror(\"could not change mode\")\\n def utime(self, tarinfo, targetpath):\\n \"\"\"set modification time of targetpath according to tarinfo.\\n \"\"\"\\n if not hasattr(os, \\'utime\\'):\\n return\\n try:\\n os.utime(targetpath, (tarinfo.mtime, tarinfo.mtime))\\n except oserror as e:\\n raise extracterror(\"could not change modification time\")\\n #--------------------------------------------------------------------------\\n def next(self):\\n \"\"\"return the next member of the archive as a tarinfo object, when\\n tarfile is opened for reading. return none if there is no more\\n available.\\n \"\"\"\\n self._check(\"ra\")\\n if self.firstmember is not none:\\n m = self.firstmember\\n self.firstmember = none\\n return m\\n # advance the file pointer.\\n if self.offset != self.fileobj.tell():\\n self.fileobj.seek(self.offset - 1)\\n if not self.fileobj.read(1):\\n raise readerror(\"unexpected end of data\")\\n # read the next block.\\n tarinfo = none\\n while true:\\n try:\\n tarinfo = self.tarinfo.fromtarfile(self)\\n except eofheadererror as e:\\n if self.ignore_zeros:\\n self._dbg(2, \"0x%x: %s\" % (self.offset, e))\\n self.offset += blocksize\\n continue\\n except invalidheadererror as e:\\n if self.ignore_zeros:\\n self._dbg(2, \"0x%x: %s\" % (self.offset, e))\\n self.offset += blocksize\\n continue\\n elif self.offset == 0:\\n raise readerror(str(e))\\n except emptyheadererror:\\n if self.offset == 0:\\n raise readerror(\"empty file\")\\n except truncatedheadererror as e:\\n if self.offset == 0:\\n raise readerror(str(e))\\n except subsequentheadererror as e:\\n raise readerror(str(e))\\n break\\n if tarinfo is not none:\\n self.members.append(tarinfo)\\n else:\\n self._loaded = true\\n return tarinfo\\n #--------------------------------------------------------------------------\\n # little helper methods:\\n def _getmember(self, name, tarinfo=none, normalize=false):\\n \"\"\"find an archive member by name from bottom to top.\\n if tarinfo is given, it is used as the starting point.\\n \"\"\"\\n # ensure that all members have been loaded.\\n members = self.getmembers()\\n # limit the member search list up to tarinfo.\\n if tarinfo is not none:\\n members = members[:members.index(tarinfo)]\\n if normalize:\\n name = os.path.normpath(name)\\n for member in reversed(members):\\n if normalize:\\n member_name = os.path.normpath(member.name)\\n else:\\n member_name = member.name\\n if name == member_name:\\n return member\\n def _load(self):\\n \"\"\"read through the entire archive file and look for readable\\n members.\\n \"\"\"\\n while true:\\n tarinfo = self.next()\\n if tarinfo is none:\\n break\\n self._loaded = true\\n def _check(self, mode=none):\\n \"\"\"check if tarfile is still open, and if the operation\\'s mode\\n corresponds to tarfile\\'s mode.\\n \"\"\"\\n if self.closed:\\n raise oserror(\"%s is closed\" % self.__class__.__name__)\\n if mode is not none and self.mode not in mode:\\n raise oserror(\"bad operation for mode %r\" % self.mode)\\n def _find_link_target(self, tarinfo):\\n \"\"\"find the target member of a symlink or hardlink member in the\\n archive.\\n \"\"\"\\n if tarinfo.issym():\\n # always search the entire archive.\\n linkname = \"/\".join(filter(none, (os.path.dirname(tarinfo.name), tarinfo.linkname)))\\n limit = none\\n else:\\n # search the archive before the link, because a hard link is\\n # just a reference to an already archived file.\\n linkname = tarinfo.linkname\\n limit = tarinfo\\n member = self._getmember(linkname, tarinfo=limit, normalize=true)\\n if member is none:\\n raise keyerror(\"linkname %r not found\" % linkname)\\n return member\\n def __iter__(self):\\n \"\"\"provide an iterator object.\\n \"\"\"\\n if self._loaded:\\n return iter(self.members)\\n else:\\n return tariter(self)\\n def _dbg(self, level, msg):\\n \"\"\"write debugging output to sys.stderr.\\n \"\"\"\\n if level <= self.debug:\\n print(msg, file=sys.stderr)\\n def __enter__(self):\\n self._check()\\n return self\\n def __exit__(self, type, value, traceback):\\n if type is none:\\n self.close()\\n else:\\n # an exception occurred. we must not call close() because\\n # it would try to write end-of-archive blocks and padding.\\n if not self._extfileobj:\\n self.fileobj.close()\\n self.closed = true\\n # class tarfile\\n class tariter:\\n \"\"\"iterator class.\\n for tarinfo in tarfile(...):\\n suite...\\n \"\"\"\\n def __init__(self, tarfile):\\n \"\"\"construct a tariter object.\\n \"\"\"\\n self.tarfile = tarfile\\n self.index = 0\\n def __iter__(self):\\n \"\"\"return iterator object.\\n \"\"\"\\n return self\\n def __next__(self):\\n \"\"\"return the next item using tarfile\\'s next() method.\\n when all members have been read, set tarfile as _loaded.\\n \"\"\"\\n # fix for sf #1100429: under rare circumstances it can\\n # happen that getmembers() is called during iteration,\\n # which will cause tariter to stop prematurely.\\n if self.index == 0 and self.tarfile.firstmember is not none:\\n tarinfo = self.tarfile.next()\\n elif self.index < len(self.tarfile.members):\\n tarinfo = self.tarfile.members[self.index]\\n elif not self.tarfile._loaded:\\n tarinfo = self.tarfile.next()\\n if not tarinfo:\\n self.tarfile._loaded = true\\n raise stopiteration\\n else:\\n raise stopiteration\\n self.index += 1\\n return tarinfo\\n #--------------------\\n # exported functions\\n #--------------------\\n def is_tarfile(name):\\n \"\"\"return true if name points to a tar archive that we\\n are able to handle, else return false.\\n \"\"\"\\n try:\\n t = open(name)\\n t.close()\\n return true\\n except tarerror:\\n return false\\n open = tarfile.open\\n def main():\\n import argparse\\n description = \\'a simple command line interface for tarfile module.\\'\\n parser = argparse.argumentparser(description=description)\\n parser.add_argument(\\'-v\\', \\'--verbose\\', action=\\'store_true\\', default=false,\\n help=\\'verbose output\\')\\n group = parser.add_mutually_exclusive_group()\\n group.add_argument(\\'-l\\', \\'--list\\', metavar=\\'<tarfile>\\',\\n help=\\'show listing of a tarfile\\')\\n group.add_argument(\\'-e\\', \\'--extract\\', nargs=\\'+\\',\\n metavar=(\\'<tarfile>\\', \\'<output_dir>\\'),\\n help=\\'extract tarfile into target dir\\')\\n group.add_argument(\\'-c\\', \\'--create\\', nargs=\\'+\\',\\n metavar=(\\'<name>\\', \\'<file>\\'),\\n help=\\'create tarfile from sources\\')\\n group.add_argument(\\'-t\\', \\'--test\\', metavar=\\'<tarfile>\\',\\n help=\\'test if a tarfile is valid\\')\\n args = parser.parse_args()\\n if args.test:\\n src = args.test\\n if is_tarfile(src):\\n with open(src, \\'r\\') as tar:\\n tar.getmembers()\\n print(tar.getmembers(), file=sys.stderr)\\n if args.verbose:\\n print(\\'{!r} is a tar archive.\\'.format(src))\\n else:\\n parser.exit(1, \\'{!r} is not a tar archive.\\\\n\\'.format(src))\\n elif args.list:\\n src = args.list\\n if is_tarfile(src):\\n with tarfile.open(src, \\'r:*\\') as tf:\\n tf.list(verbose=args.verbose)\\n else:\\n parser.exit(1, \\'{!r} is not a tar archive.\\\\n\\'.format(src))\\n elif args.extract:\\n if len(args.extract) == 1:\\n src = args.extract[0]\\n curdir = os.curdir\\n elif len(args.extract) == 2:\\n src, curdir = args.extract\\n else:\\n parser.exit(1, parser.format_help())\\n if is_tarfile(src):\\n with tarfile.open(src, \\'r:*\\') as tf:\\n tf.extractall(path=curdir)\\n if args.verbose:\\n if curdir == \\'.\\':\\n msg = \\'{!r} file is extracted.\\'.format(src)\\n else:\\n msg = (\\'{!r} file is extracted \\'\\n \\'into {!r} directory.\\').format(src, curdir)\\n print(msg)\\n else:\\n parser.exit(1, \\'{!r} is not a tar archive.\\\\n\\'.format(src))\\n elif args.create:\\n tar_name = args.create.pop(0)\\n _, ext = os.path.splitext(tar_name)\\n compressions = {\\n # gz\\n \\'.gz\\': \\'gz\\',\\n \\'.tgz\\': \\'gz\\',\\n # xz\\n \\'.xz\\': \\'xz\\',\\n \\'.txz\\': \\'xz\\',\\n # bz2\\n \\'.bz2\\': \\'bz2\\',\\n \\'.tbz\\': \\'bz2\\',\\n \\'.tbz2\\': \\'bz2\\',\\n \\'.tb2\\': \\'bz2\\',\\n }\\n tar_mode = \\'w:\\' + compressions[ext] if ext in compressions else \\'w\\'\\n tar_files = args.create\\n with tarfile.open(tar_name, tar_mode) as tf:\\n for file_name in tar_files:\\n tf.add(file_name)\\n if args.verbose:\\n print(\\'{!r} file created.\\'.format(tar_name))\\n else:\\n parser.exit(1, parser.format_help())\\n if __name__ == \\'__main__\\':\\n main()\\n \"\"\"temporary files.\\n this module provides generic, low- and high-level interfaces for\\n creating temporary files and directories.  all of the interfaces\\n provided by this module can be used without fear of race conditions\\n except for \\'mktemp\\'.  \\'mktemp\\' is subject to race conditions and\\n should not be used; it is provided for backward compatibility only.\\n the default path names are returned as str.  if you supply bytes as\\n input, all return values will be in bytes.  ex:\\n >>> tempfile.mkstemp()\\n (4, \\'/tmp/tmptpu9nin8\\')\\n >>> tempfile.mkdtemp(suffix=b\\'\\')\\n b\\'/tmp/tmppbi8f0hy\\'\\n this module also provides some data items to the user:\\n tmp_max  - maximum number of names that will be tried before\\n giving up.\\n tempdir  - if this is set to a string before the first use of\\n any routine from this module, it will be considered as\\n another candidate location to store temporary files.\\n \"\"\"\\n __all__ = [\\n \"namedtemporaryfile\", \"temporaryfile\", # high level safe interfaces\\n \"spooledtemporaryfile\", \"temporarydirectory\",\\n \"mkstemp\", \"mkdtemp\",                  # low level safe interfaces\\n \"mktemp\",                              # deprecated unsafe interface\\n \"tmp_max\", \"gettempprefix\",            # constants\\n \"tempdir\", \"gettempdir\",\\n \"gettempprefixb\", \"gettempdirb\",\\n ]\\n # imports.\\n import functools as _functools\\n import warnings as _warnings\\n import io as _io\\n import os as _os\\n try:\\n import shutil as _shutil\\n _rmtree = _shutil.rmtree\\n except importerror:\\n import sys as _sys\\n import stat as _stat\\n # version vulnerable to race conditions\\n def _rmtree_unsafe(path, onerror):\\n try:\\n if _os.path.islink(path):\\n # symlinks to directories are forbidden, see bug #1669\\n raise oserror(\"cannot call rmtree on a symbolic link\")\\n except oserror:\\n onerror(_os.path.islink, path, _sys.exc_info())\\n # can\\'t continue even if onerror hook returns\\n return\\n names = []\\n try:\\n names = _os.listdir(path)\\n except oserror:\\n onerror(_os.listdir, path, _sys.exc_info())\\n for name in names:\\n fullname = _os.path.join(path, name)\\n try:\\n mode = _os.lstat(fullname).st_mode\\n except oserror:\\n mode = 0\\n if _stat.s_isdir(mode):\\n _rmtree_unsafe(fullname, onerror)\\n else:\\n try:\\n _os.unlink(fullname)\\n except oserror:\\n onerror(_os.unlink, fullname, _sys.exc_info())\\n try:\\n _os.rmdir(path)\\n except oserror:\\n onerror(_os.rmdir, path, _sys.exc_info())\\n # version using fd-based apis to protect against races\\n def _rmtree_safe_fd(topfd, path, onerror):\\n names = []\\n try:\\n names = _os.listdir(topfd)\\n except oserror as err:\\n err.filename = path\\n onerror(_os.listdir, path, _sys.exc_info())\\n for name in names:\\n fullname = _os.path.join(path, name)\\n try:\\n orig_st = _os.stat(name, dir_fd=topfd, follow_symlinks=false)\\n mode = orig_st.st_mode\\n except oserror:\\n mode = 0\\n if _stat.s_isdir(mode):\\n try:\\n dirfd = _os.open(name, _os.o_rdonly, dir_fd=topfd)\\n except oserror:\\n onerror(_os.open, fullname, _sys.exc_info())\\n else:\\n try:\\n if _os.path.samestat(orig_st, _os.fstat(dirfd)):\\n _rmtree_safe_fd(dirfd, fullname, onerror)\\n try:\\n _os.rmdir(name, dir_fd=topfd)\\n except oserror:\\n onerror(_os.rmdir, fullname, _sys.exc_info())\\n else:\\n try:\\n # this can only happen if someone replaces\\n # a directory with a symlink after the call to\\n # stat.s_isdir above.\\n raise oserror(\"cannot call rmtree on a symbolic \"\\n \"link\")\\n except oserror:\\n onerror(_os.path.islink, fullname, _sys.exc_info())\\n finally:\\n _os.close(dirfd)\\n else:\\n try:\\n _os.unlink(name, dir_fd=topfd)\\n except oserror:\\n onerror(_os.unlink, fullname, _sys.exc_info())\\n _use_fd_functions = ({_os.open, _os.stat, _os.unlink, _os.rmdir} <=\\n _os.supports_dir_fd and\\n _os.listdir in _os.supports_fd and\\n _os.stat in _os.supports_follow_symlinks)\\n def _rmtree(path, ignore_errors=false, onerror=none):\\n \"\"\"recursively delete a directory tree.\\n if ignore_errors is set, errors are ignored; otherwise, if onerror\\n is set, it is called to handle the error with arguments (func,\\n path, exc_info) where func is platform and implementation dependent;\\n path is the argument to that function that caused it to fail; and\\n exc_info is a tuple returned by sys.exc_info().  if ignore_errors\\n is false and onerror is none, an exception is raised.\\n \"\"\"\\n if ignore_errors:\\n def onerror(*args):\\n pass\\n elif onerror is none:\\n def onerror(*args):\\n raise\\n if _use_fd_functions:\\n # while the unsafe rmtree works fine on bytes, the fd based does not.\\n if isinstance(path, bytes):\\n path = _os.fsdecode(path)\\n # note: to guard against symlink races, we use the standard\\n # lstat()/open()/fstat() trick.\\n try:\\n orig_st = _os.lstat(path)\\n except exception:\\n onerror(_os.lstat, path, _sys.exc_info())\\n return\\n try:\\n fd = _os.open(path, _os.o_rdonly)\\n except exception:\\n onerror(_os.lstat, path, _sys.exc_info())\\n return\\n try:\\n if _os.path.samestat(orig_st, _os.fstat(fd)):\\n _rmtree_safe_fd(fd, path, onerror)\\n try:\\n _os.rmdir(path)\\n except oserror:\\n onerror(_os.rmdir, path, _sys.exc_info())\\n else:\\n try:\\n # symlinks to directories are forbidden, see bug #1669\\n raise oserror(\"cannot call rmtree on a symbolic link\")\\n except oserror:\\n onerror(_os.path.islink, path, _sys.exc_info())\\n finally:\\n _os.close(fd)\\n else:\\n return _rmtree_unsafe(path, onerror)\\n import errno as _errno\\n from random import random as _random\\n import weakref as _weakref\\n try:\\n import _thread\\n except importerror:\\n import _dummy_thread as _thread\\n _allocate_lock = _thread.allocate_lock\\n _text_openflags = _os.o_rdwr | _os.o_creat | _os.o_excl\\n if hasattr(_os, \\'o_nofollow\\'):\\n _text_openflags |= _os.o_nofollow\\n _bin_openflags = _text_openflags\\n if hasattr(_os, \\'o_binary\\'):\\n _bin_openflags |= _os.o_binary\\n if hasattr(_os, \\'tmp_max\\'):\\n tmp_max = _os.tmp_max\\n else:\\n tmp_max = 10000\\n # this variable _was_ unused for legacy reasons, see issue 10354.\\n # but as of 3.5 we actually use it at runtime so changing it would\\n # have a possibly desirable side effect...  but we do not want to support\\n # that as an api.  it is undocumented on purpose.  do not depend on this.\\n template = \"tmp\"\\n # internal routines.\\n _once_lock = _allocate_lock()\\n if hasattr(_os, \"lstat\"):\\n _stat = _os.lstat\\n elif hasattr(_os, \"stat\"):\\n _stat = _os.stat\\n else:\\n # fallback.  all we need is something that raises oserror if the\\n # file doesn\\'t exist.\\n def _stat(fn):\\n fd = _os.open(fn, _os.o_rdonly)\\n _os.close(fd)\\n def _exists(fn):\\n try:\\n _stat(fn)\\n except oserror:\\n return false\\n else:\\n return true\\n def _infer_return_type(*args):\\n \"\"\"look at the type of all args and divine their implied return type.\"\"\"\\n return_type = none\\n for arg in args:\\n if arg is none:\\n continue\\n if isinstance(arg, bytes):\\n if return_type is str:\\n raise typeerror(\"can\\'t mix bytes and non-bytes in \"\\n \"path components.\")\\n return_type = bytes\\n else:\\n if return_type is bytes:\\n raise typeerror(\"can\\'t mix bytes and non-bytes in \"\\n \"path components.\")\\n return_type = str\\n if return_type is none:\\n return str  # tempfile apis return a str by default.\\n return return_type\\n def _sanitize_params(prefix, suffix, dir):\\n \"\"\"common parameter processing for most apis in this module.\"\"\"\\n output_type = _infer_return_type(prefix, suffix, dir)\\n if suffix is none:\\n suffix = output_type()\\n if prefix is none:\\n if output_type is str:\\n prefix = template\\n else:\\n prefix = _os.fsencode(template)\\n if dir is none:\\n if output_type is str:\\n dir = gettempdir()\\n else:\\n dir = gettempdirb()\\n return prefix, suffix, dir, output_type\\n class _randomnamesequence:\\n \"\"\"an instance of _randomnamesequence generates an endless\\n sequence of unpredictable strings which can safely be incorporated\\n into file names.  each string is six characters long.  multiple\\n threads can safely use the same instance at the same time.\\n _randomnamesequence is an iterator.\"\"\"\\n characters = \"abcdefghijklmnopqrstuvwxyz0123456789_\"\\n @property\\n def rng(self):\\n cur_pid = _os.getpid()\\n if cur_pid != getattr(self, \\'_rng_pid\\', none):\\n self._rng = _random()\\n self._rng_pid = cur_pid\\n return self._rng\\n def __iter__(self):\\n return self\\n def __next__(self):\\n c = self.characters\\n choose = self.rng.choice\\n letters = [choose(c) for dummy in range(8)]\\n return \\'\\'.join(letters)\\n def _candidate_tempdir_list():\\n \"\"\"generate a list of candidate temporary directories which\\n _get_default_tempdir will try.\"\"\"\\n dirlist = []\\n # first, try the environment.\\n for envname in \\'tmpdir\\', \\'temp\\', \\'tmp\\':\\n dirname = _os.getenv(envname)\\n if dirname: dirlist.append(dirname)\\n # failing that, try os-specific locations.\\n if _os.name == \\'nt\\':\\n dirlist.extend([ r\\'c:\\\\temp\\', r\\'c:\\\\tmp\\', r\\'\\\\temp\\', r\\'\\\\tmp\\' ])\\n else:\\n dirlist.extend([ \\'/tmp\\', \\'/var/tmp\\', \\'/usr/tmp\\' ])\\n # as a last resort, the current directory.\\n try:\\n dirlist.append(_os.getcwd())\\n except (attributeerror, oserror):\\n dirlist.append(_os.curdir)\\n return dirlist\\n def _get_default_tempdir():\\n \"\"\"calculate the default directory to use for temporary files.\\n this routine should be called exactly once.\\n we determine whether or not a candidate temp dir is usable by\\n trying to create and write to a file in that directory.  if this\\n is successful, the test file is deleted.  to prevent denial of\\n service, the name of the test file must be randomized.\"\"\"\\n namer = _randomnamesequence()\\n dirlist = _candidate_tempdir_list()\\n for dir in dirlist:\\n if dir != _os.curdir:\\n dir = _os.path.abspath(dir)\\n # try only a few names per directory.\\n for seq in range(100):\\n name = next(namer)\\n filename = _os.path.join(dir, name)\\n try:\\n fd = _os.open(filename, _bin_openflags, 0o600)\\n try:\\n try:\\n with _io.open(fd, \\'wb\\', closefd=false) as fp:\\n fp.write(b\\'blat\\')\\n finally:\\n _os.close(fd)\\n finally:\\n _os.unlink(filename)\\n return dir\\n except fileexistserror:\\n pass\\n except permissionerror:\\n # this exception is thrown when a directory with the chosen name\\n # already exists on windows.\\n if (_os.name == \\'nt\\' and _os.path.isdir(dir) and\\n _os.access(dir, _os.w_ok)):\\n continue\\n break   # no point trying more names in this directory\\n except oserror:\\n break   # no point trying more names in this directory\\n raise filenotfounderror(_errno.enoent,\\n \"no usable temporary directory found in %s\" %\\n dirlist)\\n _name_sequence = none\\n def _get_candidate_names():\\n \"\"\"common setup sequence for all user-callable interfaces.\"\"\"\\n global _name_sequence\\n if _name_sequence is none:\\n _once_lock.acquire()\\n try:\\n if _name_sequence is none:\\n _name_sequence = _randomnamesequence()\\n finally:\\n _once_lock.release()\\n return _name_sequence\\n def _mkstemp_inner(dir, pre, suf, flags, output_type):\\n \"\"\"code common to mkstemp, temporaryfile, and namedtemporaryfile.\"\"\"\\n names = _get_candidate_names()\\n if output_type is bytes:\\n names = map(_os.fsencode, names)\\n for seq in range(tmp_max):\\n name = next(names)\\n file = _os.path.join(dir, pre + name + suf)\\n try:\\n fd = _os.open(file, flags, 0o600)\\n except fileexistserror:\\n continue    # try again\\n except permissionerror:\\n # this exception is thrown when a directory with the chosen name\\n # already exists on windows.\\n if (_os.name == \\'nt\\' and _os.path.isdir(dir) and\\n _os.access(dir, _os.w_ok)):\\n continue\\n else:\\n raise\\n return (fd, _os.path.abspath(file))\\n raise fileexistserror(_errno.eexist,\\n \"no usable temporary file name found\")\\n # user visible interfaces.\\n def gettempprefix():\\n \"\"\"the default prefix for temporary directories.\"\"\"\\n return template\\n def gettempprefixb():\\n \"\"\"the default prefix for temporary directories as bytes.\"\"\"\\n return _os.fsencode(gettempprefix())\\n tempdir = none\\n def gettempdir():\\n \"\"\"accessor for tempfile.tempdir.\"\"\"\\n global tempdir\\n if tempdir is none:\\n _once_lock.acquire()\\n try:\\n if tempdir is none:\\n tempdir = _get_default_tempdir()\\n finally:\\n _once_lock.release()\\n return tempdir\\n def gettempdirb():\\n \"\"\"a bytes version of tempfile.gettempdir().\"\"\"\\n return _os.fsencode(gettempdir())\\n def mkstemp(suffix=none, prefix=none, dir=none, text=false):\\n \"\"\"user-callable function to create and return a unique temporary\\n file.  the return value is a pair (fd, name) where fd is the\\n file descriptor returned by os.open, and name is the filename.\\n if \\'suffix\\' is not none, the file name will end with that suffix,\\n otherwise there will be no suffix.\\n if \\'prefix\\' is not none, the file name will begin with that prefix,\\n otherwise a default prefix is used.\\n if \\'dir\\' is not none, the file will be created in that directory,\\n otherwise a default directory is used.\\n if \\'text\\' is specified and true, the file is opened in text\\n mode.  else (the default) the file is opened in binary mode.  on\\n some operating systems, this makes no difference.\\n if any of \\'suffix\\', \\'prefix\\' and \\'dir\\' are not none, they must be the\\n same type.  if they are bytes, the returned name will be bytes; str\\n otherwise.\\n the file is readable and writable only by the creating user id.\\n if the operating system uses permission bits to indicate whether a\\n file is executable, the file is executable by no one. the file\\n descriptor is not inherited by children of this process.\\n caller is responsible for deleting the file when done with it.\\n \"\"\"\\n prefix, suffix, dir, output_type = _sanitize_params(prefix, suffix, dir)\\n if text:\\n flags = _text_openflags\\n else:\\n flags = _bin_openflags\\n return _mkstemp_inner(dir, prefix, suffix, flags, output_type)\\n def mkdtemp(suffix=none, prefix=none, dir=none):\\n \"\"\"user-callable function to create and return a unique temporary\\n directory.  the return value is the pathname of the directory.\\n arguments are as for mkstemp, except that the \\'text\\' argument is\\n not accepted.\\n the directory is readable, writable, and searchable only by the\\n creating user.\\n caller is responsible for deleting the directory when done with it.\\n \"\"\"\\n prefix, suffix, dir, output_type = _sanitize_params(prefix, suffix, dir)\\n names = _get_candidate_names()\\n if output_type is bytes:\\n names = map(_os.fsencode, names)\\n for seq in range(tmp_max):\\n name = next(names)\\n file = _os.path.join(dir, prefix + name + suffix)\\n try:\\n _os.mkdir(file, 0o700)\\n except fileexistserror:\\n continue    # try again\\n except permissionerror:\\n # this exception is thrown when a directory with the chosen name\\n # already exists on windows.\\n if (_os.name == \\'nt\\' and _os.path.isdir(dir) and\\n _os.access(dir, _os.w_ok)):\\n continue\\n else:\\n raise\\n return file\\n raise fileexistserror(_errno.eexist,\\n \"no usable temporary directory name found\")\\n def mktemp(suffix=\"\", prefix=template, dir=none):\\n \"\"\"user-callable function to return a unique temporary file name.  the\\n file is not created.\\n arguments are similar to mkstemp, except that the \\'text\\' argument is\\n not accepted, and suffix=none, prefix=none and bytes file names are not\\n supported.\\n this function is unsafe and should not be used.  the file name may\\n refer to a file that did not exist at some point, but by the time\\n you get around to creating it, someone else may have beaten you to\\n the punch.\\n \"\"\"\\n ##    from warnings import warn as _warn\\n ##    _warn(\"mktemp is a potential security risk to your program\",\\n ##          runtimewarning, stacklevel=2)\\n if dir is none:\\n dir = gettempdir()\\n names = _get_candidate_names()\\n for seq in range(tmp_max):\\n name = next(names)\\n file = _os.path.join(dir, prefix + name + suffix)\\n if not _exists(file):\\n return file\\n raise fileexistserror(_errno.eexist,\\n \"no usable temporary filename found\")\\n class _temporaryfilecloser:\\n \"\"\"a separate object allowing proper closing of a temporary file\\'s\\n underlying file object, without adding a __del__ method to the\\n temporary file.\"\"\"\\n file = none  # set here since __del__ checks it\\n close_called = false\\n def __init__(self, file, name, delete=true):\\n self.file = file\\n self.name = name\\n self.delete = delete\\n # nt provides delete-on-close as a primitive, so we don\\'t need\\n # the wrapper to do anything special.  we still use it so that\\n # file.name is useful (i.e. not \"(fdopen)\") with namedtemporaryfile.\\n if _os.name != \\'nt\\':\\n # cache the unlinker so we don\\'t get spurious errors at\\n # shutdown when the module-level \"os\" is none\\'d out.  note\\n # that this must be referenced as self.unlink, because the\\n # name temporaryfilewrapper may also get none\\'d out before\\n # __del__ is called.\\n def close(self, unlink=_os.unlink):\\n if not self.close_called and self.file is not none:\\n self.close_called = true\\n try:\\n self.file.close()\\n finally:\\n if self.delete:\\n unlink(self.name)\\n # need to ensure the file is deleted on __del__\\n def __del__(self):\\n self.close()\\n else:\\n def close(self):\\n if not self.close_called:\\n self.close_called = true\\n self.file.close()\\n class _temporaryfilewrapper:\\n \"\"\"temporary file wrapper\\n this class provides a wrapper around files opened for\\n temporary use.  in particular, it seeks to automatically\\n remove the file when it is no longer needed.\\n \"\"\"\\n def __init__(self, file, name, delete=true):\\n self.file = file\\n self.name = name\\n self.delete = delete\\n self._closer = _temporaryfilecloser(file, name, delete)\\n def __getattr__(self, name):\\n # attribute lookups are delegated to the underlying file\\n # and cached for non-numeric results\\n # (i.e. methods are cached, closed and friends are not)\\n file = self.__dict__[\\'file\\']\\n a = getattr(file, name)\\n if hasattr(a, \\'__call__\\'):\\n func = a\\n @_functools.wraps(func)\\n def func_wrapper(*args, **kwargs):\\n return func(*args, **kwargs)\\n # avoid closing the file as long as the wrapper is alive,\\n # see issue #18879.\\n func_wrapper._closer = self._closer\\n a = func_wrapper\\n if not isinstance(a, int):\\n setattr(self, name, a)\\n return a\\n # the underlying __enter__ method returns the wrong object\\n # (self.file) so override it to return the wrapper\\n def __enter__(self):\\n self.file.__enter__()\\n return self\\n # need to trap __exit__ as well to ensure the file gets\\n # deleted when used in a with statement\\n def __exit__(self, exc, value, tb):\\n result = self.file.__exit__(exc, value, tb)\\n self.close()\\n return result\\n def close(self):\\n \"\"\"\\n close the temporary file, possibly deleting it.\\n \"\"\"\\n self._closer.close()\\n # iter() doesn\\'t use __getattr__ to find the __iter__ method\\n def __iter__(self):\\n # don\\'t return iter(self.file), but yield from it to avoid closing\\n # file as long as it\\'s being used as iterator (see issue #23700).  we\\n # can\\'t use \\'yield from\\' here because iter(file) returns the file\\n # object itself, which has a close method, and thus the file would get\\n # closed when the generator is finalized, due to pep380 semantics.\\n for line in self.file:\\n yield line\\n def namedtemporaryfile(mode=\\'w+b\\', buffering=-1, encoding=none,\\n newline=none, suffix=none, prefix=none,\\n dir=none, delete=true):\\n \"\"\"create and return a temporary file.\\n arguments:\\n \\'prefix\\', \\'suffix\\', \\'dir\\' -- as for mkstemp.\\n \\'mode\\' -- the mode argument to io.open (default \"w+b\").\\n \\'buffering\\' -- the buffer size argument to io.open (default -1).\\n \\'encoding\\' -- the encoding argument to io.open (default none)\\n \\'newline\\' -- the newline argument to io.open (default none)\\n \\'delete\\' -- whether the file is deleted on close (default true).\\n the file is created as mkstemp() would do it.\\n returns an object with a file-like interface; the name of the file\\n is accessible as its \\'name\\' attribute.  the file will be automatically\\n deleted when it is closed unless the \\'delete\\' argument is set to false.\\n \"\"\"\\n prefix, suffix, dir, output_type = _sanitize_params(prefix, suffix, dir)\\n flags = _bin_openflags\\n # setting o_temporary in the flags causes the os to delete\\n # the file when it is closed.  this is only supported by windows.\\n if _os.name == \\'nt\\' and delete:\\n flags |= _os.o_temporary\\n (fd, name) = _mkstemp_inner(dir, prefix, suffix, flags, output_type)\\n try:\\n file = _io.open(fd, mode, buffering=buffering,\\n newline=newline, encoding=encoding)\\n return _temporaryfilewrapper(file, name, delete)\\n except baseexception:\\n _os.unlink(name)\\n _os.close(fd)\\n raise\\n if _os.name != \\'posix\\' or _os.sys.platform == \\'cygwin\\':\\n # on non-posix and cygwin systems, assume that we cannot unlink a file\\n # while it is open.\\n temporaryfile = namedtemporaryfile\\n else:\\n # is the o_tmpfile flag available and does it work?\\n # the flag is set to false if os.open(dir, os.o_tmpfile) raises an\\n # isadirectoryerror exception\\n _o_tmpfile_works = hasattr(_os, \\'o_tmpfile\\')\\n def temporaryfile(mode=\\'w+b\\', buffering=-1, encoding=none,\\n newline=none, suffix=none, prefix=none,\\n dir=none):\\n \"\"\"create and return a temporary file.\\n arguments:\\n \\'prefix\\', \\'suffix\\', \\'dir\\' -- as for mkstemp.\\n \\'mode\\' -- the mode argument to io.open (default \"w+b\").\\n \\'buffering\\' -- the buffer size argument to io.open (default -1).\\n \\'encoding\\' -- the encoding argument to io.open (default none)\\n \\'newline\\' -- the newline argument to io.open (default none)\\n the file is created as mkstemp() would do it.\\n returns an object with a file-like interface.  the file has no\\n name, and will cease to exist when it is closed.\\n \"\"\"\\n global _o_tmpfile_works\\n prefix, suffix, dir, output_type = _sanitize_params(prefix, suffix, dir)\\n flags = _bin_openflags\\n if _o_tmpfile_works:\\n try:\\n flags2 = (flags | _os.o_tmpfile) & ~_os.o_creat\\n fd = _os.open(dir, flags2, 0o600)\\n except isadirectoryerror:\\n # linux kernel older than 3.11 ignores the o_tmpfile flag:\\n # o_tmpfile is read as o_directory. trying to open a directory\\n # with o_rdwr|o_directory fails with isadirectoryerror, a\\n # directory cannot be open to write. set flag to false to not\\n # try again.\\n _o_tmpfile_works = false\\n except oserror:\\n # the filesystem of the directory does not support o_tmpfile.\\n # for example, oserror(95, \\'operation not supported\\').\\n #\\n # on linux kernel older than 3.11, trying to open a regular\\n # file (or a symbolic link to a regular file) with o_tmpfile\\n # fails with notadirectoryerror, because o_tmpfile is read as\\n # o_directory.\\n pass\\n else:\\n try:\\n return _io.open(fd, mode, buffering=buffering,\\n newline=newline, encoding=encoding)\\n except:\\n _os.close(fd)\\n raise\\n # fallback to _mkstemp_inner().\\n (fd, name) = _mkstemp_inner(dir, prefix, suffix, flags, output_type)\\n try:\\n _os.unlink(name)\\n return _io.open(fd, mode, buffering=buffering,\\n newline=newline, encoding=encoding)\\n except:\\n _os.close(fd)\\n raise\\n class spooledtemporaryfile:\\n \"\"\"temporary file wrapper, specialized to switch from bytesio\\n or stringio to a real file when it exceeds a certain size or\\n when a fileno is needed.\\n \"\"\"\\n _rolled = false\\n def __init__(self, max_size=0, mode=\\'w+b\\', buffering=-1,\\n encoding=none, newline=none,\\n suffix=none, prefix=none, dir=none):\\n if \\'b\\' in mode:\\n self._file = _io.bytesio()\\n else:\\n # setting newline=\"\\\\n\" avoids newline translation;\\n # this is important because otherwise on windows we\\'d\\n # get double newline translation upon rollover().\\n self._file = _io.stringio(newline=\"\\\\n\")\\n self._max_size = max_size\\n self._rolled = false\\n self._temporaryfileargs = {\\'mode\\': mode, \\'buffering\\': buffering,\\n \\'suffix\\': suffix, \\'prefix\\': prefix,\\n \\'encoding\\': encoding, \\'newline\\': newline,\\n \\'dir\\': dir}\\n def _check(self, file):\\n if self._rolled: return\\n max_size = self._max_size\\n if max_size and file.tell() > max_size:\\n self.rollover()\\n def rollover(self):\\n if self._rolled: return\\n file = self._file\\n newfile = self._file = temporaryfile(**self._temporaryfileargs)\\n del self._temporaryfileargs\\n newfile.write(file.getvalue())\\n newfile.seek(file.tell(), 0)\\n self._rolled = true\\n # the method caching trick from namedtemporaryfile\\n # won\\'t work here, because _file may change from a\\n # bytesio/stringio instance to a real file. so we list\\n # all the methods directly.\\n # context management protocol\\n def __enter__(self):\\n if self._file.closed:\\n raise valueerror(\"cannot enter context with closed file\")\\n return self\\n def __exit__(self, exc, value, tb):\\n self._file.close()\\n # file protocol\\n def __iter__(self):\\n return self._file.__iter__()\\n def close(self):\\n self._file.close()\\n @property\\n def closed(self):\\n return self._file.closed\\n @property\\n def encoding(self):\\n try:\\n return self._file.encoding\\n except attributeerror:\\n if \\'b\\' in self._temporaryfileargs[\\'mode\\']:\\n raise\\n return self._temporaryfileargs[\\'encoding\\']\\n def fileno(self):\\n self.rollover()\\n return self._file.fileno()\\n def flush(self):\\n self._file.flush()\\n def isatty(self):\\n return self._file.isatty()\\n @property\\n def mode(self):\\n try:\\n return self._file.mode\\n except attributeerror:\\n return self._temporaryfileargs[\\'mode\\']\\n @property\\n def name(self):\\n try:\\n return self._file.name\\n except attributeerror:\\n return none\\n @property\\n def newlines(self):\\n try:\\n return self._file.newlines\\n except attributeerror:\\n if \\'b\\' in self._temporaryfileargs[\\'mode\\']:\\n raise\\n return self._temporaryfileargs[\\'newline\\']\\n def read(self, *args):\\n return self._file.read(*args)\\n def readline(self, *args):\\n return self._file.readline(*args)\\n def readlines(self, *args):\\n return self._file.readlines(*args)\\n def seek(self, *args):\\n self._file.seek(*args)\\n @property\\n def softspace(self):\\n return self._file.softspace\\n def tell(self):\\n return self._file.tell()\\n def truncate(self, size=none):\\n if size is none:\\n self._file.truncate()\\n else:\\n if size > self._max_size:\\n self.rollover()\\n self._file.truncate(size)\\n def write(self, s):\\n file = self._file\\n rv = file.write(s)\\n self._check(file)\\n return rv\\n def writelines(self, iterable):\\n file = self._file\\n rv = file.writelines(iterable)\\n self._check(file)\\n return rv\\n class temporarydirectory(object):\\n \"\"\"create and return a temporary directory.  this has the same\\n behavior as mkdtemp but can be used as a context manager.  for\\n example:\\n with temporarydirectory() as tmpdir:\\n ...\\n upon exiting the context, the directory and everything contained\\n in it are removed.\\n \"\"\"\\n def __init__(self, suffix=none, prefix=none, dir=none):\\n self.name = mkdtemp(suffix, prefix, dir)\\n self._finalizer = _weakref.finalize(\\n self, self._cleanup, self.name,\\n warn_message=\"implicitly cleaning up {!r}\".format(self))\\n @classmethod\\n def _cleanup(cls, name, warn_message):\\n _rmtree(name)\\n _warnings.warn(warn_message, resourcewarning)\\n def __repr__(self):\\n return \"<{} {!r}>\".format(self.__class__.__name__, self.name)\\n def __enter__(self):\\n return self.name\\n def __exit__(self, exc, value, tb):\\n self.cleanup()\\n def cleanup(self):\\n if self._finalizer.detach():\\n _rmtree(self.name)\\n \"\"\"token constants (from \"token.h\").\"\"\"\\n __all__ = [\\'tok_name\\', \\'isterminal\\', \\'isnonterminal\\', \\'iseof\\']\\n #  this file is automatically generated; please don\\'t muck it up!\\n #\\n #  to update the symbols in this file, \\'cd\\' to the top directory of\\n #  the python source tree after building the interpreter and run:\\n #\\n #    ./python lib/token.py\\n #--start constants--\\n endmarker = 0\\n name = 1\\n number = 2\\n string = 3\\n newline = 4\\n indent = 5\\n dedent = 6\\n lpar = 7\\n rpar = 8\\n lsqb = 9\\n rsqb = 10\\n colon = 11\\n comma = 12\\n semi = 13\\n plus = 14\\n minus = 15\\n star = 16\\n slash = 17\\n vbar = 18\\n amper = 19\\n less = 20\\n greater = 21\\n equal = 22\\n dot = 23\\n percent = 24\\n lbrace = 25\\n rbrace = 26\\n eqequal = 27\\n notequal = 28\\n lessequal = 29\\n greaterequal = 30\\n tilde = 31\\n circumflex = 32\\n leftshift = 33\\n rightshift = 34\\n doublestar = 35\\n plusequal = 36\\n minequal = 37\\n starequal = 38\\n slashequal = 39\\n percentequal = 40\\n amperequal = 41\\n vbarequal = 42\\n circumflexequal = 43\\n leftshiftequal = 44\\n rightshiftequal = 45\\n doublestarequal = 46\\n doubleslash = 47\\n doubleslashequal = 48\\n at = 49\\n atequal = 50\\n rarrow = 51\\n ellipsis = 52\\n op = 53\\n await = 54\\n async = 55\\n errortoken = 56\\n n_tokens = 57\\n nt_offset = 256\\n #--end constants--\\n tok_name = {value: name\\n for name, value in globals().items()\\n if isinstance(value, int) and not name.startswith(\\'_\\')}\\n __all__.extend(tok_name.values())\\n def isterminal(x):\\n return x < nt_offset\\n def isnonterminal(x):\\n return x >= nt_offset\\n def iseof(x):\\n return x == endmarker\\n def _main():\\n import re\\n import sys\\n args = sys.argv[1:]\\n infilename = args and args[0] or \"include/token.h\"\\n outfilename = \"lib/token.py\"\\n if len(args) > 1:\\n outfilename = args[1]\\n try:\\n fp = open(infilename)\\n except oserror as err:\\n sys.stdout.write(\"i/o error: %s\\\\n\" % str(err))\\n sys.exit(1)\\n with fp:\\n lines = fp.read().split(\"\\\\n\")\\n prog = re.compile(\\n \"#define[ \\\\t][ \\\\t]*([a-z0-9][a-z0-9_]*)[ \\\\t][ \\\\t]*([0-9][0-9]*)\",\\n re.ignorecase)\\n tokens = {}\\n for line in lines:\\n match = prog.match(line)\\n if match:\\n name, val = match.group(1, 2)\\n val = int(val)\\n tokens[val] = name          # reverse so we can sort them...\\n keys = sorted(tokens.keys())\\n # load the output skeleton from the target:\\n try:\\n fp = open(outfilename)\\n except oserror as err:\\n sys.stderr.write(\"i/o error: %s\\\\n\" % str(err))\\n sys.exit(2)\\n with fp:\\n format = fp.read().split(\"\\\\n\")\\n try:\\n start = format.index(\"#--start constants--\") + 1\\n end = format.index(\"#--end constants--\")\\n except valueerror:\\n sys.stderr.write(\"target does not contain format markers\")\\n sys.exit(3)\\n lines = []\\n for val in keys:\\n lines.append(\"%s = %d\" % (tokens[val], val))\\n format[start:end] = lines\\n try:\\n fp = open(outfilename, \\'w\\')\\n except oserror as err:\\n sys.stderr.write(\"i/o error: %s\\\\n\" % str(err))\\n sys.exit(4)\\n with fp:\\n fp.write(\"\\\\n\".join(format))\\n if __name__ == \"__main__\":\\n _main()\\n \"\"\"tokenization help for python programs.\\n tokenize(readline) is a generator that breaks a stream of bytes into\\n python tokens.  it decodes the bytes according to pep-0263 for\\n determining source file encoding.\\n it accepts a readline-like method which is called repeatedly to get the\\n next line of input (or b\"\" for eof).  it generates 5-tuples with these\\n members:\\n the token type (see token.py)\\n the token (a string)\\n the starting (row, column) indices of the token (a 2-tuple of ints)\\n the ending (row, column) indices of the token (a 2-tuple of ints)\\n the original line (string)\\n it is designed to match the working of the python tokenizer exactly, except\\n that it produces comment tokens for comments and gives type op for all\\n operators.  additionally, all token lists start with an encoding token\\n which tells you which encoding was used to decode the bytes stream.\\n \"\"\"\\n __author__ = \\'ka-ping yee <ping@lfw.org>\\'\\n __credits__ = (\\'gvr, esr, tim peters, thomas wouters, fred drake, \\'\\n \\'skip montanaro, raymond hettinger, trent nelson, \\'\\n \\'michael foord\\')\\n from builtins import open as _builtin_open\\n from codecs import lookup, bom_utf8\\n import collections\\n from io import textiowrapper\\n from itertools import chain\\n import re\\n import sys\\n from token import *\\n cookie_re = re.compile(r\\'^[ \\\\t\\\\f]*#.*?coding[:=][ \\\\t]*([-\\\\w.]+)\\', re.ascii)\\n blank_re = re.compile(br\\'^[ \\\\t\\\\f]*(?:[#\\\\r\\\\n]|$)\\', re.ascii)\\n import token\\n __all__ = token.__all__ + [\"comment\", \"tokenize\", \"detect_encoding\",\\n \"nl\", \"untokenize\", \"encoding\", \"tokeninfo\"]\\n del token\\n comment = n_tokens\\n tok_name[comment] = \\'comment\\'\\n nl = n_tokens + 1\\n tok_name[nl] = \\'nl\\'\\n encoding = n_tokens + 2\\n tok_name[encoding] = \\'encoding\\'\\n n_tokens += 3\\n exact_token_types = {\\n \\'(\\':   lpar,\\n \\')\\':   rpar,\\n \\'[\\':   lsqb,\\n \\']\\':   rsqb,\\n \\':\\':   colon,\\n \\',\\':   comma,\\n \\';\\':   semi,\\n \\'+\\':   plus,\\n \\'-\\':   minus,\\n \\'*\\':   star,\\n \\'/\\':   slash,\\n \\'|\\':   vbar,\\n \\'&\\':   amper,\\n \\'<\\':   less,\\n \\'>\\':   greater,\\n \\'=\\':   equal,\\n \\'.\\':   dot,\\n \\'%\\':   percent,\\n \\'{\\':   lbrace,\\n \\'}\\':   rbrace,\\n \\'==\\':  eqequal,\\n \\'!=\\':  notequal,\\n \\'<=\\':  lessequal,\\n \\'>=\\':  greaterequal,\\n \\'~\\':   tilde,\\n \\'^\\':   circumflex,\\n \\'<<\\':  leftshift,\\n \\'>>\\':  rightshift,\\n \\'**\\':  doublestar,\\n \\'+=\\':  plusequal,\\n \\'-=\\':  minequal,\\n \\'*=\\':  starequal,\\n \\'/=\\':  slashequal,\\n \\'%=\\':  percentequal,\\n \\'&=\\':  amperequal,\\n \\'|=\\':  vbarequal,\\n \\'^=\\': circumflexequal,\\n \\'<<=\\': leftshiftequal,\\n \\'>>=\\': rightshiftequal,\\n \\'**=\\': doublestarequal,\\n \\'//\\':  doubleslash,\\n \\'//=\\': doubleslashequal,\\n \\'@\\':   at,\\n \\'@=\\':  atequal,\\n }\\n class tokeninfo(collections.namedtuple(\\'tokeninfo\\', \\'type string start end line\\')):\\n def __repr__(self):\\n annotated_type = \\'%d (%s)\\' % (self.type, tok_name[self.type])\\n return (\\'tokeninfo(type=%s, string=%r, start=%r, end=%r, line=%r)\\' %\\n self._replace(type=annotated_type))\\n @property\\n def exact_type(self):\\n if self.type == op and self.string in exact_token_types:\\n return exact_token_types[self.string]\\n else:\\n return self.type\\n def group(*choices): return \\'(\\' + \\'|\\'.join(choices) + \\')\\'\\n def any(*choices): return group(*choices) + \\'*\\'\\n def maybe(*choices): return group(*choices) + \\'?\\'\\n # note: we use unicode matching for names (\"\\\\w\") but ascii matching for\\n # number literals.\\n whitespace = r\\'[ \\\\f\\\\t]*\\'\\n comment = r\\'#[^\\\\r\\\\n]*\\'\\n ignore = whitespace + any(r\\'\\\\\\\\\\\\r?\\\\n\\' + whitespace) + maybe(comment)\\n name = r\\'\\\\w+\\'\\n hexnumber = r\\'0[xx][0-9a-fa-f]+\\'\\n binnumber = r\\'0[bb][01]+\\'\\n octnumber = r\\'0[oo][0-7]+\\'\\n decnumber = r\\'(?:0+|[1-9][0-9]*)\\'\\n intnumber = group(hexnumber, binnumber, octnumber, decnumber)\\n exponent = r\\'[ee][-+]?[0-9]+\\'\\n pointfloat = group(r\\'[0-9]+\\\\.[0-9]*\\', r\\'\\\\.[0-9]+\\') + maybe(exponent)\\n expfloat = r\\'[0-9]+\\' + exponent\\n floatnumber = group(pointfloat, expfloat)\\n imagnumber = group(r\\'[0-9]+[jj]\\', floatnumber + r\\'[jj]\\')\\n number = group(imagnumber, floatnumber, intnumber)\\n stringprefix = r\\'(?:[bb][rr]?|[rr][bb]?|[uu])?\\'\\n # tail end of \\' string.\\n single = r\"[^\\'\\\\\\\\]*(?:\\\\\\\\.[^\\'\\\\\\\\]*)*\\'\"\\n # tail end of \" string.\\n double = r\\'[^\"\\\\\\\\]*(?:\\\\\\\\.[^\"\\\\\\\\]*)*\"\\'\\n # tail end of \\'\\'\\' string.\\n single3 = r\"[^\\'\\\\\\\\]*(?:(?:\\\\\\\\.|\\'(?!\\'\\'))[^\\'\\\\\\\\]*)*\\'\\'\\'\"\\n # tail end of \"\"\" string.\\n double3 = r\\'[^\"\\\\\\\\]*(?:(?:\\\\\\\\.|\"(?!\"\"))[^\"\\\\\\\\]*)*\"\"\"\\'\\n triple = group(stringprefix + \"\\'\\'\\'\", stringprefix + \\'\"\"\"\\')\\n # single-line \\' or \" string.\\n string = group(stringprefix + r\"\\'[^\\\\n\\'\\\\\\\\]*(?:\\\\\\\\.[^\\\\n\\'\\\\\\\\]*)*\\'\",\\n stringprefix + r\\'\"[^\\\\n\"\\\\\\\\]*(?:\\\\\\\\.[^\\\\n\"\\\\\\\\]*)*\"\\')\\n # because of leftmost-then-longest match semantics, be sure to put the\\n # longest operators first (e.g., if = came before ==, == would get\\n # recognized as two instances of =).\\n operator = group(r\"\\\\*\\\\*=?\", r\">>=?\", r\"<<=?\", r\"!=\",\\n r\"//=?\", r\"->\",\\n r\"[+\\\\-*/%&@|^=<>]=?\",\\n r\"~\")\\n bracket = \\'[][(){}]\\'\\n special = group(r\\'\\\\r?\\\\n\\', r\\'\\\\.\\\\.\\\\.\\', r\\'[:;.,@]\\')\\n funny = group(operator, bracket, special)\\n plaintoken = group(number, funny, string, name)\\n token = ignore + plaintoken\\n # first (or only) line of \\' or \" string.\\n contstr = group(stringprefix + r\"\\'[^\\\\n\\'\\\\\\\\]*(?:\\\\\\\\.[^\\\\n\\'\\\\\\\\]*)*\" +\\n group(\"\\'\", r\\'\\\\\\\\\\\\r?\\\\n\\'),\\n stringprefix + r\\'\"[^\\\\n\"\\\\\\\\]*(?:\\\\\\\\.[^\\\\n\"\\\\\\\\]*)*\\' +\\n group(\\'\"\\', r\\'\\\\\\\\\\\\r?\\\\n\\'))\\n pseudoextras = group(r\\'\\\\\\\\\\\\r?\\\\n|\\\\z\\', comment, triple)\\n pseudotoken = whitespace + group(pseudoextras, number, funny, contstr, name)\\n def _compile(expr):\\n return re.compile(expr, re.unicode)\\n endpats = {\"\\'\": single, \\'\"\\': double,\\n \"\\'\\'\\'\": single3, \\'\"\"\"\\': double3,\\n \"r\\'\\'\\'\": single3, \\'r\"\"\"\\': double3,\\n \"b\\'\\'\\'\": single3, \\'b\"\"\"\\': double3,\\n \"r\\'\\'\\'\": single3, \\'r\"\"\"\\': double3,\\n \"b\\'\\'\\'\": single3, \\'b\"\"\"\\': double3,\\n \"br\\'\\'\\'\": single3, \\'br\"\"\"\\': double3,\\n \"br\\'\\'\\'\": single3, \\'br\"\"\"\\': double3,\\n \"br\\'\\'\\'\": single3, \\'br\"\"\"\\': double3,\\n \"br\\'\\'\\'\": single3, \\'br\"\"\"\\': double3,\\n \"rb\\'\\'\\'\": single3, \\'rb\"\"\"\\': double3,\\n \"rb\\'\\'\\'\": single3, \\'rb\"\"\"\\': double3,\\n \"rb\\'\\'\\'\": single3, \\'rb\"\"\"\\': double3,\\n \"rb\\'\\'\\'\": single3, \\'rb\"\"\"\\': double3,\\n \"u\\'\\'\\'\": single3, \\'u\"\"\"\\': double3,\\n \"u\\'\\'\\'\": single3, \\'u\"\"\"\\': double3,\\n \\'r\\': none, \\'r\\': none, \\'b\\': none, \\'b\\': none,\\n \\'u\\': none, \\'u\\': none}\\n triple_quoted = {}\\n for t in (\"\\'\\'\\'\", \\'\"\"\"\\',\\n \"r\\'\\'\\'\", \\'r\"\"\"\\', \"r\\'\\'\\'\", \\'r\"\"\"\\',\\n \"b\\'\\'\\'\", \\'b\"\"\"\\', \"b\\'\\'\\'\", \\'b\"\"\"\\',\\n \"br\\'\\'\\'\", \\'br\"\"\"\\', \"br\\'\\'\\'\", \\'br\"\"\"\\',\\n \"br\\'\\'\\'\", \\'br\"\"\"\\', \"br\\'\\'\\'\", \\'br\"\"\"\\',\\n \"rb\\'\\'\\'\", \\'rb\"\"\"\\', \"rb\\'\\'\\'\", \\'rb\"\"\"\\',\\n \"rb\\'\\'\\'\", \\'rb\"\"\"\\', \"rb\\'\\'\\'\", \\'rb\"\"\"\\',\\n \"u\\'\\'\\'\", \\'u\"\"\"\\', \"u\\'\\'\\'\", \\'u\"\"\"\\',\\n ):\\n triple_quoted[t] = t\\n single_quoted = {}\\n for t in (\"\\'\", \\'\"\\',\\n \"r\\'\", \\'r\"\\', \"r\\'\", \\'r\"\\',\\n \"b\\'\", \\'b\"\\', \"b\\'\", \\'b\"\\',\\n \"br\\'\", \\'br\"\\', \"br\\'\", \\'br\"\\',\\n \"br\\'\", \\'br\"\\', \"br\\'\", \\'br\"\\' ,\\n \"rb\\'\", \\'rb\"\\', \"rb\\'\", \\'rb\"\\',\\n \"rb\\'\", \\'rb\"\\', \"rb\\'\", \\'rb\"\\' ,\\n \"u\\'\", \\'u\"\\', \"u\\'\", \\'u\"\\',\\n ):\\n single_quoted[t] = t\\n tabsize = 8\\n class tokenerror(exception): pass\\n class stoptokenizing(exception): pass\\n class untokenizer:\\n def __init__(self):\\n self.tokens = []\\n self.prev_row = 1\\n self.prev_col = 0\\n self.encoding = none\\n def add_whitespace(self, start):\\n row, col = start\\n if row < self.prev_row or row == self.prev_row and col < self.prev_col:\\n raise valueerror(\"start ({},{}) precedes previous end ({},{})\"\\n .format(row, col, self.prev_row, self.prev_col))\\n row_offset = row - self.prev_row\\n if row_offset:\\n self.tokens.append(\"\\\\\\\\\\\\n\" * row_offset)\\n self.prev_col = 0\\n col_offset = col - self.prev_col\\n if col_offset:\\n self.tokens.append(\" \" * col_offset)\\n def untokenize(self, iterable):\\n it = iter(iterable)\\n indents = []\\n startline = false\\n for t in it:\\n if len(t) == 2:\\n self.compat(t, it)\\n break\\n tok_type, token, start, end, line = t\\n if tok_type == encoding:\\n self.encoding = token\\n continue\\n if tok_type == endmarker:\\n break\\n if tok_type == indent:\\n indents.append(token)\\n continue\\n elif tok_type == dedent:\\n indents.pop()\\n self.prev_row, self.prev_col = end\\n continue\\n elif tok_type in (newline, nl):\\n startline = true\\n elif startline and indents:\\n indent = indents[-1]\\n if start[1] >= len(indent):\\n self.tokens.append(indent)\\n self.prev_col = len(indent)\\n startline = false\\n self.add_whitespace(start)\\n self.tokens.append(token)\\n self.prev_row, self.prev_col = end\\n if tok_type in (newline, nl):\\n self.prev_row += 1\\n self.prev_col = 0\\n return \"\".join(self.tokens)\\n def compat(self, token, iterable):\\n indents = []\\n toks_append = self.tokens.append\\n startline = token[0] in (newline, nl)\\n prevstring = false\\n for tok in chain([token], iterable):\\n toknum, tokval = tok[:2]\\n if toknum == encoding:\\n self.encoding = tokval\\n continue\\n if toknum in (name, number, async, await):\\n tokval += \\' \\'\\n # insert a space between two consecutive strings\\n if toknum == string:\\n if prevstring:\\n tokval = \\' \\' + tokval\\n prevstring = true\\n else:\\n prevstring = false\\n if toknum == indent:\\n indents.append(tokval)\\n continue\\n elif toknum == dedent:\\n indents.pop()\\n continue\\n elif toknum in (newline, nl):\\n startline = true\\n elif startline and indents:\\n toks_append(indents[-1])\\n startline = false\\n toks_append(tokval)\\n def untokenize(iterable):\\n \"\"\"transform tokens back into python source code.\\n it returns a bytes object, encoded using the encoding\\n token, which is the first token sequence output by tokenize.\\n each element returned by the iterable must be a token sequence\\n with at least two elements, a token number and token value.  if\\n only two tokens are passed, the resulting output is poor.\\n round-trip invariant for full input:\\n untokenized source will match input source exactly\\n round-trip invariant for limited input:\\n # output bytes will tokenize back to the input\\n t1 = [tok[:2] for tok in tokenize(f.readline)]\\n newcode = untokenize(t1)\\n readline = bytesio(newcode).readline\\n t2 = [tok[:2] for tok in tokenize(readline)]\\n assert t1 == t2\\n \"\"\"\\n ut = untokenizer()\\n out = ut.untokenize(iterable)\\n if ut.encoding is not none:\\n out = out.encode(ut.encoding)\\n return out\\n def _get_normal_name(orig_enc):\\n \"\"\"imitates get_normal_name in tokenizer.c.\"\"\"\\n # only care about the first 12 characters.\\n enc = orig_enc[:12].lower().replace(\"_\", \"-\")\\n if enc == \"utf-8\" or enc.startswith(\"utf-8-\"):\\n return \"utf-8\"\\n if enc in (\"latin-1\", \"iso-8859-1\", \"iso-latin-1\") or \\\\\\n enc.startswith((\"latin-1-\", \"iso-8859-1-\", \"iso-latin-1-\")):\\n return \"iso-8859-1\"\\n return orig_enc\\n def detect_encoding(readline):\\n \"\"\"\\n the detect_encoding() function is used to detect the encoding that should\\n be used to decode a python source file.  it requires one argument, readline,\\n in the same way as the tokenize() generator.\\n it will call readline a maximum of twice, and return the encoding used\\n (as a string) and a list of any lines (left as bytes) it has read in.\\n it detects the encoding from the presence of a utf-8 bom or an encoding\\n cookie as specified in pep-0263.  if both a bom and a cookie are present,\\n but disagree, a syntaxerror will be raised.  if the encoding cookie is an\\n invalid charset, raise a syntaxerror.  note that if a utf-8 bom is found,\\n \\'utf-8-sig\\' is returned.\\n if no encoding is specified, then the default of \\'utf-8\\' will be returned.\\n \"\"\"\\n try:\\n filename = readline.__self__.name\\n except attributeerror:\\n filename = none\\n bom_found = false\\n encoding = none\\n default = \\'utf-8\\'\\n def read_or_stop():\\n try:\\n return readline()\\n except stopiteration:\\n return b\\'\\'\\n def find_cookie(line):\\n try:\\n # decode as utf-8. either the line is an encoding declaration,\\n # in which case it should be pure ascii, or it must be utf-8\\n # per default encoding.\\n line_string = line.decode(\\'utf-8\\')\\n except unicodedecodeerror:\\n msg = \"invalid or missing encoding declaration\"\\n if filename is not none:\\n msg = \\'{} for {!r}\\'.format(msg, filename)\\n raise syntaxerror(msg)\\n match = cookie_re.match(line_string)\\n if not match:\\n return none\\n encoding = _get_normal_name(match.group(1))\\n try:\\n codec = lookup(encoding)\\n except lookuperror:\\n # this behaviour mimics the python interpreter\\n if filename is none:\\n msg = \"unknown encoding: \" + encoding\\n else:\\n msg = \"unknown encoding for {!r}: {}\".format(filename,\\n encoding)\\n raise syntaxerror(msg)\\n if bom_found:\\n if encoding != \\'utf-8\\':\\n # this behaviour mimics the python interpreter\\n if filename is none:\\n msg = \\'encoding problem: utf-8\\'\\n else:\\n msg = \\'encoding problem for {!r}: utf-8\\'.format(filename)\\n raise syntaxerror(msg)\\n encoding += \\'-sig\\'\\n return encoding\\n first = read_or_stop()\\n if first.startswith(bom_utf8):\\n bom_found = true\\n first = first[3:]\\n default = \\'utf-8-sig\\'\\n if not first:\\n return default, []\\n encoding = find_cookie(first)\\n if encoding:\\n return encoding, [first]\\n if not blank_re.match(first):\\n return default, [first]\\n second = read_or_stop()\\n if not second:\\n return default, [first]\\n encoding = find_cookie(second)\\n if encoding:\\n return encoding, [first, second]\\n return default, [first, second]\\n def open(filename):\\n \"\"\"open a file in read only mode using the encoding detected by\\n detect_encoding().\\n \"\"\"\\n buffer = _builtin_open(filename, \\'rb\\')\\n try:\\n encoding, lines = detect_encoding(buffer.readline)\\n buffer.seek(0)\\n text = textiowrapper(buffer, encoding, line_buffering=true)\\n text.mode = \\'r\\'\\n return text\\n except:\\n buffer.close()\\n raise\\n def tokenize(readline):\\n \"\"\"\\n the tokenize() generator requires one argument, readline, which\\n must be a callable object which provides the same interface as the\\n readline() method of built-in file objects.  each call to the function\\n should return one line of input as bytes.  alternatively, readline\\n can be a callable function terminating with stopiteration:\\n readline = open(myfile, \\'rb\\').__next__  # example of alternate readline\\n the generator produces 5-tuples with these members: the token type; the\\n token string; a 2-tuple (srow, scol) of ints specifying the row and\\n column where the token begins in the source; a 2-tuple (erow, ecol) of\\n ints specifying the row and column where the token ends in the source;\\n and the line on which the token was found.  the line passed is the\\n logical line; continuation lines are included.\\n the first token sequence will always be an encoding token\\n which tells you which encoding was used to decode the bytes stream.\\n \"\"\"\\n # this import is here to avoid problems when the itertools module is not\\n # built yet and tokenize is imported.\\n from itertools import chain, repeat\\n encoding, consumed = detect_encoding(readline)\\n rl_gen = iter(readline, b\"\")\\n empty = repeat(b\"\")\\n return _tokenize(chain(consumed, rl_gen, empty).__next__, encoding)\\n def _tokenize(readline, encoding):\\n lnum = parenlev = continued = 0\\n numchars = \\'0123456789\\'\\n contstr, needcont = \\'\\', 0\\n contline = none\\n indents = [0]\\n # \\'stashed\\' and \\'async_*\\' are used for async/await parsing\\n stashed = none\\n async_def = false\\n async_def_indent = 0\\n async_def_nl = false\\n if encoding is not none:\\n if encoding == \"utf-8-sig\":\\n # bom will already have been stripped.\\n encoding = \"utf-8\"\\n yield tokeninfo(encoding, encoding, (0, 0), (0, 0), \\'\\')\\n while true:             # loop over lines in stream\\n try:\\n line = readline()\\n except stopiteration:\\n line = b\\'\\'\\n if encoding is not none:\\n line = line.decode(encoding)\\n lnum += 1\\n pos, max = 0, len(line)\\n if contstr:                            # continued string\\n if not line:\\n raise tokenerror(\"eof in multi-line string\", strstart)\\n endmatch = endprog.match(line)\\n if endmatch:\\n pos = end = endmatch.end(0)\\n yield tokeninfo(string, contstr + line[:end],\\n strstart, (lnum, end), contline + line)\\n contstr, needcont = \\'\\', 0\\n contline = none\\n elif needcont and line[-2:] != \\'\\\\\\\\\\\\n\\' and line[-3:] != \\'\\\\\\\\\\\\r\\\\n\\':\\n yield tokeninfo(errortoken, contstr + line,\\n strstart, (lnum, len(line)), contline)\\n contstr = \\'\\'\\n contline = none\\n continue\\n else:\\n contstr = contstr + line\\n contline = contline + line\\n continue\\n elif parenlev == 0 and not continued:  # new statement\\n if not line: break\\n column = 0\\n while pos < max:                   # measure leading whitespace\\n if line[pos] == \\' \\':\\n column += 1\\n elif line[pos] == \\'\\\\t\\':\\n column = (column//tabsize + 1)*tabsize\\n elif line[pos] == \\'\\\\f\\':\\n column = 0\\n else:\\n break\\n pos += 1\\n if pos == max:\\n break\\n if line[pos] in \\'#\\\\r\\\\n\\':           # skip comments or blank lines\\n if line[pos] == \\'#\\':\\n comment_token = line[pos:].rstrip(\\'\\\\r\\\\n\\')\\n nl_pos = pos + len(comment_token)\\n yield tokeninfo(comment, comment_token,\\n (lnum, pos), (lnum, pos + len(comment_token)), line)\\n yield tokeninfo(nl, line[nl_pos:],\\n (lnum, nl_pos), (lnum, len(line)), line)\\n else:\\n yield tokeninfo((nl, comment)[line[pos] == \\'#\\'], line[pos:],\\n (lnum, pos), (lnum, len(line)), line)\\n continue\\n if column > indents[-1]:           # count indents or dedents\\n indents.append(column)\\n yield tokeninfo(indent, line[:pos], (lnum, 0), (lnum, pos), line)\\n while column < indents[-1]:\\n if column not in indents:\\n raise indentationerror(\\n \"unindent does not match any outer indentation level\",\\n (\"<tokenize>\", lnum, pos, line))\\n indents = indents[:-1]\\n if async_def and async_def_indent >= indents[-1]:\\n async_def = false\\n async_def_nl = false\\n async_def_indent = 0\\n yield tokeninfo(dedent, \\'\\', (lnum, pos), (lnum, pos), line)\\n if async_def and async_def_nl and async_def_indent >= indents[-1]:\\n async_def = false\\n async_def_nl = false\\n async_def_indent = 0\\n else:                                  # continued statement\\n if not line:\\n raise tokenerror(\"eof in multi-line statement\", (lnum, 0))\\n continued = 0\\n while pos < max:\\n pseudomatch = _compile(pseudotoken).match(line, pos)\\n if pseudomatch:                                # scan for tokens\\n start, end = pseudomatch.span(1)\\n spos, epos, pos = (lnum, start), (lnum, end), end\\n if start == end:\\n continue\\n token, initial = line[start:end], line[start]\\n if (initial in numchars or                  # ordinary number\\n (initial == \\'.\\' and token != \\'.\\' and token != \\'...\\')):\\n yield tokeninfo(number, token, spos, epos, line)\\n elif initial in \\'\\\\r\\\\n\\':\\n if stashed:\\n yield stashed\\n stashed = none\\n if parenlev > 0:\\n yield tokeninfo(nl, token, spos, epos, line)\\n else:\\n yield tokeninfo(newline, token, spos, epos, line)\\n if async_def:\\n async_def_nl = true\\n elif initial == \\'#\\':\\n assert not token.endswith(\"\\\\n\")\\n if stashed:\\n yield stashed\\n stashed = none\\n yield tokeninfo(comment, token, spos, epos, line)\\n elif token in triple_quoted:\\n endprog = _compile(endpats[token])\\n endmatch = endprog.match(line, pos)\\n if endmatch:                           # all on one line\\n pos = endmatch.end(0)\\n token = line[start:pos]\\n yield tokeninfo(string, token, spos, (lnum, pos), line)\\n else:\\n strstart = (lnum, start)           # multiple lines\\n contstr = line[start:]\\n contline = line\\n break\\n elif initial in single_quoted or \\\\\\n token[:2] in single_quoted or \\\\\\n token[:3] in single_quoted:\\n if token[-1] == \\'\\\\n\\':                  # continued string\\n strstart = (lnum, start)\\n endprog = _compile(endpats[initial] or\\n endpats[token[1]] or\\n endpats[token[2]])\\n contstr, needcont = line[start:], 1\\n contline = line\\n break\\n else:                                  # ordinary string\\n yield tokeninfo(string, token, spos, epos, line)\\n elif initial.isidentifier():               # ordinary name\\n if token in (\\'async\\', \\'await\\'):\\n if async_def:\\n yield tokeninfo(\\n async if token == \\'async\\' else await,\\n token, spos, epos, line)\\n continue\\n tok = tokeninfo(name, token, spos, epos, line)\\n if token == \\'async\\' and not stashed:\\n stashed = tok\\n continue\\n if token == \\'def\\':\\n if (stashed\\n and stashed.type == name\\n and stashed.string == \\'async\\'):\\n async_def = true\\n async_def_indent = indents[-1]\\n yield tokeninfo(async, stashed.string,\\n stashed.start, stashed.end,\\n stashed.line)\\n stashed = none\\n if stashed:\\n yield stashed\\n stashed = none\\n yield tok\\n elif initial == \\'\\\\\\\\\\':                      # continued stmt\\n continued = 1\\n else:\\n if initial in \\'([{\\':\\n parenlev += 1\\n elif initial in \\')]}\\':\\n parenlev -= 1\\n if stashed:\\n yield stashed\\n stashed = none\\n yield tokeninfo(op, token, spos, epos, line)\\n else:\\n yield tokeninfo(errortoken, line[pos],\\n (lnum, pos), (lnum, pos+1), line)\\n pos += 1\\n if stashed:\\n yield stashed\\n stashed = none\\n for indent in indents[1:]:                 # pop remaining indent levels\\n yield tokeninfo(dedent, \\'\\', (lnum, 0), (lnum, 0), \\'\\')\\n yield tokeninfo(endmarker, \\'\\', (lnum, 0), (lnum, 0), \\'\\')\\n # an undocumented, backwards compatible, api for all the places in the standard\\n # library that expect to be able to use tokenize with strings\\n def generate_tokens(readline):\\n return _tokenize(readline, none)\\n def main():\\n import argparse\\n # helper error handling routines\\n def perror(message):\\n print(message, file=sys.stderr)\\n def error(message, filename=none, location=none):\\n if location:\\n args = (filename,) + location + (message,)\\n perror(\"%s:%d:%d: error: %s\" % args)\\n elif filename:\\n perror(\"%s: error: %s\" % (filename, message))\\n else:\\n perror(\"error: %s\" % message)\\n sys.exit(1)\\n # parse the arguments and options\\n parser = argparse.argumentparser(prog=\\'python -m tokenize\\')\\n parser.add_argument(dest=\\'filename\\', nargs=\\'?\\',\\n metavar=\\'filename.py\\',\\n help=\\'the file to tokenize; defaults to stdin\\')\\n parser.add_argument(\\'-e\\', \\'--exact\\', dest=\\'exact\\', action=\\'store_true\\',\\n help=\\'display token names using the exact type\\')\\n args = parser.parse_args()\\n try:\\n # tokenize the input\\n if args.filename:\\n filename = args.filename\\n with _builtin_open(filename, \\'rb\\') as f:\\n tokens = list(tokenize(f.readline))\\n else:\\n filename = \"<stdin>\"\\n tokens = _tokenize(sys.stdin.readline, none)\\n # output the tokenization\\n for token in tokens:\\n token_type = token.type\\n if args.exact:\\n token_type = token.exact_type\\n token_range = \"%d,%d-%d,%d:\" % (token.start + token.end)\\n print(\"%-20s%-15s%-15r\" %\\n (token_range, tok_name[token_type], token.string))\\n except indentationerror as err:\\n line, column = err.args[1][1:3]\\n error(err.args[0], filename, (line, column))\\n except tokenerror as err:\\n line, column = err.args[1]\\n error(err.args[0], filename, (line, column))\\n except syntaxerror as err:\\n error(err, filename)\\n except oserror as err:\\n error(err)\\n except keyboardinterrupt:\\n print(\"interrupted\\\\n\")\\n except exception as err:\\n perror(\"unexpected error: %s\" % err)\\n raise\\n if __name__ == \"__main__\":\\n main()\\n \"\"\"\\n define names for built-in types that aren\\'t directly accessible as a builtin.\\n \"\"\"\\n import sys\\n # iterators in python aren\\'t a matter of type but of protocol.  a large\\n # and changing number of builtin types implement *some* flavor of\\n # iterator.  don\\'t check the type!  use hasattr to check for both\\n # \"__iter__\" and \"__next__\" attributes instead.\\n def _f(): pass\\n functiontype = type(_f)\\n lambdatype = type(lambda: none)         # same as functiontype\\n codetype = type(_f.__code__)\\n mappingproxytype = type(type.__dict__)\\n simplenamespace = type(sys.implementation)\\n def _g():\\n yield 1\\n generatortype = type(_g())\\n async def _c(): pass\\n _c = _c()\\n coroutinetype = type(_c)\\n _c.close()  # prevent resourcewarning\\n class _c:\\n def _m(self): pass\\n methodtype = type(_c()._m)\\n builtinfunctiontype = type(len)\\n builtinmethodtype = type([].append)     # same as builtinfunctiontype\\n moduletype = type(sys)\\n try:\\n raise typeerror\\n except typeerror:\\n tb = sys.exc_info()[2]\\n tracebacktype = type(tb)\\n frametype = type(tb.tb_frame)\\n tb = none; del tb\\n # for jython, the following two types are identical\\n getsetdescriptortype = type(functiontype.__code__)\\n memberdescriptortype = type(functiontype.__globals__)\\n del sys, _f, _g, _c, _c,                           # not for export\\n # provide a pep 3115 compliant mechanism for class creation\\n def new_class(name, bases=(), kwds=none, exec_body=none):\\n \"\"\"create a class object dynamically using the appropriate metaclass.\"\"\"\\n meta, ns, kwds = prepare_class(name, bases, kwds)\\n if exec_body is not none:\\n exec_body(ns)\\n return meta(name, bases, ns, **kwds)\\n def prepare_class(name, bases=(), kwds=none):\\n \"\"\"call the __prepare__ method of the appropriate metaclass.\\n returns (metaclass, namespace, kwds) as a 3-tuple\\n *metaclass* is the appropriate metaclass\\n *namespace* is the prepared class namespace\\n *kwds* is an updated copy of the passed in kwds argument with any\\n \\'metaclass\\' entry removed. if no kwds argument is passed in, this will\\n be an empty dict.\\n \"\"\"\\n if kwds is none:\\n kwds = {}\\n else:\\n kwds = dict(kwds) # don\\'t alter the provided mapping\\n if \\'metaclass\\' in kwds:\\n meta = kwds.pop(\\'metaclass\\')\\n else:\\n if bases:\\n meta = type(bases[0])\\n else:\\n meta = type\\n if isinstance(meta, type):\\n # when meta is a type, we first determine the most-derived metaclass\\n # instead of invoking the initial candidate directly\\n meta = _calculate_meta(meta, bases)\\n if hasattr(meta, \\'__prepare__\\'):\\n ns = meta.__prepare__(name, bases, **kwds)\\n else:\\n ns = {}\\n return meta, ns, kwds\\n def _calculate_meta(meta, bases):\\n \"\"\"calculate the most derived metaclass.\"\"\"\\n winner = meta\\n for base in bases:\\n base_meta = type(base)\\n if issubclass(winner, base_meta):\\n continue\\n if issubclass(base_meta, winner):\\n winner = base_meta\\n continue\\n # else:\\n raise typeerror(\"metaclass conflict: \"\\n \"the metaclass of a derived class \"\\n \"must be a (non-strict) subclass \"\\n \"of the metaclasses of all its bases\")\\n return winner\\n class dynamicclassattribute:\\n \"\"\"route attribute access on a class to __getattr__.\\n this is a descriptor, used to define attributes that act differently when\\n accessed through an instance and through a class.  instance access remains\\n normal, but access to an attribute through a class will be routed to the\\n class\\'s __getattr__ method; this is done by raising attributeerror.\\n this allows one to have properties active on an instance, and have virtual\\n attributes on the class with the same name (see enum for an example).\\n \"\"\"\\n def __init__(self, fget=none, fset=none, fdel=none, doc=none):\\n self.fget = fget\\n self.fset = fset\\n self.fdel = fdel\\n # next two lines make dynamicclassattribute act the same as property\\n self.__doc__ = doc or fget.__doc__\\n self.overwrite_doc = doc is none\\n # support for abstract methods\\n self.__isabstractmethod__ = bool(getattr(fget, \\'__isabstractmethod__\\', false))\\n def __get__(self, instance, ownerclass=none):\\n if instance is none:\\n if self.__isabstractmethod__:\\n return self\\n raise attributeerror()\\n elif self.fget is none:\\n raise attributeerror(\"unreadable attribute\")\\n return self.fget(instance)\\n def __set__(self, instance, value):\\n if self.fset is none:\\n raise attributeerror(\"can\\'t set attribute\")\\n self.fset(instance, value)\\n def __delete__(self, instance):\\n if self.fdel is none:\\n raise attributeerror(\"can\\'t delete attribute\")\\n self.fdel(instance)\\n def getter(self, fget):\\n fdoc = fget.__doc__ if self.overwrite_doc else none\\n result = type(self)(fget, self.fset, self.fdel, fdoc or self.__doc__)\\n result.overwrite_doc = self.overwrite_doc\\n return result\\n def setter(self, fset):\\n result = type(self)(self.fget, fset, self.fdel, self.__doc__)\\n result.overwrite_doc = self.overwrite_doc\\n return result\\n def deleter(self, fdel):\\n result = type(self)(self.fget, self.fset, fdel, self.__doc__)\\n result.overwrite_doc = self.overwrite_doc\\n return result\\n import functools as _functools\\n import collections.abc as _collections_abc\\n class _generatorwrapper:\\n # todo: implement this in c.\\n def __init__(self, gen):\\n self.__wrapped = gen\\n self.__isgen = gen.__class__ is generatortype\\n self.__name__ = getattr(gen, \\'__name__\\', none)\\n self.__qualname__ = getattr(gen, \\'__qualname__\\', none)\\n def send(self, val):\\n return self.__wrapped.send(val)\\n def throw(self, tp, *rest):\\n return self.__wrapped.throw(tp, *rest)\\n def close(self):\\n return self.__wrapped.close()\\n @property\\n def gi_code(self):\\n return self.__wrapped.gi_code\\n @property\\n def gi_frame(self):\\n return self.__wrapped.gi_frame\\n @property\\n def gi_running(self):\\n return self.__wrapped.gi_running\\n @property\\n def gi_yieldfrom(self):\\n return self.__wrapped.gi_yieldfrom\\n cr_code = gi_code\\n cr_frame = gi_frame\\n cr_running = gi_running\\n cr_await = gi_yieldfrom\\n def __next__(self):\\n return next(self.__wrapped)\\n def __iter__(self):\\n if self.__isgen:\\n return self.__wrapped\\n return self\\n __await__ = __iter__\\n def coroutine(func):\\n \"\"\"convert regular generator function to a coroutine.\"\"\"\\n if not callable(func):\\n raise typeerror(\\'types.coroutine() expects a callable\\')\\n if (func.__class__ is functiontype and\\n getattr(func, \\'__code__\\', none).__class__ is codetype):\\n co_flags = func.__code__.co_flags\\n # check if \\'func\\' is a coroutine function.\\n # (0x180 == co_coroutine | co_iterable_coroutine)\\n if co_flags & 0x180:\\n return func\\n # check if \\'func\\' is a generator function.\\n # (0x20 == co_generator)\\n if co_flags & 0x20:\\n # todo: implement this in c.\\n co = func.__code__\\n func.__code__ = codetype(\\n co.co_argcount, co.co_kwonlyargcount, co.co_nlocals,\\n co.co_stacksize,\\n co.co_flags | 0x100,  # 0x100 == co_iterable_coroutine\\n co.co_code,\\n co.co_consts, co.co_names, co.co_varnames, co.co_filename,\\n co.co_name, co.co_firstlineno, co.co_lnotab, co.co_freevars,\\n co.co_cellvars)\\n return func\\n # the following code is primarily to support functions that\\n # return generator-like objects (for instance generators\\n # compiled with cython).\\n @_functools.wraps(func)\\n def wrapped(*args, **kwargs):\\n coro = func(*args, **kwargs)\\n if (coro.__class__ is coroutinetype or\\n coro.__class__ is generatortype and coro.gi_code.co_flags & 0x100):\\n # \\'coro\\' is a native coroutine object or an iterable coroutine\\n return coro\\n if (isinstance(coro, _collections_abc.generator) and\\n not isinstance(coro, _collections_abc.coroutine)):\\n # \\'coro\\' is either a pure python generator iterator, or it\\n # implements collections.abc.generator (and does not implement\\n # collections.abc.coroutine).\\n return _generatorwrapper(coro)\\n # \\'coro\\' is either an instance of collections.abc.coroutine or\\n # some other object -- pass it through.\\n return coro\\n return wrapped\\n __all__ = [n for n in globals() if n[:1] != \\'_\\']\\n \"\"\"python part of the warnings subsystem.\"\"\"\\n import sys\\n __all__ = [\"warn\", \"warn_explicit\", \"showwarning\",\\n \"formatwarning\", \"filterwarnings\", \"simplefilter\",\\n \"resetwarnings\", \"catch_warnings\"]\\n def showwarning(message, category, filename, lineno, file=none, line=none):\\n \"\"\"hook to write a warning to a file; replace if you like.\"\"\"\\n if file is none:\\n file = sys.stderr\\n if file is none:\\n # sys.stderr is none when run with pythonw.exe - warnings get lost\\n return\\n try:\\n file.write(formatwarning(message, category, filename, lineno, line))\\n except oserror:\\n pass # the file (probably stderr) is invalid - this warning gets lost.\\n def formatwarning(message, category, filename, lineno, line=none):\\n \"\"\"function to format a warning the standard way.\"\"\"\\n s =  \"%s:%s: %s: %s\\\\n\" % (filename, lineno, category.__name__, message)\\n if line is none:\\n try:\\n import linecache\\n line = linecache.getline(filename, lineno)\\n except exception:\\n # when a warning is logged during python shutdown, linecache\\n # and the import machinery don\\'t work anymore\\n line = none\\n if line:\\n line = line.strip()\\n s += \"  %s\\\\n\" % line\\n return s\\n def filterwarnings(action, message=\"\", category=warning, module=\"\", lineno=0,\\n append=false):\\n \"\"\"insert an entry into the list of warnings filters (at the front).\\n \\'action\\' -- one of \"error\", \"ignore\", \"always\", \"default\", \"module\",\\n or \"once\"\\n \\'message\\' -- a regex that the warning message must match\\n \\'category\\' -- a class that the warning must be a subclass of\\n \\'module\\' -- a regex that the module name must match\\n \\'lineno\\' -- an integer line number, 0 matches all warnings\\n \\'append\\' -- if true, append to the list of filters\\n \"\"\"\\n import re\\n assert action in (\"error\", \"ignore\", \"always\", \"default\", \"module\",\\n \"once\"), \"invalid action: %r\" % (action,)\\n assert isinstance(message, str), \"message must be a string\"\\n assert isinstance(category, type), \"category must be a class\"\\n assert issubclass(category, warning), \"category must be a warning subclass\"\\n assert isinstance(module, str), \"module must be a string\"\\n assert isinstance(lineno, int) and lineno >= 0, \\\\\\n \"lineno must be an int >= 0\"\\n _add_filter(action, re.compile(message, re.i), category,\\n re.compile(module), lineno, append=append)\\n def simplefilter(action, category=warning, lineno=0, append=false):\\n \"\"\"insert a simple entry into the list of warnings filters (at the front).\\n a simple filter matches all modules and messages.\\n \\'action\\' -- one of \"error\", \"ignore\", \"always\", \"default\", \"module\",\\n or \"once\"\\n \\'category\\' -- a class that the warning must be a subclass of\\n \\'lineno\\' -- an integer line number, 0 matches all warnings\\n \\'append\\' -- if true, append to the list of filters\\n \"\"\"\\n assert action in (\"error\", \"ignore\", \"always\", \"default\", \"module\",\\n \"once\"), \"invalid action: %r\" % (action,)\\n assert isinstance(lineno, int) and lineno >= 0, \\\\\\n \"lineno must be an int >= 0\"\\n _add_filter(action, none, category, none, lineno, append=append)\\n def _add_filter(*item, append):\\n # remove possible duplicate filters, so new one will be placed\\n # in correct place. if append=true and duplicate exists, do nothing.\\n if not append:\\n try:\\n filters.remove(item)\\n except valueerror:\\n pass\\n filters.insert(0, item)\\n else:\\n if item not in filters:\\n filters.append(item)\\n _filters_mutated()\\n def resetwarnings():\\n \"\"\"clear the list of warning filters, so that no filters are active.\"\"\"\\n filters[:] = []\\n _filters_mutated()\\n class _optionerror(exception):\\n \"\"\"exception used by option processing helpers.\"\"\"\\n pass\\n # helper to process -w options passed via sys.warnoptions\\n def _processoptions(args):\\n for arg in args:\\n try:\\n _setoption(arg)\\n except _optionerror as msg:\\n print(\"invalid -w option ignored:\", msg, file=sys.stderr)\\n # helper for _processoptions()\\n def _setoption(arg):\\n import re\\n parts = arg.split(\\':\\')\\n if len(parts) > 5:\\n raise _optionerror(\"too many fields (max 5): %r\" % (arg,))\\n while len(parts) < 5:\\n parts.append(\\'\\')\\n action, message, category, module, lineno = [s.strip()\\n for s in parts]\\n action = _getaction(action)\\n message = re.escape(message)\\n category = _getcategory(category)\\n module = re.escape(module)\\n if module:\\n module = module + \\'$\\'\\n if lineno:\\n try:\\n lineno = int(lineno)\\n if lineno < 0:\\n raise valueerror\\n except (valueerror, overflowerror):\\n raise _optionerror(\"invalid lineno %r\" % (lineno,))\\n else:\\n lineno = 0\\n filterwarnings(action, message, category, module, lineno)\\n # helper for _setoption()\\n def _getaction(action):\\n if not action:\\n return \"default\"\\n if action == \"all\": return \"always\" # alias\\n for a in (\\'default\\', \\'always\\', \\'ignore\\', \\'module\\', \\'once\\', \\'error\\'):\\n if a.startswith(action):\\n return a\\n raise _optionerror(\"invalid action: %r\" % (action,))\\n # helper for _setoption()\\n def _getcategory(category):\\n import re\\n if not category:\\n return warning\\n if re.match(\"^[a-za-z0-9_]+$\", category):\\n try:\\n cat = eval(category)\\n except nameerror:\\n raise _optionerror(\"unknown warning category: %r\" % (category,))\\n else:\\n i = category.rfind(\".\")\\n module = category[:i]\\n klass = category[i+1:]\\n try:\\n m = __import__(module, none, none, [klass])\\n except importerror:\\n raise _optionerror(\"invalid module name: %r\" % (module,))\\n try:\\n cat = getattr(m, klass)\\n except attributeerror:\\n raise _optionerror(\"unknown warning category: %r\" % (category,))\\n if not issubclass(cat, warning):\\n raise _optionerror(\"invalid warning category: %r\" % (category,))\\n return cat\\n def _is_internal_frame(frame):\\n \"\"\"signal whether the frame is an internal cpython implementation detail.\"\"\"\\n filename = frame.f_code.co_filename\\n return \\'importlib\\' in filename and \\'_bootstrap\\' in filename\\n def _next_external_frame(frame):\\n \"\"\"find the next frame that doesn\\'t involve cpython internals.\"\"\"\\n frame = frame.f_back\\n while frame is not none and _is_internal_frame(frame):\\n frame = frame.f_back\\n return frame\\n # code typically replaced by _warnings\\n def warn(message, category=none, stacklevel=1):\\n \"\"\"issue a warning, or maybe ignore it or raise an exception.\"\"\"\\n # check if message is already a warning object\\n if isinstance(message, warning):\\n category = message.__class__\\n # check category argument\\n if category is none:\\n category = userwarning\\n if not (isinstance(category, type) and issubclass(category, warning)):\\n raise typeerror(\"category must be a warning subclass, \"\\n \"not \\'{:s}\\'\".format(type(category).__name__))\\n # get context information\\n try:\\n if stacklevel <= 1 or _is_internal_frame(sys._getframe(1)):\\n # if frame is too small to care or if the warning originated in\\n # internal code, then do not try to hide any frames.\\n frame = sys._getframe(stacklevel)\\n else:\\n frame = sys._getframe(1)\\n # look for one frame less since the above line starts us off.\\n for x in range(stacklevel-1):\\n frame = _next_external_frame(frame)\\n if frame is none:\\n raise valueerror\\n except valueerror:\\n globals = sys.__dict__\\n lineno = 1\\n else:\\n globals = frame.f_globals\\n lineno = frame.f_lineno\\n if \\'__name__\\' in globals:\\n module = globals[\\'__name__\\']\\n else:\\n module = \"<string>\"\\n filename = globals.get(\\'__file__\\')\\n if filename:\\n fnl = filename.lower()\\n if fnl.endswith(\".pyc\"):\\n filename = filename[:-1]\\n else:\\n if module == \"__main__\":\\n try:\\n filename = sys.argv[0]\\n except attributeerror:\\n # embedded interpreters don\\'t have sys.argv, see bug #839151\\n filename = \\'__main__\\'\\n if not filename:\\n filename = module\\n registry = globals.setdefault(\"__warningregistry__\", {})\\n warn_explicit(message, category, filename, lineno, module, registry,\\n globals)\\n def warn_explicit(message, category, filename, lineno,\\n module=none, registry=none, module_globals=none):\\n lineno = int(lineno)\\n if module is none:\\n module = filename or \"<unknown>\"\\n if module[-3:].lower() == \".py\":\\n module = module[:-3] # xxx what about leading pathname?\\n if registry is none:\\n registry = {}\\n if registry.get(\\'version\\', 0) != _filters_version:\\n registry.clear()\\n registry[\\'version\\'] = _filters_version\\n if isinstance(message, warning):\\n text = str(message)\\n category = message.__class__\\n else:\\n text = message\\n message = category(message)\\n key = (text, category, lineno)\\n # quick test for common case\\n if registry.get(key):\\n return\\n # search the filters\\n for item in filters:\\n action, msg, cat, mod, ln = item\\n if ((msg is none or msg.match(text)) and\\n issubclass(category, cat) and\\n (mod is none or mod.match(module)) and\\n (ln == 0 or lineno == ln)):\\n break\\n else:\\n action = defaultaction\\n # early exit actions\\n if action == \"ignore\":\\n registry[key] = 1\\n return\\n # prime the linecache for formatting, in case the\\n # \"file\" is actually in a zipfile or something.\\n import linecache\\n linecache.getlines(filename, module_globals)\\n if action == \"error\":\\n raise message\\n # other actions\\n if action == \"once\":\\n registry[key] = 1\\n oncekey = (text, category)\\n if onceregistry.get(oncekey):\\n return\\n onceregistry[oncekey] = 1\\n elif action == \"always\":\\n pass\\n elif action == \"module\":\\n registry[key] = 1\\n altkey = (text, category, 0)\\n if registry.get(altkey):\\n return\\n registry[altkey] = 1\\n elif action == \"default\":\\n registry[key] = 1\\n else:\\n # unrecognized actions are errors\\n raise runtimeerror(\\n \"unrecognized action (%r) in warnings.filters:\\\\n %s\" %\\n (action, item))\\n if not callable(showwarning):\\n raise typeerror(\"warnings.showwarning() must be set to a \"\\n \"function or method\")\\n # print message and context\\n showwarning(message, category, filename, lineno)\\n class warningmessage(object):\\n \"\"\"holds the result of a single showwarning() call.\"\"\"\\n _warning_details = (\"message\", \"category\", \"filename\", \"lineno\", \"file\",\\n \"line\")\\n def __init__(self, message, category, filename, lineno, file=none,\\n line=none):\\n local_values = locals()\\n for attr in self._warning_details:\\n setattr(self, attr, local_values[attr])\\n self._category_name = category.__name__ if category else none\\n def __str__(self):\\n return (\"{message : %r, category : %r, filename : %r, lineno : %s, \"\\n \"line : %r}\" % (self.message, self._category_name,\\n self.filename, self.lineno, self.line))\\n class catch_warnings(object):\\n \"\"\"a context manager that copies and restores the warnings filter upon\\n exiting the context.\\n the \\'record\\' argument specifies whether warnings should be captured by a\\n custom implementation of warnings.showwarning() and be appended to a list\\n returned by the context manager. otherwise none is returned by the context\\n manager. the objects appended to the list are arguments whose attributes\\n mirror the arguments to showwarning().\\n the \\'module\\' argument is to specify an alternative module to the module\\n named \\'warnings\\' and imported under that name. this argument is only useful\\n when testing the warnings module itself.\\n \"\"\"\\n def __init__(self, *, record=false, module=none):\\n \"\"\"specify whether to record warnings and if an alternative module\\n should be used other than sys.modules[\\'warnings\\'].\\n for compatibility with python 3.0, please consider all arguments to be\\n keyword-only.\\n \"\"\"\\n self._record = record\\n self._module = sys.modules[\\'warnings\\'] if module is none else module\\n self._entered = false\\n def __repr__(self):\\n args = []\\n if self._record:\\n args.append(\"record=true\")\\n if self._module is not sys.modules[\\'warnings\\']:\\n args.append(\"module=%r\" % self._module)\\n name = type(self).__name__\\n return \"%s(%s)\" % (name, \", \".join(args))\\n def __enter__(self):\\n if self._entered:\\n raise runtimeerror(\"cannot enter %r twice\" % self)\\n self._entered = true\\n self._filters = self._module.filters\\n self._module.filters = self._filters[:]\\n self._module._filters_mutated()\\n self._showwarning = self._module.showwarning\\n if self._record:\\n log = []\\n def showwarning(*args, **kwargs):\\n log.append(warningmessage(*args, **kwargs))\\n self._module.showwarning = showwarning\\n return log\\n else:\\n return none\\n def __exit__(self, *exc_info):\\n if not self._entered:\\n raise runtimeerror(\"cannot exit %r without entering first\" % self)\\n self._module.filters = self._filters\\n self._module._filters_mutated()\\n self._module.showwarning = self._showwarning\\n # filters contains a sequence of filter 5-tuples\\n # the components of the 5-tuple are:\\n # - an action: error, ignore, always, default, module, or once\\n # - a compiled regex that must match the warning message\\n # - a class representing the warning category\\n # - a compiled regex that must match the module that is being warned\\n # - a line number for the line being warning, or 0 to mean any line\\n # if either if the compiled regexs are none, match anything.\\n _warnings_defaults = false\\n try:\\n from _warnings import (filters, _defaultaction, _onceregistry,\\n warn, warn_explicit, _filters_mutated)\\n defaultaction = _defaultaction\\n onceregistry = _onceregistry\\n _warnings_defaults = true\\n except importerror:\\n filters = []\\n defaultaction = \"default\"\\n onceregistry = {}\\n _filters_version = 1\\n def _filters_mutated():\\n global _filters_version\\n _filters_version += 1\\n # module initialization\\n _processoptions(sys.warnoptions)\\n if not _warnings_defaults:\\n silence = [importwarning, pendingdeprecationwarning]\\n silence.append(deprecationwarning)\\n for cls in silence:\\n simplefilter(\"ignore\", category=cls)\\n bytes_warning = sys.flags.bytes_warning\\n if bytes_warning > 1:\\n bytes_action = \"error\"\\n elif bytes_warning:\\n bytes_action = \"default\"\\n else:\\n bytes_action = \"ignore\"\\n simplefilter(bytes_action, category=byteswarning, append=1)\\n # resource usage warnings are enabled by default in pydebug mode\\n if hasattr(sys, \\'gettotalrefcount\\'):\\n resource_action = \"always\"\\n else:\\n resource_action = \"ignore\"\\n simplefilter(resource_action, category=resourcewarning, append=1)\\n del _warnings_defaults\\n \"\"\"weak reference support for python.\\n this module is an implementation of pep 205:\\n http://www.python.org/dev/peps/pep-0205/\\n \"\"\"\\n # naming convention: variables named \"wr\" are weak reference objects;\\n # they are called this instead of \"ref\" to avoid name collisions with\\n # the module-global ref() function imported from _weakref.\\n from _weakref import (\\n getweakrefcount,\\n getweakrefs,\\n ref,\\n proxy,\\n callableproxytype,\\n proxytype,\\n referencetype)\\n from _weakrefset import weakset, _iterationguard\\n import collections  # import after _weakref to avoid circular import.\\n import sys\\n import itertools\\n proxytypes = (proxytype, callableproxytype)\\n __all__ = [\"ref\", \"proxy\", \"getweakrefcount\", \"getweakrefs\",\\n \"weakkeydictionary\", \"referencetype\", \"proxytype\",\\n \"callableproxytype\", \"proxytypes\", \"weakvaluedictionary\",\\n \"weakset\", \"weakmethod\", \"finalize\"]\\n class weakmethod(ref):\\n \"\"\"\\n a custom `weakref.ref` subclass which simulates a weak reference to\\n a bound method, working around the lifetime problem of bound methods.\\n \"\"\"\\n __slots__ = \"_func_ref\", \"_meth_type\", \"_alive\", \"__weakref__\"\\n def __new__(cls, meth, callback=none):\\n try:\\n obj = meth.__self__\\n func = meth.__func__\\n except attributeerror:\\n raise typeerror(\"argument should be a bound method, not {}\"\\n .format(type(meth))) from none\\n def _cb(arg):\\n # the self-weakref trick is needed to avoid creating a reference\\n # cycle.\\n self = self_wr()\\n if self._alive:\\n self._alive = false\\n if callback is not none:\\n callback(self)\\n self = ref.__new__(cls, obj, _cb)\\n self._func_ref = ref(func, _cb)\\n self._meth_type = type(meth)\\n self._alive = true\\n self_wr = ref(self)\\n return self\\n def __call__(self):\\n obj = super().__call__()\\n func = self._func_ref()\\n if obj is none or func is none:\\n return none\\n return self._meth_type(func, obj)\\n def __eq__(self, other):\\n if isinstance(other, weakmethod):\\n if not self._alive or not other._alive:\\n return self is other\\n return ref.__eq__(self, other) and self._func_ref == other._func_ref\\n return false\\n def __ne__(self, other):\\n if isinstance(other, weakmethod):\\n if not self._alive or not other._alive:\\n return self is not other\\n return ref.__ne__(self, other) or self._func_ref != other._func_ref\\n return true\\n __hash__ = ref.__hash__\\n class weakvaluedictionary(collections.mutablemapping):\\n \"\"\"mapping class that references values weakly.\\n entries in the dictionary will be discarded when no strong\\n reference to the value exists anymore\\n \"\"\"\\n # we inherit the constructor without worrying about the input\\n # dictionary; since it uses our .update() method, we get the right\\n # checks (if the other dictionary is a weakvaluedictionary,\\n # objects are unwrapped on the way out, and we always wrap on the\\n # way in).\\n def __init__(*args, **kw):\\n if not args:\\n raise typeerror(\"descriptor \\'__init__\\' of \\'weakvaluedictionary\\' \"\\n \"object needs an argument\")\\n self, *args = args\\n if len(args) > 1:\\n raise typeerror(\\'expected at most 1 arguments, got %d\\' % len(args))\\n def remove(wr, selfref=ref(self)):\\n self = selfref()\\n if self is not none:\\n if self._iterating:\\n self._pending_removals.append(wr.key)\\n else:\\n del self.data[wr.key]\\n self._remove = remove\\n # a list of keys to be removed\\n self._pending_removals = []\\n self._iterating = set()\\n self.data = d = {}\\n self.update(*args, **kw)\\n def _commit_removals(self):\\n l = self._pending_removals\\n d = self.data\\n # we shouldn\\'t encounter any keyerror, because this method should\\n # always be called *before* mutating the dict.\\n while l:\\n del d[l.pop()]\\n def __getitem__(self, key):\\n o = self.data[key]()\\n if o is none:\\n raise keyerror(key)\\n else:\\n return o\\n def __delitem__(self, key):\\n if self._pending_removals:\\n self._commit_removals()\\n del self.data[key]\\n def __len__(self):\\n return len(self.data) - len(self._pending_removals)\\n def __contains__(self, key):\\n try:\\n o = self.data[key]()\\n except keyerror:\\n return false\\n return o is not none\\n def __repr__(self):\\n return \"<%s at %#x>\" % (self.__class__.__name__, id(self))\\n def __setitem__(self, key, value):\\n if self._pending_removals:\\n self._commit_removals()\\n self.data[key] = keyedref(value, self._remove, key)\\n def copy(self):\\n new = weakvaluedictionary()\\n for key, wr in self.data.items():\\n o = wr()\\n if o is not none:\\n new[key] = o\\n return new\\n __copy__ = copy\\n def __deepcopy__(self, memo):\\n from copy import deepcopy\\n new = self.__class__()\\n for key, wr in self.data.items():\\n o = wr()\\n if o is not none:\\n new[deepcopy(key, memo)] = o\\n return new\\n def get(self, key, default=none):\\n try:\\n wr = self.data[key]\\n except keyerror:\\n return default\\n else:\\n o = wr()\\n if o is none:\\n # this should only happen\\n return default\\n else:\\n return o\\n def items(self):\\n with _iterationguard(self):\\n for k, wr in self.data.items():\\n v = wr()\\n if v is not none:\\n yield k, v\\n def keys(self):\\n with _iterationguard(self):\\n for k, wr in self.data.items():\\n if wr() is not none:\\n yield k\\n __iter__ = keys\\n def itervaluerefs(self):\\n \"\"\"return an iterator that yields the weak references to the values.\\n the references are not guaranteed to be \\'live\\' at the time\\n they are used, so the result of calling the references needs\\n to be checked before being used.  this can be used to avoid\\n creating references that will cause the garbage collector to\\n keep the values around longer than needed.\\n \"\"\"\\n with _iterationguard(self):\\n yield from self.data.values()\\n def values(self):\\n with _iterationguard(self):\\n for wr in self.data.values():\\n obj = wr()\\n if obj is not none:\\n yield obj\\n def popitem(self):\\n if self._pending_removals:\\n self._commit_removals()\\n while true:\\n key, wr = self.data.popitem()\\n o = wr()\\n if o is not none:\\n return key, o\\n def pop(self, key, *args):\\n if self._pending_removals:\\n self._commit_removals()\\n try:\\n o = self.data.pop(key)()\\n except keyerror:\\n if args:\\n return args[0]\\n raise\\n if o is none:\\n raise keyerror(key)\\n else:\\n return o\\n def setdefault(self, key, default=none):\\n try:\\n wr = self.data[key]\\n except keyerror:\\n if self._pending_removals:\\n self._commit_removals()\\n self.data[key] = keyedref(default, self._remove, key)\\n return default\\n else:\\n return wr()\\n def update(*args, **kwargs):\\n if not args:\\n raise typeerror(\"descriptor \\'update\\' of \\'weakvaluedictionary\\' \"\\n \"object needs an argument\")\\n self, *args = args\\n if len(args) > 1:\\n raise typeerror(\\'expected at most 1 arguments, got %d\\' % len(args))\\n dict = args[0] if args else none\\n if self._pending_removals:\\n self._commit_removals()\\n d = self.data\\n if dict is not none:\\n if not hasattr(dict, \"items\"):\\n dict = type({})(dict)\\n for key, o in dict.items():\\n d[key] = keyedref(o, self._remove, key)\\n if len(kwargs):\\n self.update(kwargs)\\n def valuerefs(self):\\n \"\"\"return a list of weak references to the values.\\n the references are not guaranteed to be \\'live\\' at the time\\n they are used, so the result of calling the references needs\\n to be checked before being used.  this can be used to avoid\\n creating references that will cause the garbage collector to\\n keep the values around longer than needed.\\n \"\"\"\\n return list(self.data.values())\\n class keyedref(ref):\\n \"\"\"specialized reference that includes a key corresponding to the value.\\n this is used in the weakvaluedictionary to avoid having to create\\n a function object for each key stored in the mapping.  a shared\\n callback object can use the \\'key\\' attribute of a keyedref instead\\n of getting a reference to the key from an enclosing scope.\\n \"\"\"\\n __slots__ = \"key\",\\n def __new__(type, ob, callback, key):\\n self = ref.__new__(type, ob, callback)\\n self.key = key\\n return self\\n def __init__(self, ob, callback, key):\\n super().__init__(ob, callback)\\n class weakkeydictionary(collections.mutablemapping):\\n \"\"\" mapping class that references keys weakly.\\n entries in the dictionary will be discarded when there is no\\n longer a strong reference to the key. this can be used to\\n associate additional data with an object owned by other parts of\\n an application without adding attributes to those objects. this\\n can be especially useful with objects that override attribute\\n accesses.\\n \"\"\"\\n def __init__(self, dict=none):\\n self.data = {}\\n def remove(k, selfref=ref(self)):\\n self = selfref()\\n if self is not none:\\n if self._iterating:\\n self._pending_removals.append(k)\\n else:\\n del self.data[k]\\n self._remove = remove\\n # a list of dead weakrefs (keys to be removed)\\n self._pending_removals = []\\n self._iterating = set()\\n self._dirty_len = false\\n if dict is not none:\\n self.update(dict)\\n def _commit_removals(self):\\n # note: we don\\'t need to call this method before mutating the dict,\\n # because a dead weakref never compares equal to a live weakref,\\n # even if they happened to refer to equal objects.\\n # however, it means keys may already have been removed.\\n l = self._pending_removals\\n d = self.data\\n while l:\\n try:\\n del d[l.pop()]\\n except keyerror:\\n pass\\n def _scrub_removals(self):\\n d = self.data\\n self._pending_removals = [k for k in self._pending_removals if k in d]\\n self._dirty_len = false\\n def __delitem__(self, key):\\n self._dirty_len = true\\n del self.data[ref(key)]\\n def __getitem__(self, key):\\n return self.data[ref(key)]\\n def __len__(self):\\n if self._dirty_len and self._pending_removals:\\n # self._pending_removals may still contain keys which were\\n # explicitly removed, we have to scrub them (see issue #21173).\\n self._scrub_removals()\\n return len(self.data) - len(self._pending_removals)\\n def __repr__(self):\\n return \"<%s at %#x>\" % (self.__class__.__name__, id(self))\\n def __setitem__(self, key, value):\\n self.data[ref(key, self._remove)] = value\\n def copy(self):\\n new = weakkeydictionary()\\n for key, value in self.data.items():\\n o = key()\\n if o is not none:\\n new[o] = value\\n return new\\n __copy__ = copy\\n def __deepcopy__(self, memo):\\n from copy import deepcopy\\n new = self.__class__()\\n for key, value in self.data.items():\\n o = key()\\n if o is not none:\\n new[o] = deepcopy(value, memo)\\n return new\\n def get(self, key, default=none):\\n return self.data.get(ref(key),default)\\n def __contains__(self, key):\\n try:\\n wr = ref(key)\\n except typeerror:\\n return false\\n return wr in self.data\\n def items(self):\\n with _iterationguard(self):\\n for wr, value in self.data.items():\\n key = wr()\\n if key is not none:\\n yield key, value\\n def keys(self):\\n with _iterationguard(self):\\n for wr in self.data:\\n obj = wr()\\n if obj is not none:\\n yield obj\\n __iter__ = keys\\n def values(self):\\n with _iterationguard(self):\\n for wr, value in self.data.items():\\n if wr() is not none:\\n yield value\\n def keyrefs(self):\\n \"\"\"return a list of weak references to the keys.\\n the references are not guaranteed to be \\'live\\' at the time\\n they are used, so the result of calling the references needs\\n to be checked before being used.  this can be used to avoid\\n creating references that will cause the garbage collector to\\n keep the keys around longer than needed.\\n \"\"\"\\n return list(self.data)\\n def popitem(self):\\n self._dirty_len = true\\n while true:\\n key, value = self.data.popitem()\\n o = key()\\n if o is not none:\\n return o, value\\n def pop(self, key, *args):\\n self._dirty_len = true\\n return self.data.pop(ref(key), *args)\\n def setdefault(self, key, default=none):\\n return self.data.setdefault(ref(key, self._remove),default)\\n def update(self, dict=none, **kwargs):\\n d = self.data\\n if dict is not none:\\n if not hasattr(dict, \"items\"):\\n dict = type({})(dict)\\n for key, value in dict.items():\\n d[ref(key, self._remove)] = value\\n if len(kwargs):\\n self.update(kwargs)\\n class finalize:\\n \"\"\"class for finalization of weakrefable objects\\n finalize(obj, func, *args, **kwargs) returns a callable finalizer\\n object which will be called when obj is garbage collected. the\\n first time the finalizer is called it evaluates func(*arg, **kwargs)\\n and returns the result. after this the finalizer is dead, and\\n calling it just returns none.\\n when the program exits any remaining finalizers for which the\\n atexit attribute is true will be run in reverse order of creation.\\n by default atexit is true.\\n \"\"\"\\n # finalizer objects don\\'t have any state of their own.  they are\\n # just used as keys to lookup _info objects in the registry.  this\\n # ensures that they cannot be part of a ref-cycle.\\n __slots__ = ()\\n _registry = {}\\n _shutdown = false\\n _index_iter = itertools.count()\\n _dirty = false\\n _registered_with_atexit = false\\n class _info:\\n __slots__ = (\"weakref\", \"func\", \"args\", \"kwargs\", \"atexit\", \"index\")\\n def __init__(self, obj, func, *args, **kwargs):\\n if not self._registered_with_atexit:\\n # we may register the exit function more than once because\\n # of a thread race, but that is harmless\\n import atexit\\n atexit.register(self._exitfunc)\\n finalize._registered_with_atexit = true\\n info = self._info()\\n info.weakref = ref(obj, self)\\n info.func = func\\n info.args = args\\n info.kwargs = kwargs or none\\n info.atexit = true\\n info.index = next(self._index_iter)\\n self._registry[self] = info\\n finalize._dirty = true\\n def __call__(self, _=none):\\n \"\"\"if alive then mark as dead and return func(*args, **kwargs);\\n otherwise return none\"\"\"\\n info = self._registry.pop(self, none)\\n if info and not self._shutdown:\\n return info.func(*info.args, **(info.kwargs or {}))\\n def detach(self):\\n \"\"\"if alive then mark as dead and return (obj, func, args, kwargs);\\n otherwise return none\"\"\"\\n info = self._registry.get(self)\\n obj = info and info.weakref()\\n if obj is not none and self._registry.pop(self, none):\\n return (obj, info.func, info.args, info.kwargs or {})\\n def peek(self):\\n \"\"\"if alive then return (obj, func, args, kwargs);\\n otherwise return none\"\"\"\\n info = self._registry.get(self)\\n obj = info and info.weakref()\\n if obj is not none:\\n return (obj, info.func, info.args, info.kwargs or {})\\n @property\\n def alive(self):\\n \"\"\"whether finalizer is alive\"\"\"\\n return self in self._registry\\n @property\\n def atexit(self):\\n \"\"\"whether finalizer should be called at exit\"\"\"\\n info = self._registry.get(self)\\n return bool(info) and info.atexit\\n @atexit.setter\\n def atexit(self, value):\\n info = self._registry.get(self)\\n if info:\\n info.atexit = bool(value)\\n def __repr__(self):\\n info = self._registry.get(self)\\n obj = info and info.weakref()\\n if obj is none:\\n return \\'<%s object at %#x; dead>\\' % (type(self).__name__, id(self))\\n else:\\n return \\'<%s object at %#x; for %r at %#x>\\' % \\\\\\n (type(self).__name__, id(self), type(obj).__name__, id(obj))\\n @classmethod\\n def _select_for_exit(cls):\\n # return live finalizers marked for exit, oldest first\\n l = [(f,i) for (f,i) in cls._registry.items() if i.atexit]\\n l.sort(key=lambda item:item[1].index)\\n return [f for (f,i) in l]\\n @classmethod\\n def _exitfunc(cls):\\n # at shutdown invoke finalizers for which atexit is true.\\n # this is called once all other non-daemonic threads have been\\n # joined.\\n reenable_gc = false\\n try:\\n if cls._registry:\\n import gc\\n if gc.isenabled():\\n reenable_gc = true\\n gc.disable()\\n pending = none\\n while true:\\n if pending is none or finalize._dirty:\\n pending = cls._select_for_exit()\\n finalize._dirty = false\\n if not pending:\\n break\\n f = pending.pop()\\n try:\\n # gc is disabled, so (assuming no daemonic\\n # threads) the following is the only line in\\n # this function which might trigger creation\\n # of a new finalizer\\n f()\\n except exception:\\n sys.excepthook(*sys.exc_info())\\n assert f not in cls._registry\\n finally:\\n # prevent any more finalizers from executing during shutdown\\n finalize._shutdown = true\\n if reenable_gc:\\n gc.enable()\\n # access weakset through the weakref module.\\n # this code is separated-out because it is needed\\n # by abc.py to load everything else at startup.\\n from _weakref import ref\\n __all__ = [\\'weakset\\']\\n class _iterationguard:\\n # this context manager registers itself in the current iterators of the\\n # weak container, such as to delay all removals until the context manager\\n # exits.\\n # this technique should be relatively thread-safe (since sets are).\\n def __init__(self, weakcontainer):\\n # don\\'t create cycles\\n self.weakcontainer = ref(weakcontainer)\\n def __enter__(self):\\n w = self.weakcontainer()\\n if w is not none:\\n w._iterating.add(self)\\n return self\\n def __exit__(self, e, t, b):\\n w = self.weakcontainer()\\n if w is not none:\\n s = w._iterating\\n s.remove(self)\\n if not s:\\n w._commit_removals()\\n class weakset:\\n def __init__(self, data=none):\\n self.data = set()\\n def _remove(item, selfref=ref(self)):\\n self = selfref()\\n if self is not none:\\n if self._iterating:\\n self._pending_removals.append(item)\\n else:\\n self.data.discard(item)\\n self._remove = _remove\\n # a list of keys to be removed\\n self._pending_removals = []\\n self._iterating = set()\\n if data is not none:\\n self.update(data)\\n def _commit_removals(self):\\n l = self._pending_removals\\n discard = self.data.discard\\n while l:\\n discard(l.pop())\\n def __iter__(self):\\n with _iterationguard(self):\\n for itemref in self.data:\\n item = itemref()\\n if item is not none:\\n # caveat: the iterator will keep a strong reference to\\n # `item` until it is resumed or closed.\\n yield item\\n def __len__(self):\\n return len(self.data) - len(self._pending_removals)\\n def __contains__(self, item):\\n try:\\n wr = ref(item)\\n except typeerror:\\n return false\\n return wr in self.data\\n def __reduce__(self):\\n return (self.__class__, (list(self),),\\n getattr(self, \\'__dict__\\', none))\\n def add(self, item):\\n if self._pending_removals:\\n self._commit_removals()\\n self.data.add(ref(item, self._remove))\\n def clear(self):\\n if self._pending_removals:\\n self._commit_removals()\\n self.data.clear()\\n def copy(self):\\n return self.__class__(self)\\n def pop(self):\\n if self._pending_removals:\\n self._commit_removals()\\n while true:\\n try:\\n itemref = self.data.pop()\\n except keyerror:\\n raise keyerror(\\'pop from empty weakset\\')\\n item = itemref()\\n if item is not none:\\n return item\\n def remove(self, item):\\n if self._pending_removals:\\n self._commit_removals()\\n self.data.remove(ref(item))\\n def discard(self, item):\\n if self._pending_removals:\\n self._commit_removals()\\n self.data.discard(ref(item))\\n def update(self, other):\\n if self._pending_removals:\\n self._commit_removals()\\n for element in other:\\n self.add(element)\\n def __ior__(self, other):\\n self.update(other)\\n return self\\n def difference(self, other):\\n newset = self.copy()\\n newset.difference_update(other)\\n return newset\\n __sub__ = difference\\n def difference_update(self, other):\\n self.__isub__(other)\\n def __isub__(self, other):\\n if self._pending_removals:\\n self._commit_removals()\\n if self is other:\\n self.data.clear()\\n else:\\n self.data.difference_update(ref(item) for item in other)\\n return self\\n def intersection(self, other):\\n return self.__class__(item for item in other if item in self)\\n __and__ = intersection\\n def intersection_update(self, other):\\n self.__iand__(other)\\n def __iand__(self, other):\\n if self._pending_removals:\\n self._commit_removals()\\n self.data.intersection_update(ref(item) for item in other)\\n return self\\n def issubset(self, other):\\n return self.data.issubset(ref(item) for item in other)\\n __le__ = issubset\\n def __lt__(self, other):\\n return self.data < set(ref(item) for item in other)\\n def issuperset(self, other):\\n return self.data.issuperset(ref(item) for item in other)\\n __ge__ = issuperset\\n def __gt__(self, other):\\n return self.data > set(ref(item) for item in other)\\n def __eq__(self, other):\\n if not isinstance(other, self.__class__):\\n return notimplemented\\n return self.data == set(ref(item) for item in other)\\n def symmetric_difference(self, other):\\n newset = self.copy()\\n newset.symmetric_difference_update(other)\\n return newset\\n __xor__ = symmetric_difference\\n def symmetric_difference_update(self, other):\\n self.__ixor__(other)\\n def __ixor__(self, other):\\n if self._pending_removals:\\n self._commit_removals()\\n if self is other:\\n self.data.clear()\\n else:\\n self.data.symmetric_difference_update(ref(item, self._remove) for item in other)\\n return self\\n def union(self, other):\\n return self.__class__(e for s in (self, other) for e in s)\\n __or__ = union\\n def isdisjoint(self, other):\\n return len(self.intersection(other)) == 0\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read input text and cleanup content\n",
    "fin = open(\"../Deep learning with Keras/data/code.txt\", \"rb\")\n",
    "lines = []\n",
    "for line in fin:\n",
    "    line = line.strip().lower()\n",
    "    line = line.decode(\"ascii\",\"ignore\")\n",
    "    if len(line) == 0:\n",
    "        continue\n",
    "    line += '\\n'\n",
    "    lines.append(line)\n",
    "fin.close()\n",
    "text = \" \".join(lines)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n",
      "{':', '2', '1', ')', '*', '?', 't', 'j', '&', 'n', '$', ']', 'b', 'g', '>', 'o', 'k', '_', ',', '<', 'v', '4', '.', '^', '~', 'l', ';', 'm', '@', '9', 'q', '`', '|', '[', '\\\\', 'f', \"'\", '0', '!', '6', '{', 'e', '#', '/', '(', ' ', 'r', '3', 'x', 'i', 'u', '}', '8', '\\n', '-', 'z', 'd', '%', 'p', 'c', '7', 'w', 'h', '=', '+', 'y', '5', 'a', '\"', 's'}\n"
     ]
    }
   ],
   "source": [
    "# build vocabulary\n",
    "chars = set([c for c in text])\n",
    "nb_chars = len(chars)\n",
    "print(nb_chars)\n",
    "print(chars)\n",
    "char2index = dict((c, i) for i, c in enumerate(chars))\n",
    "index2char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the input and label texts  \n",
    "SEQLEN = 10\n",
    "STEP = 1\n",
    "\n",
    "input_chars = []\n",
    "label_chars = []\n",
    "for i in range(0, len(text) - SEQLEN, STEP):\n",
    "    input_chars.append(text[i:i + SEQLEN])\n",
    "    label_chars.append(text[i + SEQLEN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize these input and label texts\n",
    "X = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=np.bool)\n",
    "y = np.zeros((len(input_chars), nb_chars), dtype=np.bool)\n",
    "for i, input_char in enumerate(input_chars):\n",
    "    for j, ch in enumerate(input_char):\n",
    "        X[i, j, char2index[ch]] = 1\n",
    "    y[i, char2index[label_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 128)               101888    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 70)                9030      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 70)                0         \n",
      "=================================================================\n",
      "Total params: 110,918\n",
      "Trainable params: 110,918\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build our model\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 128\n",
    "NUM_ITERATIONS = 25\n",
    "NUM_EPOCHS_PER_ITERATION = 1\n",
    "NUM_PREDS_PER_EPOCH = 1000\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(HIDDEN_SIZE, return_sequences=False, input_shape=(SEQLEN, nb_chars), unroll=True))\n",
    "model.add(Dense(nb_chars))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.summary()\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==================================================\n",
      "Iteration #: 0\n",
      "Epoch 1/1\n",
      "637891/637891 [==============================] - 262s 410us/step - loss: 2.3346\n",
      "Generating from seed: an either \n",
      "an either in the arching the coreror and in the arching the coreror and in the arching the coreror and in the arching the coreror and in the arching the coreror and in the arching the coreror and in the arching the coreror and in the arching the coreror and in the arching the coreror and in the arching the coreror and in the arching the coreror and in the arching the coreror and in the arching the coreror and in the arching the coreror and in the arching the coreror and in the arching the coreror and in the arching the coreror and in the arching the coreror and in the arching the coreror and in the arching the coreror and in the arching the coreror and in the arching the coreror and in the arching the coreror and in the arching the coreror and in the arching the coreror and in the arching the coreror and in the arching the coreror and in the arching the coreror and in the arching the coreror and in the arching the coreror and in the arching the coreror and in the arching the coreror and in the a\n",
      "\n",
      "==================================================\n",
      "Iteration #: 1\n",
      "Epoch 1/1\n",
      "637891/637891 [==============================] - 241s 377us/step - loss: 1.7994\n",
      "Generating from seed: fer = []\n",
      " \n",
      "fer = []\n",
      " def __init__(self, self.__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile__self.__compile\n",
      "\n",
      "==================================================\n",
      "Iteration #: 2\n",
      "Epoch 1/1\n",
      "637891/637891 [==============================] - 268s 420us/step - loss: 1.6055\n",
      "Generating from seed: array('%s'\n",
      "array('%s' % (self, other):\n",
      " return self.__class__ = (\"source_and(self, other):\n",
      " return self.__class__ = (\"source_and(self, other):\n",
      " return self.__class__ = (\"source_and(self, other):\n",
      " return self.__class__ = (\"source_and(self, other):\n",
      " return self.__class__ = (\"source_and(self, other):\n",
      " return self.__class__ = (\"source_and(self, other):\n",
      " return self.__class__ = (\"source_and(self, other):\n",
      " return self.__class__ = (\"source_and(self, other):\n",
      " return self.__class__ = (\"source_and(self, other):\n",
      " return self.__class__ = (\"source_and(self, other):\n",
      " return self.__class__ = (\"source_and(self, other):\n",
      " return self.__class__ = (\"source_and(self, other):\n",
      " return self.__class__ = (\"source_and(self, other):\n",
      " return self.__class__ = (\"source_and(self, other):\n",
      " return self.__class__ = (\"source_and(self, other):\n",
      " return self.__class__ = (\"source_and(self, other):\n",
      " return self.__class__ = (\"source_and(self, other):\n",
      " return self.__class__ = (\"source_and(self, other):\n",
      " return self.__class__ = (\"source_and(self, ot\n",
      "\n",
      "==================================================\n",
      "Iteration #: 3\n",
      "Epoch 1/1\n",
      "637891/637891 [==============================] - 241s 378us/step - loss: 1.5003\n",
      "Generating from seed: s, some de\n",
      "s, some defined to the set of the string of the set is not none:\n",
      " raise valueerror(\"indext is not none:\n",
      " raise valueerror(\"indext is not none:\n",
      " raise valueerror(\"indext is not none:\n",
      " raise valueerror(\"indext is not none:\n",
      " raise valueerror(\"indext is not none:\n",
      " raise valueerror(\"indext is not none:\n",
      " raise valueerror(\"indext is not none:\n",
      " raise valueerror(\"indext is not none:\n",
      " raise valueerror(\"indext is not none:\n",
      " raise valueerror(\"indext is not none:\n",
      " raise valueerror(\"indext is not none:\n",
      " raise valueerror(\"indext is not none:\n",
      " raise valueerror(\"indext is not none:\n",
      " raise valueerror(\"indext is not none:\n",
      " raise valueerror(\"indext is not none:\n",
      " raise valueerror(\"indext is not none:\n",
      " raise valueerror(\"indext is not none:\n",
      " raise valueerror(\"indext is not none:\n",
      " raise valueerror(\"indext is not none:\n",
      " raise valueerror(\"indext is not none:\n",
      " raise valueerror(\"indext is not none:\n",
      " raise valueerror(\"indext is not none:\n",
      " raise valueerror(\"indext is not none:\n",
      " raise valueerror(\"indext is not none:\n",
      " raise va\n",
      "\n",
      "==================================================\n",
      "Iteration #: 4\n",
      "Epoch 1/1\n",
      "637891/637891 [==============================] - 245s 385us/step - loss: 1.4324\n",
      "Generating from seed: lar file.\"\n",
      "lar file.\"\"\"\n",
      " return a callable of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the string of the strin\n",
      "\n",
      "==================================================\n",
      "Iteration #: 5\n",
      "Epoch 1/1\n",
      "637891/637891 [==============================] - 237s 372us/step - loss: 1.3839\n",
      "Generating from seed: nks shall \n",
      "nks shall contents the encoding = self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__na\n",
      "\n",
      "==================================================\n",
      "Iteration #: 6\n",
      "Epoch 1/1\n",
      "637891/637891 [==============================] - 246s 385us/step - loss: 1.3474\n",
      "Generating from seed:  self._che\n",
      " self._check(\"same\",\"same\",\"\"\"\"\"\n",
      " return true\n",
      " if isinstance(path, bytes):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \"\"\"return the file of the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string and the string a\n",
      "\n",
      "==================================================\n",
      "Iteration #: 7\n",
      "Epoch 1/1\n",
      "637891/637891 [==============================] - 241s 378us/step - loss: 1.3186\n",
      "Generating from seed:           \n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
      "\n",
      "==================================================\n",
      "Iteration #: 8\n",
      "Epoch 1/1\n",
      "637891/637891 [==============================] - 258s 404us/step - loss: 1.2949\n",
      "Generating from seed: urn -1\n",
      " de\n",
      "urn -1\n",
      " def __init__(self):\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__\n",
      "\n",
      "==================================================\n",
      "Iteration #: 9\n",
      "Epoch 1/1\n",
      "637891/637891 [==============================] - 270s 424us/step - loss: 1.2753\n",
      "Generating from seed:  # length \n",
      " # length of the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in the path in t\n",
      "\n",
      "==================================================\n",
      "Iteration #: 10\n",
      "Epoch 1/1\n",
      "637891/637891 [==============================] - 265s 415us/step - loss: 1.2580\n",
      "Generating from seed: ator)\n",
      " ite\n",
      "ator)\n",
      " iterator.register(self, info))\n",
      " else:\n",
      " return self._file.tokenize(self, name, encoding, errors)\n",
      " def __init__(self, type, encoding, errors)\n",
      " def __init__(self, type, encoding, errors)\n",
      " def __init__(self, type, encoding, errors)\n",
      " def __init__(self, type, encoding, errors)\n",
      " def __init__(self, type, encoding, errors)\n",
      " def __init__(self, type, encoding, errors)\n",
      " def __init__(self, type, encoding, errors)\n",
      " def __init__(self, type, encoding, errors)\n",
      " def __init__(self, type, encoding, errors)\n",
      " def __init__(self, type, encoding, errors)\n",
      " def __init__(self, type, encoding, errors)\n",
      " def __init__(self, type, encoding, errors)\n",
      " def __init__(self, type, encoding, errors)\n",
      " def __init__(self, type, encoding, errors)\n",
      " def __init__(self, type, encoding, errors)\n",
      " def __init__(self, type, encoding, errors)\n",
      " def __init__(self, type, encoding, errors)\n",
      " def __init__(self, type, encoding, errors)\n",
      " def __init__(self, type, encoding, errors)\n",
      " def __init__(self, type, encoding, errors)\n",
      " def __init__(self, type, e\n",
      "\n",
      "==================================================\n",
      "Iteration #: 11\n",
      "Epoch 1/1\n",
      "637891/637891 [==============================] - 236s 371us/step - loss: 1.2432\n",
      "Generating from seed: time.local\n",
      "time.locale(name, path, start)\n",
      " else:\n",
      " raise error(\"uninging = none\n",
      " if not isinstance(path, bytes):\n",
      " \"\"\"return the path is not none:\n",
      " raise error(\"uninging = none\n",
      " if not isinstance(path, bytes):\n",
      " \"\"\"return the path is not none:\n",
      " raise error(\"uninging = none\n",
      " if not isinstance(path, bytes):\n",
      " \"\"\"return the path is not none:\n",
      " raise error(\"uninging = none\n",
      " if not isinstance(path, bytes):\n",
      " \"\"\"return the path is not none:\n",
      " raise error(\"uninging = none\n",
      " if not isinstance(path, bytes):\n",
      " \"\"\"return the path is not none:\n",
      " raise error(\"uninging = none\n",
      " if not isinstance(path, bytes):\n",
      " \"\"\"return the path is not none:\n",
      " raise error(\"uninging = none\n",
      " if not isinstance(path, bytes):\n",
      " \"\"\"return the path is not none:\n",
      " raise error(\"uninging = none\n",
      " if not isinstance(path, bytes):\n",
      " \"\"\"return the path is not none:\n",
      " raise error(\"uninging = none\n",
      " if not isinstance(path, bytes):\n",
      " \"\"\"return the path is not none:\n",
      " raise error(\"uninging = none\n",
      " if not isinstance(path, bytes):\n",
      " \"\"\"return the path is not none:\n",
      " raise error\n",
      "\n",
      "==================================================\n",
      "Iteration #: 12\n",
      "Epoch 1/1\n",
      "637891/637891 [==============================] - 296s 464us/step - loss: 1.2301\n",
      "Generating from seed: widths.app\n",
      "widths.append(self.__read(self):\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class__.__name__)\n",
      " return self.__class\n",
      "\n",
      "==================================================\n",
      "Iteration #: 13\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 62976/637891 [=>............................] - ETA: 4:34 - loss: 1.2011"
     ]
    }
   ],
   "source": [
    "for iteration in range(NUM_ITERATIONS):\n",
    "    print(\"\\n\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"Iteration #: %d\" % (iteration))\n",
    "    model.fit(X, y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION)\n",
    "    \n",
    "    test_idx = np.random.randint(len(input_chars))\n",
    "    test_chars = input_chars[test_idx]\n",
    "    print(\"Generating from seed: %s\" % (test_chars))\n",
    "    print(test_chars, end=\"\")\n",
    "    for i in range (NUM_PREDS_PER_EPOCH):\n",
    "        Xtest = np.zeros((1, SEQLEN, nb_chars))\n",
    "        for i, ch in enumerate(test_chars):\n",
    "            Xtest[0, i, char2index[ch]] = 1\n",
    "        pred = model.predict(Xtest, verbose=0)[0]\n",
    "        ypred = index2char[np.argmax(pred)]\n",
    "        print(ypred, end=\"\")\n",
    "        test_chars = test_chars[1:] + ypred\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
